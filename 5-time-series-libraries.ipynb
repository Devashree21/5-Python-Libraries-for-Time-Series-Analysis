{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## tsfresh","metadata":{"id":"cJB0IRptpHkx"}},{"cell_type":"markdown","source":"pip install tsfresh","metadata":{"id":"yOc0xm1PzQoV","outputId":"05ec591e-89ac-400f-84a4-eb17869a5158","_kg_hide-output":true}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport plotly","metadata":{"id":"Yw4xkKS6EmAX","execution":{"iopub.status.busy":"2022-05-05T08:33:46.458902Z","iopub.execute_input":"2022-05-05T08:33:46.459295Z","iopub.status.idle":"2022-05-05T08:33:46.999445Z","shell.execute_reply.started":"2022-05-05T08:33:46.459189Z","shell.execute_reply":"2022-05-05T08:33:46.998568Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from tsfresh import extract_features, extract_relevant_features, select_features\nfrom tsfresh.utilities.dataframe_functions import impute, make_forecasting_frame\nfrom tsfresh.feature_extraction import ComprehensiveFCParameters, settings\n","metadata":{"id":"vx02dtihpHk1","execution":{"iopub.status.busy":"2022-05-05T08:33:47.001008Z","iopub.execute_input":"2022-05-05T08:33:47.001241Z","iopub.status.idle":"2022-05-05T08:33:47.941967Z","shell.execute_reply.started":"2022-05-05T08:33:47.001214Z","shell.execute_reply":"2022-05-05T08:33:47.940994Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"! wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\n# Reading the data\ndata = pd.read_csv('airline-passengers.csv')","metadata":{"id":"IEM3AlQRumMG","outputId":"44284e1f-eba4-4b4a-bc0f-3296b0923fb0","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-05T08:33:47.943373Z","iopub.execute_input":"2022-05-05T08:33:47.943715Z","iopub.status.idle":"2022-05-05T08:33:49.147764Z","shell.execute_reply.started":"2022-05-05T08:33:47.943674Z","shell.execute_reply":"2022-05-05T08:33:49.146760Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2022-05-05 08:33:48--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2180 (2.1K) [text/plain]\nSaving to: ‘airline-passengers.csv.3’\n\nairline-passengers. 100%[===================>]   2.13K  --.-KB/s    in 0s      \n\n2022-05-05 08:33:49 (35.2 MB/s) - ‘airline-passengers.csv.3’ saved [2180/2180]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"data.shape","metadata":{"id":"3c_tlVKWsWqV","outputId":"7326c903-2786-4a43-b0ee-dd9352a678c3","execution":{"iopub.status.busy":"2022-05-05T08:33:49.150299Z","iopub.execute_input":"2022-05-05T08:33:49.151234Z","iopub.status.idle":"2022-05-05T08:33:49.161976Z","shell.execute_reply.started":"2022-05-05T08:33:49.151188Z","shell.execute_reply":"2022-05-05T08:33:49.161249Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(144, 2)"},"metadata":{}}]},{"cell_type":"code","source":"data.head()","metadata":{"id":"-jztSBSlpHk8","outputId":"a183c27e-a355-4ffa-e958-89a494b4d70f","execution":{"iopub.status.busy":"2022-05-05T08:33:49.163265Z","iopub.execute_input":"2022-05-05T08:33:49.163768Z","iopub.status.idle":"2022-05-05T08:33:49.185774Z","shell.execute_reply.started":"2022-05-05T08:33:49.163725Z","shell.execute_reply":"2022-05-05T08:33:49.184208Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"     Month  Passengers\n0  1949-01         112\n1  1949-02         118\n2  1949-03         132\n3  1949-04         129\n4  1949-05         121","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Month</th>\n      <th>Passengers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1949-01</td>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1949-02</td>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1949-03</td>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1949-04</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1949-05</td>\n      <td>121</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.dtypes","metadata":{"id":"0o7N6Vp4pHk9","outputId":"8569a394-9935-4fb1-accb-6748b343bc7b","execution":{"iopub.status.busy":"2022-05-05T08:33:49.187553Z","iopub.execute_input":"2022-05-05T08:33:49.188233Z","iopub.status.idle":"2022-05-05T08:33:49.195849Z","shell.execute_reply.started":"2022-05-05T08:33:49.188186Z","shell.execute_reply":"2022-05-05T08:33:49.195156Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Month         object\nPassengers     int64\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"data.columns = ['month','#Passengers']\ndata['month'] = pd.to_datetime(data['month'],infer_datetime_format=True,format='%y%m')\n\ndf_pass, y_air = make_forecasting_frame(data[\"#Passengers\"], kind=\"#Passengers\", max_timeshift=12, rolling_direction=1)\nprint(df_pass)","metadata":{"id":"fFYg9-oOpHlA","outputId":"ec127c61-82a1-4d15-d843-6dc8168449b1","execution":{"iopub.status.busy":"2022-05-05T08:33:49.197530Z","iopub.execute_input":"2022-05-05T08:33:49.198177Z","iopub.status.idle":"2022-05-05T08:33:49.877073Z","shell.execute_reply.started":"2022-05-05T08:33:49.198133Z","shell.execute_reply":"2022-05-05T08:33:49.875873Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Rolling: 100%|██████████| 10/10 [00:00<00:00, 22.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"             id  time  value         kind\n196     (id, 1)     0    112  #Passengers\n198     (id, 2)     0    112  #Passengers\n199     (id, 2)     1    118  #Passengers\n201     (id, 3)     0    112  #Passengers\n202     (id, 3)     1    118  #Passengers\n...         ...   ...    ...          ...\n1593  (id, 143)   138    622  #Passengers\n1594  (id, 143)   139    606  #Passengers\n1595  (id, 143)   140    508  #Passengers\n1596  (id, 143)   141    461  #Passengers\n1597  (id, 143)   142    390  #Passengers\n\n[1650 rows x 4 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"extraction_settings = ComprehensiveFCParameters()\nfeatures = extract_features(df_pass, column_id=\"id\", column_sort=\"time\", column_value=\"value\", impute_function=impute,\n                     show_warnings=False,\n                     default_fc_parameters=extraction_settings\n                     )","metadata":{"id":"tYcUT1t5pHlB","outputId":"9914de4d-74d9-4ffb-d921-f1e4795324b1","execution":{"iopub.status.busy":"2022-05-05T08:33:49.878847Z","iopub.execute_input":"2022-05-05T08:33:49.879206Z","iopub.status.idle":"2022-05-05T08:33:54.987656Z","shell.execute_reply.started":"2022-05-05T08:33:49.879157Z","shell.execute_reply":"2022-05-05T08:33:54.986713Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Feature Extraction: 100%|██████████| 10/10 [00:04<00:00,  2.25it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"features","metadata":{"id":"84OCjStXzH_G","outputId":"3faf4770-6506-4169-b554-971c9b8fa4de","execution":{"iopub.status.busy":"2022-05-05T08:33:54.990598Z","iopub.execute_input":"2022-05-05T08:33:54.990841Z","iopub.status.idle":"2022-05-05T08:33:55.047173Z","shell.execute_reply.started":"2022-05-05T08:33:54.990813Z","shell.execute_reply":"2022-05-05T08:33:55.046272Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"        value__variance_larger_than_standard_deviation  \\\nid 1                                               0.0   \n   2                                               1.0   \n   3                                               1.0   \n   4                                               1.0   \n   5                                               1.0   \n...                                                ...   \n   139                                             1.0   \n   140                                             1.0   \n   141                                             1.0   \n   142                                             1.0   \n   143                                             1.0   \n\n        value__has_duplicate_max  value__has_duplicate_min  \\\nid 1                         0.0                       0.0   \n   2                         0.0                       0.0   \n   3                         0.0                       0.0   \n   4                         0.0                       0.0   \n   5                         0.0                       0.0   \n...                          ...                       ...   \n   139                       0.0                       0.0   \n   140                       0.0                       0.0   \n   141                       0.0                       0.0   \n   142                       0.0                       0.0   \n   143                       0.0                       0.0   \n\n        value__has_duplicate  value__sum_values  value__abs_energy  \\\nid 1                     0.0              112.0            12544.0   \n   2                     0.0              230.0            26468.0   \n   3                     0.0              362.0            43892.0   \n   4                     0.0              491.0            60533.0   \n   5                     0.0              612.0            75174.0   \n...                      ...                ...                ...   \n   139                   0.0             5513.0          2598313.0   \n   140                   0.0             5560.0          2653068.0   \n   141                   0.0             5605.0          2696763.0   \n   142                   1.0             5659.0          2743635.0   \n   143                   1.0             5687.0          2764691.0   \n\n        value__mean_abs_change  value__mean_change  \\\nid 1                 22.363636            2.272727   \n   2                  6.000000            6.000000   \n   3                 10.000000           10.000000   \n   4                  7.666667            5.666667   \n   5                  7.750000            2.250000   \n...                        ...                 ...   \n   139               46.272727            5.727273   \n   140               39.000000           13.000000   \n   141               42.818182            9.181818   \n   142               43.000000            9.000000   \n   143               45.545455           -1.363636   \n\n        value__mean_second_derivative_central  value__median  ...  \\\nid 1                                -0.500000          112.0  ...   \n   2                                -0.500000          115.0  ...   \n   3                                 4.000000          118.0  ...   \n   4                                -2.250000          123.5  ...   \n   5                                -2.333333          121.0  ...   \n...                                       ...            ...  ...   \n   139                               9.150000          440.0  ...   \n   140                               2.000000          440.0  ...   \n   141                              -2.650000          440.0  ...   \n   142                              -4.500000          461.0  ...   \n   143                              -4.150000          461.0  ...   \n\n        value__permutation_entropy__dimension_6__tau_1  \\\nid 1                                           1.94591   \n   2                                           1.94591   \n   3                                           1.94591   \n   4                                           1.94591   \n   5                                           1.94591   \n...                                                ...   \n   139                                         1.94591   \n   140                                         1.94591   \n   141                                         1.94591   \n   142                                         1.94591   \n   143                                         1.94591   \n\n        value__permutation_entropy__dimension_7__tau_1  \\\nid 1                                          1.791759   \n   2                                          1.791759   \n   3                                          1.791759   \n   4                                          1.791759   \n   5                                          1.791759   \n...                                                ...   \n   139                                        1.791759   \n   140                                        1.791759   \n   141                                        1.791759   \n   142                                        1.791759   \n   143                                        1.791759   \n\n        value__query_similarity_count__query_None__threshold_0.0  \\\nid 1                                                  0.0          \n   2                                                  0.0          \n   3                                                  0.0          \n   4                                                  0.0          \n   5                                                  0.0          \n...                                                   ...          \n   139                                                0.0          \n   140                                                0.0          \n   141                                                0.0          \n   142                                                0.0          \n   143                                                0.0          \n\n        value__matrix_profile__feature_\"min\"__threshold_0.98  \\\nid 1                                                  0.0      \n   2                                                  0.0      \n   3                                                  0.0      \n   4                                                  0.0      \n   5                                                  0.0      \n...                                                   ...      \n   139                                                0.0      \n   140                                                0.0      \n   141                                                0.0      \n   142                                                0.0      \n   143                                                0.0      \n\n        value__matrix_profile__feature_\"max\"__threshold_0.98  \\\nid 1                                                  0.0      \n   2                                                  0.0      \n   3                                                  0.0      \n   4                                                  0.0      \n   5                                                  0.0      \n...                                                   ...      \n   139                                                0.0      \n   140                                                0.0      \n   141                                                0.0      \n   142                                                0.0      \n   143                                                0.0      \n\n        value__matrix_profile__feature_\"mean\"__threshold_0.98  \\\nid 1                                                  0.0       \n   2                                                  0.0       \n   3                                                  0.0       \n   4                                                  0.0       \n   5                                                  0.0       \n...                                                   ...       \n   139                                                0.0       \n   140                                                0.0       \n   141                                                0.0       \n   142                                                0.0       \n   143                                                0.0       \n\n        value__matrix_profile__feature_\"median\"__threshold_0.98  \\\nid 1                                                  0.0         \n   2                                                  0.0         \n   3                                                  0.0         \n   4                                                  0.0         \n   5                                                  0.0         \n...                                                   ...         \n   139                                                0.0         \n   140                                                0.0         \n   141                                                0.0         \n   142                                                0.0         \n   143                                                0.0         \n\n        value__matrix_profile__feature_\"25\"__threshold_0.98  \\\nid 1                                                  0.0     \n   2                                                  0.0     \n   3                                                  0.0     \n   4                                                  0.0     \n   5                                                  0.0     \n...                                                   ...     \n   139                                                0.0     \n   140                                                0.0     \n   141                                                0.0     \n   142                                                0.0     \n   143                                                0.0     \n\n        value__matrix_profile__feature_\"75\"__threshold_0.98  \\\nid 1                                                  0.0     \n   2                                                  0.0     \n   3                                                  0.0     \n   4                                                  0.0     \n   5                                                  0.0     \n...                                                   ...     \n   139                                                0.0     \n   140                                                0.0     \n   141                                                0.0     \n   142                                                0.0     \n   143                                                0.0     \n\n        value__mean_n_absolute_max__number_of_maxima_7  \nid 1                                        268.357143  \n   2                                        268.357143  \n   3                                        268.357143  \n   4                                        268.357143  \n   5                                        268.357143  \n...                                                ...  \n   139                                      504.428571  \n   140                                      511.142857  \n   141                                      517.571429  \n   142                                      523.571429  \n   143                                      523.571429  \n\n[143 rows x 789 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>value__variance_larger_than_standard_deviation</th>\n      <th>value__has_duplicate_max</th>\n      <th>value__has_duplicate_min</th>\n      <th>value__has_duplicate</th>\n      <th>value__sum_values</th>\n      <th>value__abs_energy</th>\n      <th>value__mean_abs_change</th>\n      <th>value__mean_change</th>\n      <th>value__mean_second_derivative_central</th>\n      <th>value__median</th>\n      <th>...</th>\n      <th>value__permutation_entropy__dimension_6__tau_1</th>\n      <th>value__permutation_entropy__dimension_7__tau_1</th>\n      <th>value__query_similarity_count__query_None__threshold_0.0</th>\n      <th>value__matrix_profile__feature_\"min\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"max\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"mean\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"median\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"25\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"75\"__threshold_0.98</th>\n      <th>value__mean_n_absolute_max__number_of_maxima_7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"11\" valign=\"top\">id</th>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>112.0</td>\n      <td>12544.0</td>\n      <td>22.363636</td>\n      <td>2.272727</td>\n      <td>-0.500000</td>\n      <td>112.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>230.0</td>\n      <td>26468.0</td>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>-0.500000</td>\n      <td>115.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>362.0</td>\n      <td>43892.0</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>4.000000</td>\n      <td>118.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>491.0</td>\n      <td>60533.0</td>\n      <td>7.666667</td>\n      <td>5.666667</td>\n      <td>-2.250000</td>\n      <td>123.5</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>612.0</td>\n      <td>75174.0</td>\n      <td>7.750000</td>\n      <td>2.250000</td>\n      <td>-2.333333</td>\n      <td>121.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5513.0</td>\n      <td>2598313.0</td>\n      <td>46.272727</td>\n      <td>5.727273</td>\n      <td>9.150000</td>\n      <td>440.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>504.428571</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5560.0</td>\n      <td>2653068.0</td>\n      <td>39.000000</td>\n      <td>13.000000</td>\n      <td>2.000000</td>\n      <td>440.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>511.142857</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5605.0</td>\n      <td>2696763.0</td>\n      <td>42.818182</td>\n      <td>9.181818</td>\n      <td>-2.650000</td>\n      <td>440.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>517.571429</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5659.0</td>\n      <td>2743635.0</td>\n      <td>43.000000</td>\n      <td>9.000000</td>\n      <td>-4.500000</td>\n      <td>461.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>523.571429</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5687.0</td>\n      <td>2764691.0</td>\n      <td>45.545455</td>\n      <td>-1.363636</td>\n      <td>-4.150000</td>\n      <td>461.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>523.571429</td>\n    </tr>\n  </tbody>\n</table>\n<p>143 rows × 789 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tsfresh.utilities.dataframe_functions import impute\nimpute(features)","metadata":{"id":"oeegO4B3zehC","outputId":"c2d90fd8-a20a-433e-d3e9-02c17090bfe5","execution":{"iopub.status.busy":"2022-05-05T08:33:55.050189Z","iopub.execute_input":"2022-05-05T08:33:55.050408Z","iopub.status.idle":"2022-05-05T08:33:55.360464Z","shell.execute_reply.started":"2022-05-05T08:33:55.050382Z","shell.execute_reply":"2022-05-05T08:33:55.359853Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        value__variance_larger_than_standard_deviation  \\\nid 1                                               0.0   \n   2                                               1.0   \n   3                                               1.0   \n   4                                               1.0   \n   5                                               1.0   \n...                                                ...   \n   139                                             1.0   \n   140                                             1.0   \n   141                                             1.0   \n   142                                             1.0   \n   143                                             1.0   \n\n        value__has_duplicate_max  value__has_duplicate_min  \\\nid 1                         0.0                       0.0   \n   2                         0.0                       0.0   \n   3                         0.0                       0.0   \n   4                         0.0                       0.0   \n   5                         0.0                       0.0   \n...                          ...                       ...   \n   139                       0.0                       0.0   \n   140                       0.0                       0.0   \n   141                       0.0                       0.0   \n   142                       0.0                       0.0   \n   143                       0.0                       0.0   \n\n        value__has_duplicate  value__sum_values  value__abs_energy  \\\nid 1                     0.0              112.0            12544.0   \n   2                     0.0              230.0            26468.0   \n   3                     0.0              362.0            43892.0   \n   4                     0.0              491.0            60533.0   \n   5                     0.0              612.0            75174.0   \n...                      ...                ...                ...   \n   139                   0.0             5513.0          2598313.0   \n   140                   0.0             5560.0          2653068.0   \n   141                   0.0             5605.0          2696763.0   \n   142                   1.0             5659.0          2743635.0   \n   143                   1.0             5687.0          2764691.0   \n\n        value__mean_abs_change  value__mean_change  \\\nid 1                 22.363636            2.272727   \n   2                  6.000000            6.000000   \n   3                 10.000000           10.000000   \n   4                  7.666667            5.666667   \n   5                  7.750000            2.250000   \n...                        ...                 ...   \n   139               46.272727            5.727273   \n   140               39.000000           13.000000   \n   141               42.818182            9.181818   \n   142               43.000000            9.000000   \n   143               45.545455           -1.363636   \n\n        value__mean_second_derivative_central  value__median  ...  \\\nid 1                                -0.500000          112.0  ...   \n   2                                -0.500000          115.0  ...   \n   3                                 4.000000          118.0  ...   \n   4                                -2.250000          123.5  ...   \n   5                                -2.333333          121.0  ...   \n...                                       ...            ...  ...   \n   139                               9.150000          440.0  ...   \n   140                               2.000000          440.0  ...   \n   141                              -2.650000          440.0  ...   \n   142                              -4.500000          461.0  ...   \n   143                              -4.150000          461.0  ...   \n\n        value__permutation_entropy__dimension_6__tau_1  \\\nid 1                                           1.94591   \n   2                                           1.94591   \n   3                                           1.94591   \n   4                                           1.94591   \n   5                                           1.94591   \n...                                                ...   \n   139                                         1.94591   \n   140                                         1.94591   \n   141                                         1.94591   \n   142                                         1.94591   \n   143                                         1.94591   \n\n        value__permutation_entropy__dimension_7__tau_1  \\\nid 1                                          1.791759   \n   2                                          1.791759   \n   3                                          1.791759   \n   4                                          1.791759   \n   5                                          1.791759   \n...                                                ...   \n   139                                        1.791759   \n   140                                        1.791759   \n   141                                        1.791759   \n   142                                        1.791759   \n   143                                        1.791759   \n\n        value__query_similarity_count__query_None__threshold_0.0  \\\nid 1                                                  0.0          \n   2                                                  0.0          \n   3                                                  0.0          \n   4                                                  0.0          \n   5                                                  0.0          \n...                                                   ...          \n   139                                                0.0          \n   140                                                0.0          \n   141                                                0.0          \n   142                                                0.0          \n   143                                                0.0          \n\n        value__matrix_profile__feature_\"min\"__threshold_0.98  \\\nid 1                                                  0.0      \n   2                                                  0.0      \n   3                                                  0.0      \n   4                                                  0.0      \n   5                                                  0.0      \n...                                                   ...      \n   139                                                0.0      \n   140                                                0.0      \n   141                                                0.0      \n   142                                                0.0      \n   143                                                0.0      \n\n        value__matrix_profile__feature_\"max\"__threshold_0.98  \\\nid 1                                                  0.0      \n   2                                                  0.0      \n   3                                                  0.0      \n   4                                                  0.0      \n   5                                                  0.0      \n...                                                   ...      \n   139                                                0.0      \n   140                                                0.0      \n   141                                                0.0      \n   142                                                0.0      \n   143                                                0.0      \n\n        value__matrix_profile__feature_\"mean\"__threshold_0.98  \\\nid 1                                                  0.0       \n   2                                                  0.0       \n   3                                                  0.0       \n   4                                                  0.0       \n   5                                                  0.0       \n...                                                   ...       \n   139                                                0.0       \n   140                                                0.0       \n   141                                                0.0       \n   142                                                0.0       \n   143                                                0.0       \n\n        value__matrix_profile__feature_\"median\"__threshold_0.98  \\\nid 1                                                  0.0         \n   2                                                  0.0         \n   3                                                  0.0         \n   4                                                  0.0         \n   5                                                  0.0         \n...                                                   ...         \n   139                                                0.0         \n   140                                                0.0         \n   141                                                0.0         \n   142                                                0.0         \n   143                                                0.0         \n\n        value__matrix_profile__feature_\"25\"__threshold_0.98  \\\nid 1                                                  0.0     \n   2                                                  0.0     \n   3                                                  0.0     \n   4                                                  0.0     \n   5                                                  0.0     \n...                                                   ...     \n   139                                                0.0     \n   140                                                0.0     \n   141                                                0.0     \n   142                                                0.0     \n   143                                                0.0     \n\n        value__matrix_profile__feature_\"75\"__threshold_0.98  \\\nid 1                                                  0.0     \n   2                                                  0.0     \n   3                                                  0.0     \n   4                                                  0.0     \n   5                                                  0.0     \n...                                                   ...     \n   139                                                0.0     \n   140                                                0.0     \n   141                                                0.0     \n   142                                                0.0     \n   143                                                0.0     \n\n        value__mean_n_absolute_max__number_of_maxima_7  \nid 1                                        268.357143  \n   2                                        268.357143  \n   3                                        268.357143  \n   4                                        268.357143  \n   5                                        268.357143  \n...                                                ...  \n   139                                      504.428571  \n   140                                      511.142857  \n   141                                      517.571429  \n   142                                      523.571429  \n   143                                      523.571429  \n\n[143 rows x 789 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>value__variance_larger_than_standard_deviation</th>\n      <th>value__has_duplicate_max</th>\n      <th>value__has_duplicate_min</th>\n      <th>value__has_duplicate</th>\n      <th>value__sum_values</th>\n      <th>value__abs_energy</th>\n      <th>value__mean_abs_change</th>\n      <th>value__mean_change</th>\n      <th>value__mean_second_derivative_central</th>\n      <th>value__median</th>\n      <th>...</th>\n      <th>value__permutation_entropy__dimension_6__tau_1</th>\n      <th>value__permutation_entropy__dimension_7__tau_1</th>\n      <th>value__query_similarity_count__query_None__threshold_0.0</th>\n      <th>value__matrix_profile__feature_\"min\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"max\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"mean\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"median\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"25\"__threshold_0.98</th>\n      <th>value__matrix_profile__feature_\"75\"__threshold_0.98</th>\n      <th>value__mean_n_absolute_max__number_of_maxima_7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"11\" valign=\"top\">id</th>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>112.0</td>\n      <td>12544.0</td>\n      <td>22.363636</td>\n      <td>2.272727</td>\n      <td>-0.500000</td>\n      <td>112.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>230.0</td>\n      <td>26468.0</td>\n      <td>6.000000</td>\n      <td>6.000000</td>\n      <td>-0.500000</td>\n      <td>115.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>362.0</td>\n      <td>43892.0</td>\n      <td>10.000000</td>\n      <td>10.000000</td>\n      <td>4.000000</td>\n      <td>118.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>491.0</td>\n      <td>60533.0</td>\n      <td>7.666667</td>\n      <td>5.666667</td>\n      <td>-2.250000</td>\n      <td>123.5</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>612.0</td>\n      <td>75174.0</td>\n      <td>7.750000</td>\n      <td>2.250000</td>\n      <td>-2.333333</td>\n      <td>121.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>268.357143</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5513.0</td>\n      <td>2598313.0</td>\n      <td>46.272727</td>\n      <td>5.727273</td>\n      <td>9.150000</td>\n      <td>440.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>504.428571</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5560.0</td>\n      <td>2653068.0</td>\n      <td>39.000000</td>\n      <td>13.000000</td>\n      <td>2.000000</td>\n      <td>440.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>511.142857</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5605.0</td>\n      <td>2696763.0</td>\n      <td>42.818182</td>\n      <td>9.181818</td>\n      <td>-2.650000</td>\n      <td>440.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>517.571429</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5659.0</td>\n      <td>2743635.0</td>\n      <td>43.000000</td>\n      <td>9.000000</td>\n      <td>-4.500000</td>\n      <td>461.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>523.571429</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>5687.0</td>\n      <td>2764691.0</td>\n      <td>45.545455</td>\n      <td>-1.363636</td>\n      <td>-4.150000</td>\n      <td>461.0</td>\n      <td>...</td>\n      <td>1.94591</td>\n      <td>1.791759</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>523.571429</td>\n    </tr>\n  </tbody>\n</table>\n<p>143 rows × 789 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from tsfresh import select_features\nfiltered_features = select_features(features, y_air)\nfiltered_features","metadata":{"id":"H8TSRTRYzh6l","outputId":"cf923b5b-ee99-4b41-e5fe-cc233ab1d398","execution":{"iopub.status.busy":"2022-05-05T08:33:55.361672Z","iopub.execute_input":"2022-05-05T08:33:55.361865Z","iopub.status.idle":"2022-05-05T08:34:50.890778Z","shell.execute_reply.started":"2022-05-05T08:33:55.361842Z","shell.execute_reply":"2022-05-05T08:34:50.889909Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"        value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.6__ql_0.4  \\\nid 1                                                  0.0                   \n   2                                                  0.0                   \n   3                                                  0.0                   \n   4                                                  0.0                   \n   5                                                  0.0                   \n...                                                   ...                   \n   139                                                0.0                   \n   140                                                0.0                   \n   141                                                0.0                   \n   142                                                0.0                   \n   143                                                0.0                   \n\n        value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.6__ql_0.4  \\\nid 1                                                  0.0                  \n   2                                                  0.0                  \n   3                                                  0.0                  \n   4                                                  0.0                  \n   5                                                  0.0                  \n...                                                   ...                  \n   139                                                0.0                  \n   140                                                0.0                  \n   141                                                0.0                  \n   142                                                0.0                  \n   143                                                0.0                  \n\n        value__length  value__range_count__max_1000000000000.0__min_0  \\\nid 1              1.0                                             1.0   \n   2              2.0                                             2.0   \n   3              3.0                                             3.0   \n   4              4.0                                             4.0   \n   5              5.0                                             5.0   \n...               ...                                             ...   \n   139           12.0                                            12.0   \n   140           12.0                                            12.0   \n   141           12.0                                            12.0   \n   142           12.0                                            12.0   \n   143           12.0                                            12.0   \n\n        value__index_mass_quantile__q_0.2  value__index_mass_quantile__q_0.1  \\\nid 1                             1.000000                           1.000000   \n   2                             0.500000                           0.500000   \n   3                             0.333333                           0.333333   \n   4                             0.250000                           0.250000   \n   5                             0.400000                           0.200000   \n...                                   ...                                ...   \n   139                           0.250000                           0.083333   \n   140                           0.250000                           0.166667   \n   141                           0.250000                           0.166667   \n   142                           0.250000                           0.166667   \n   143                           0.250000                           0.166667   \n\n        value__permutation_entropy__dimension_7__tau_1  \\\nid 1                                          1.791759   \n   2                                          1.791759   \n   3                                          1.791759   \n   4                                          1.791759   \n   5                                          1.791759   \n...                                                ...   \n   139                                        1.791759   \n   140                                        1.791759   \n   141                                        1.791759   \n   142                                        1.791759   \n   143                                        1.791759   \n\n        value__permutation_entropy__dimension_6__tau_1  \\\nid 1                                           1.94591   \n   2                                           1.94591   \n   3                                           1.94591   \n   4                                           1.94591   \n   5                                           1.94591   \n...                                                ...   \n   139                                         1.94591   \n   140                                         1.94591   \n   141                                         1.94591   \n   142                                         1.94591   \n   143                                         1.94591   \n\n        value__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.6  \\\nid 1                                                  0.0                   \n   2                                                  0.0                   \n   3                                                  0.0                   \n   4                                                  0.0                   \n   5                                                  0.0                   \n...                                                   ...                   \n   139                                                0.0                   \n   140                                                0.0                   \n   141                                                0.0                   \n   142                                                0.0                   \n   143                                                0.0                   \n\n        value__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.8__ql_0.6  \nid 1                                                  0.0                   \n   2                                                  0.0                   \n   3                                                  0.0                   \n   4                                                  0.0                   \n   5                                                  0.0                   \n...                                                   ...                   \n   139                                                0.0                   \n   140                                                0.0                   \n   141                                                0.0                   \n   142                                                0.0                   \n   143                                                0.0                   \n\n[143 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>value__change_quantiles__f_agg_\"var\"__isabs_False__qh_0.6__ql_0.4</th>\n      <th>value__change_quantiles__f_agg_\"var\"__isabs_True__qh_0.6__ql_0.4</th>\n      <th>value__length</th>\n      <th>value__range_count__max_1000000000000.0__min_0</th>\n      <th>value__index_mass_quantile__q_0.2</th>\n      <th>value__index_mass_quantile__q_0.1</th>\n      <th>value__permutation_entropy__dimension_7__tau_1</th>\n      <th>value__permutation_entropy__dimension_6__tau_1</th>\n      <th>value__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.6</th>\n      <th>value__change_quantiles__f_agg_\"mean\"__isabs_False__qh_0.8__ql_0.6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"11\" valign=\"top\">id</th>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.500000</td>\n      <td>0.500000</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>0.250000</td>\n      <td>0.250000</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>5.0</td>\n      <td>0.400000</td>\n      <td>0.200000</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>0.250000</td>\n      <td>0.083333</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>0.250000</td>\n      <td>0.166667</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>0.250000</td>\n      <td>0.166667</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>0.250000</td>\n      <td>0.166667</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>0.250000</td>\n      <td>0.166667</td>\n      <td>1.791759</td>\n      <td>1.94591</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>143 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## AutoTS","metadata":{"id":"kpUEaaBNpHlE"}},{"cell_type":"markdown","source":"pip install autots","metadata":{"id":"kgapLmqlzUHm","outputId":"b4b8ec00-10f4-4ad9-dea5-dd327de6c340","_kg_hide-output":true}},{"cell_type":"code","source":"# Loading the package\nfrom autots import AutoTS","metadata":{"id":"AHqW7DWvpHlG","execution":{"iopub.status.busy":"2022-05-05T08:34:50.892287Z","iopub.execute_input":"2022-05-05T08:34:50.893227Z","iopub.status.idle":"2022-05-05T08:34:51.067782Z","shell.execute_reply.started":"2022-05-05T08:34:50.893183Z","shell.execute_reply":"2022-05-05T08:34:51.066994Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data_air=data.set_index('month')\ndata_air","metadata":{"id":"a-85OXp6ZfAP","outputId":"a2483bf0-4139-42ba-fb9d-5aa194b343c3","execution":{"iopub.status.busy":"2022-05-05T08:34:51.069185Z","iopub.execute_input":"2022-05-05T08:34:51.069755Z","iopub.status.idle":"2022-05-05T08:34:51.083865Z","shell.execute_reply.started":"2022-05-05T08:34:51.069706Z","shell.execute_reply":"2022-05-05T08:34:51.083095Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"            #Passengers\nmonth                  \n1949-01-01          112\n1949-02-01          118\n1949-03-01          132\n1949-04-01          129\n1949-05-01          121\n...                 ...\n1960-08-01          606\n1960-09-01          508\n1960-10-01          461\n1960-11-01          390\n1960-12-01          432\n\n[144 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>#Passengers</th>\n    </tr>\n    <tr>\n      <th>month</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1949-01-01</th>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>1949-02-01</th>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>1949-03-01</th>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>1949-04-01</th>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>1949-05-01</th>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1960-08-01</th>\n      <td>606</td>\n    </tr>\n    <tr>\n      <th>1960-09-01</th>\n      <td>508</td>\n    </tr>\n    <tr>\n      <th>1960-10-01</th>\n      <td>461</td>\n    </tr>\n    <tr>\n      <th>1960-11-01</th>\n      <td>390</td>\n    </tr>\n    <tr>\n      <th>1960-12-01</th>\n      <td>432</td>\n    </tr>\n  </tbody>\n</table>\n<p>144 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data[[\"month\", \"#Passengers\"]]\ndata[\"#Passengers\"].plot(figsize=(12, 8), title=\"Predictions for passengeres\", fontsize=20, label=\"Passengers\")\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"id":"SNmutVxCYiju","outputId":"2f4eed4c-2568-405f-dc7e-7f17686e12fc","execution":{"iopub.status.busy":"2022-05-05T08:34:51.085167Z","iopub.execute_input":"2022-05-05T08:34:51.085846Z","iopub.status.idle":"2022-05-05T08:34:51.343274Z","shell.execute_reply.started":"2022-05-05T08:34:51.085808Z","shell.execute_reply":"2022-05-05T08:34:51.342703Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x576 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"from autots import AutoTS\nmodel = AutoTS(forecast_length=12, frequency='infer', \n               ensemble='simple')\nmodel = model.fit(data, date_col='month', value_col='#Passengers', id_col=None)","metadata":{"id":"cN0v1BLiYiew","outputId":"141ed6e7-ab85-465b-8335-24a0b9aea953","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-05T08:34:51.344188Z","iopub.execute_input":"2022-05-05T08:34:51.344849Z","iopub.status.idle":"2022-05-05T08:41:10.811545Z","shell.execute_reply.started":"2022-05-05T08:34:51.344816Z","shell.execute_reply":"2022-05-05T08:41:10.810783Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Inferred frequency is: MS\nModel Number: 1 with model AverageValueNaive in generation 0 of 10\nModel Number: 2 with model AverageValueNaive in generation 0 of 10\nModel Number: 3 with model AverageValueNaive in generation 0 of 10\nModel Number: 4 with model DatepartRegression in generation 0 of 10\nModel Number: 5 with model DatepartRegression in generation 0 of 10\nModel Number: 6 with model DatepartRegression in generation 0 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 7 with model DatepartRegression in generation 0 of 10\n","output_type":"stream"},{"name":"stderr","text":"2022-05-05 08:34:55.476640: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2022-05-05 08:34:56.144709: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n5/5 [==============================] - 7s 6ms/step - loss: 0.3799\nEpoch 2/50\n5/5 [==============================] - 0s 6ms/step - loss: 0.3801\nEpoch 3/50\n5/5 [==============================] - 0s 6ms/step - loss: 0.3794\nEpoch 4/50\n5/5 [==============================] - 0s 6ms/step - loss: 0.3818\nEpoch 5/50\n5/5 [==============================] - 0s 6ms/step - loss: 0.3812\nEpoch 6/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3801\nEpoch 7/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3750\nEpoch 8/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3777\nEpoch 9/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3831\nEpoch 10/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3747\nEpoch 11/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3779\nEpoch 12/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3746\nEpoch 13/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3778\nEpoch 14/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3757\nEpoch 15/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3772\nEpoch 16/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3744\nEpoch 17/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3749\nEpoch 18/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3752\nEpoch 19/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3774\nEpoch 20/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3731\nEpoch 21/50\n5/5 [==============================] - 0s 6ms/step - loss: 0.3746\nEpoch 22/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3724\nEpoch 23/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3736\nEpoch 24/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3729\nEpoch 25/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3754\nEpoch 26/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3768\nEpoch 27/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3791\nEpoch 28/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3726\nEpoch 29/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3729\nEpoch 30/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3744\nEpoch 31/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3739\nEpoch 32/50\n5/5 [==============================] - 0s 12ms/step - loss: 0.3781\nEpoch 33/50\n5/5 [==============================] - 0s 11ms/step - loss: 0.3761\nEpoch 34/50\n5/5 [==============================] - 0s 7ms/step - loss: 0.3743\nEpoch 35/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3699\nEpoch 36/50\n5/5 [==============================] - 0s 6ms/step - loss: 0.3731\nEpoch 37/50\n5/5 [==============================] - 0s 6ms/step - loss: 0.3708\nEpoch 38/50\n5/5 [==============================] - 0s 6ms/step - loss: 0.3713\nEpoch 39/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3671\nEpoch 40/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3714\nEpoch 41/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3733\nEpoch 42/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3646\nEpoch 43/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3733\nEpoch 44/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3728\nEpoch 45/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3688\nEpoch 46/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3665\nEpoch 47/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3635\nEpoch 48/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3697\nEpoch 49/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3680\nEpoch 50/50\n5/5 [==============================] - 0s 5ms/step - loss: 0.3679\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 7: DatepartRegression\nModel Number: 8 with model ETS in generation 0 of 10\nModel Number: 9 with model ETS in generation 0 of 10\nModel Number: 10 with model GLM in generation 0 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 10: GLM\nModel Number: 11 with model GLM in generation 0 of 10\nModel Number: 12 with model GLS in generation 0 of 10\nModel Number: 13 with model GLS in generation 0 of 10\nModel Number: 14 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 14: GluonTS\nModel Number: 15 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 15: GluonTS\nModel Number: 16 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 16: GluonTS\nModel Number: 17 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 17: GluonTS\nModel Number: 18 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 18: GluonTS\nModel Number: 19 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 19: GluonTS\nModel Number: 20 with model LastValueNaive in generation 0 of 10\nModel Number: 21 with model LastValueNaive in generation 0 of 10\nModel Number: 22 with model LastValueNaive in generation 0 of 10\nModel Number: 23 with model LastValueNaive in generation 0 of 10\nModel Number: 24 with model SeasonalNaive in generation 0 of 10\nModel Number: 25 with model SeasonalNaive in generation 0 of 10\nModel Number: 26 with model SeasonalNaive in generation 0 of 10\nModel Number: 27 with model SeasonalNaive in generation 0 of 10\nModel Number: 28 with model UnobservedComponents in generation 0 of 10\nModel Number: 29 with model UnobservedComponents in generation 0 of 10\nModel Number: 30 with model UnobservedComponents in generation 0 of 10\nModel Number: 31 with model VAR in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 31: VAR\nModel Number: 32 with model VAR in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 32: VAR\nModel Number: 33 with model VAR in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 33: VAR\nModel Number: 34 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 34: VECM\nModel Number: 35 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 35: VECM\nModel Number: 36 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 36: VECM\nModel Number: 37 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 37: VECM\nModel Number: 38 with model WindowRegression in generation 0 of 10\nModel Number: 39 with model ZeroesNaive in generation 0 of 10\nModel Number: 40 with model ZeroesNaive in generation 0 of 10\nModel Number: 41 with model LastValueNaive in generation 0 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 42 with model AverageValueNaive in generation 0 of 10\nModel Number: 43 with model GLS in generation 0 of 10\nModel Number: 44 with model SeasonalNaive in generation 0 of 10\nModel Number: 45 with model GLM in generation 0 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 45: GLM\nModel Number: 46 with model ETS in generation 0 of 10\nModel Number: 47 with model FBProphet in generation 0 of 10\nInitial log joint probability = -3.51411\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       208.986   0.000376467       104.187      0.7848      0.7848      127   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     170       209.338   6.24775e-05       98.7875   7.547e-07       0.001      253  LS failed, Hessian reset \n     199       209.343   5.32821e-07       80.2914      0.5096      0.5096      288   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     236       209.343   8.90891e-09       74.2199      0.5059      0.5059      333   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 48 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 48: GluonTS\nModel Number: 49 with model UnobservedComponents in generation 0 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 49: UnobservedComponents\nModel Number: 50 with model VAR in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 50: VAR\nModel Number: 51 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 51: VECM\nModel Number: 52 with model WindowRegression in generation 0 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 52: WindowRegression\nModel Number: 53 with model DatepartRegression in generation 0 of 10\nModel Number: 54 with model MultivariateRegression in generation 0 of 10\nModel Number: 55 with model UnivariateMotif in generation 0 of 10\nModel Number: 56 with model MultivariateMotif in generation 0 of 10\nModel Number: 57 with model SectionalMotif in generation 0 of 10\nModel Number: 58 with model NVAR in generation 0 of 10\nModel Number: 59 with model Theta in generation 0 of 10\nModel Number: 60 with model ARDL in generation 0 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 60: ARDL\nModel Number: 61 with model VAR in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 61: VAR\nModel Number: 62 with model VECM in generation 0 of 10\nTemplate Eval Error: Exception('Transformer StandardScaler failed on fit') in model 62: VECM\nModel Number: 63 with model SeasonalNaive in generation 0 of 10\nModel Number: 64 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 64: GluonTS\nModel Number: 65 with model UnivariateMotif in generation 0 of 10\nModel Number: 66 with model WindowRegression in generation 0 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/extmath.py:985: RuntimeWarning: invalid value encountered in true_divide\n  updated_mean = (last_sum + new_sum) / updated_sample_count\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/extmath.py:990: RuntimeWarning: invalid value encountered in true_divide\n  T = new_sum / new_sample_count\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/extmath.py:1020: RuntimeWarning: invalid value encountered in true_divide\n  new_unnormalized_variance -= correction ** 2 / new_sample_count\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 67 with model GLM in generation 0 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 67: GLM\nModel Number: 68 with model LastValueNaive in generation 0 of 10\nModel Number: 69 with model SectionalMotif in generation 0 of 10\nModel Number: 70 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 70: VECM\nModel Number: 71 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 71: VECM\nModel Number: 72 with model MultivariateMotif in generation 0 of 10\nModel Number: 73 with model GLS in generation 0 of 10\nModel Number: 74 with model MultivariateMotif in generation 0 of 10\nModel Number: 75 with model DatepartRegression in generation 0 of 10\nEpoch 1/50\n4/4 [==============================] - 4s 185ms/step - loss: 95.0180 - val_loss: 64.5773\nEpoch 2/50\n4/4 [==============================] - 0s 47ms/step - loss: 88.6466 - val_loss: 18.2453\nEpoch 3/50\n4/4 [==============================] - 0s 49ms/step - loss: 77.6613 - val_loss: 8.6036\nEpoch 4/50\n4/4 [==============================] - 0s 42ms/step - loss: 85.0130 - val_loss: 54.4362\nEpoch 5/50\n4/4 [==============================] - 0s 44ms/step - loss: 87.6206 - val_loss: 14.1942\nEpoch 6/50\n4/4 [==============================] - 0s 45ms/step - loss: 66.5668 - val_loss: 60.9071\nEpoch 7/50\n4/4 [==============================] - 0s 47ms/step - loss: 81.7125 - val_loss: 8.9047\nEpoch 8/50\n4/4 [==============================] - 0s 46ms/step - loss: 82.4793 - val_loss: 9.6266\nEpoch 9/50\n4/4 [==============================] - 0s 45ms/step - loss: 78.9848 - val_loss: 50.2251\nEpoch 10/50\n4/4 [==============================] - 0s 47ms/step - loss: 72.3655 - val_loss: 60.8495\nEpoch 11/50\n4/4 [==============================] - 0s 43ms/step - loss: 73.5428 - val_loss: 44.1370\nEpoch 12/50\n4/4 [==============================] - 0s 45ms/step - loss: 73.9559 - val_loss: 50.6629\nEpoch 13/50\n4/4 [==============================] - 0s 43ms/step - loss: 67.6072 - val_loss: 32.3009\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 75: DatepartRegression\nModel Number: 76 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 76: VECM\nModel Number: 77 with model MultivariateMotif in generation 0 of 10\nModel Number: 78 with model LastValueNaive in generation 0 of 10\nModel Number: 79 with model GLS in generation 0 of 10\nModel Number: 80 with model WindowRegression in generation 0 of 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000175 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nModel Number: 81 with model AverageValueNaive in generation 0 of 10\nModel Number: 82 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 82: GluonTS\nModel Number: 83 with model ETS in generation 0 of 10\nTemplate Eval Error: ValueError('Model ETS returned NaN for one or more series. fail_on_forecast_nan=True') in model 83: ETS\nModel Number: 84 with model UnobservedComponents in generation 0 of 10\nModel Number: 85 with model LastValueNaive in generation 0 of 10\nModel Number: 86 with model WindowRegression in generation 0 of 10\nModel Number: 87 with model SectionalMotif in generation 0 of 10\nModel Number: 88 with model LastValueNaive in generation 0 of 10\nModel Number: 89 with model SectionalMotif in generation 0 of 10\nModel Number: 90 with model WindowRegression in generation 0 of 10\nModel Number: 91 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 91: VECM\nModel Number: 92 with model GLS in generation 0 of 10\nModel Number: 93 with model ZeroesNaive in generation 0 of 10\nModel Number: 94 with model UnobservedComponents in generation 0 of 10\nModel Number: 95 with model UnivariateMotif in generation 0 of 10\nModel Number: 96 with model WindowRegression in generation 0 of 10\nModel Number: 97 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 97: GluonTS\nModel Number: 98 with model WindowRegression in generation 0 of 10\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\nTemplate Eval Error: LightGBMError('[tweedie]: at least one target label is negative') in model 98: WindowRegression\nModel Number: 99 with model FBProphet in generation 0 of 10\n","output_type":"stream"},{"name":"stderr","text":"[LightGBM] [Fatal] [tweedie]: at least one target label is negative\n","output_type":"stream"},{"name":"stdout","text":"Initial log joint probability = -2.10007\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       939.991   0.000677466       4251.34           1           1      129   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       941.184   0.000526157       5099.75      0.4694           1      236   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       941.661   3.86708e-05       1803.72           1           1      344   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       941.849   4.99062e-06       403.664      0.3057           1      455   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     499        941.88   1.20694e-05       59.7648           1           1      566   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     599       941.888    7.9971e-07       61.3673           1           1      672   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     612       941.889   3.41101e-07       20.6153           1           1      685   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 100 with model GLS in generation 0 of 10\nModel Number: 101 with model UnobservedComponents in generation 0 of 10\nModel Number: 102 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 102: GluonTS\nModel Number: 103 with model SeasonalNaive in generation 0 of 10\nModel Number: 104 with model UnobservedComponents in generation 0 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 104: UnobservedComponents\nModel Number: 105 with model NVAR in generation 0 of 10\nModel Number: 106 with model FBProphet in generation 0 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 106: FBProphet\nModel Number: 107 with model Theta in generation 0 of 10\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  4.82534D+00    |proj g|=  2.02652D-03\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      3      4      1     0     0   4.936D-07   4.824D+00\n  F =   4.8242590383654225     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \nModel Number: 108 with model UnobservedComponents in generation 0 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 108: UnobservedComponents\nModel Number: 109 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 109: VECM\nModel Number: 110 with model MultivariateRegression in generation 0 of 10\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000089 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","output_type":"stream"},{"name":"stderr","text":" This problem is unconstrained.\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 111 with model MultivariateMotif in generation 0 of 10\nModel Number: 112 with model DatepartRegression in generation 0 of 10\nEpoch 1/100\n2/2 [==============================] - 8s 791ms/step - loss: 38.9223 - val_loss: 33.4226\nEpoch 2/100\n2/2 [==============================] - 0s 112ms/step - loss: 38.7913 - val_loss: 33.4263\nEpoch 3/100\n2/2 [==============================] - 0s 125ms/step - loss: 38.8397 - val_loss: 33.4266\nEpoch 4/100\n2/2 [==============================] - 0s 97ms/step - loss: 38.8782 - val_loss: 33.4093\nEpoch 5/100\n2/2 [==============================] - 0s 107ms/step - loss: 38.9548 - val_loss: 33.4204\nEpoch 6/100\n2/2 [==============================] - 0s 104ms/step - loss: 38.8427 - val_loss: 33.4187\nEpoch 7/100\n2/2 [==============================] - 0s 104ms/step - loss: 38.9971 - val_loss: 33.4475\nEpoch 8/100\n2/2 [==============================] - 0s 116ms/step - loss: 38.8288 - val_loss: 33.4598\nEpoch 9/100\n2/2 [==============================] - 0s 111ms/step - loss: 38.8996 - val_loss: 33.4777\nEpoch 10/100\n2/2 [==============================] - 0s 107ms/step - loss: 38.8777 - val_loss: 33.4538\nEpoch 11/100\n2/2 [==============================] - 0s 108ms/step - loss: 38.9313 - val_loss: 33.4772\nEpoch 12/100\n2/2 [==============================] - 0s 102ms/step - loss: 38.8046 - val_loss: 33.4760\nEpoch 13/100\n2/2 [==============================] - 0s 109ms/step - loss: 38.7797 - val_loss: 33.4524\nEpoch 14/100\n2/2 [==============================] - 0s 114ms/step - loss: 38.7142 - val_loss: 33.4659\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 112: DatepartRegression\nModel Number: 113 with model ZeroesNaive in generation 0 of 10\nModel Number: 114 with model SeasonalNaive in generation 0 of 10\nModel Number: 115 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 115: GluonTS\nModel Number: 116 with model SeasonalNaive in generation 0 of 10\nModel Number: 117 with model GLS in generation 0 of 10\nModel Number: 118 with model AverageValueNaive in generation 0 of 10\nModel Number: 119 with model SeasonalNaive in generation 0 of 10\nModel Number: 120 with model NVAR in generation 0 of 10\nModel Number: 121 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 121: VECM\nModel Number: 122 with model MultivariateMotif in generation 0 of 10\nModel Number: 123 with model LastValueNaive in generation 0 of 10\nModel Number: 124 with model ARDL in generation 0 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 124: ARDL\nModel Number: 125 with model MultivariateMotif in generation 0 of 10\nModel Number: 126 with model FBProphet in generation 0 of 10\nInitial log joint probability = -3.46048\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n       1      -3.46048             0       134.027       1e-12       0.001       22   \nOptimization terminated with error: \n  Line search failed to achieve a sufficient decrease, no more progress can be made\n\nInitial log joint probability = -87.431\nIteration  1. Log joint probability =    80.6093. Improved by 168.04.\nIteration  2. Log joint probability =    202.536. Improved by 121.926.\nIteration  3. Log joint probability =    247.404. Improved by 44.8682.\nIteration  4. Log joint probability =    259.551. Improved by 12.147.\nIteration  5. Log joint probability =    266.719. Improved by 7.16796.\nIteration  6. Log joint probability =    274.333. Improved by 7.61422.\nIteration  7. Log joint probability =    275.053. Improved by 0.720477.\nIteration  8. Log joint probability =    275.495. Improved by 0.441258.\nIteration  9. Log joint probability =    275.819. Improved by 0.323853.\nIteration 10. Log joint probability =    275.976. Improved by 0.15773.\nIteration 11. Log joint probability =    276.041. Improved by 0.0650328.\nIteration 12. Log joint probability =    276.058. Improved by 0.0167117.\nIteration 13. Log joint probability =    276.088. Improved by 0.0301461.\nIteration 14. Log joint probability =    276.093. Improved by 0.00525438.\nIteration 15. Log joint probability =    276.113. Improved by 0.019653.\nIteration 16. Log joint probability =    276.121. Improved by 0.00745614.\nIteration 17. Log joint probability =    276.122. Improved by 0.00116718.\nIteration 18. Log joint probability =    276.126. Improved by 0.00447049.\nIteration 19. Log joint probability =    276.126. Improved by 0.00011495.\nIteration 20. Log joint probability =    276.129. Improved by 0.00232232.\nIteration 21. Log joint probability =    276.129. Improved by 0.000318661.\nIteration 22. Log joint probability =     276.13. Improved by 0.00121374.\nIteration 23. Log joint probability =    276.131. Improved by 0.000620023.\nIteration 24. Log joint probability =    276.131. Improved by 0.00025231.\nIteration 25. Log joint probability =    276.131. Improved by 0.000146757.\nIteration 26. Log joint probability =    276.131. Improved by 8.73737e-05.\nIteration 27. Log joint probability =    276.131. Improved by 2.38214e-05.\nIteration 28. Log joint probability =    276.131. Improved by 3.69216e-06.\nIteration 29. Log joint probability =    276.131. Improved by 1.89997e-05.\nIteration 30. Log joint probability =    276.131. Improved by 9.16625e-06.\nIteration 31. Log joint probability =    276.131. Improved by 8.1889e-08.\nIteration 32. Log joint probability =    276.131. Improved by 5.57141e-08.\nIteration 33. Log joint probability =    276.131. Improved by 2.14785e-07.\nIteration 34. Log joint probability =    276.131. Improved by 1.62973e-08.\nIteration 35. Log joint probability =    276.131. Improved by 3.03009e-07.\nIteration 36. Log joint probability =    276.131. Improved by 3.69204e-06.\nIteration 37. Log joint probability =    276.131. Improved by 2.20393e-06.\nIteration 38. Log joint probability =    276.131. Improved by 1.42038e-06.\nIteration 39. Log joint probability =    276.131. Improved by 1.01577e-06.\nIteration 40. Log joint probability =    276.131. Improved by 2.41761e-07.\nIteration 41. Log joint probability =    276.131. Improved by 5.16623e-07.\nIteration 42. Log joint probability =    276.131. Improved by 2.2117e-08.\nIteration 43. Log joint probability =    276.131. Improved by 3.32035e-07.\nIteration 44. Log joint probability =    276.131. Improved by 1.06447e-07.\nIteration 45. Log joint probability =    276.131. Improved by 1.47901e-08.\nIteration 46. Log joint probability =    276.131. Improved by 7.04543e-08.\nIteration 47. Log joint probability =    276.131. Improved by 4.47455e-08.\nIteration 48. Log joint probability =    276.131. Improved by 1.04324e-08.\nIteration 49. Log joint probability =    276.131. Improved by 3.68334e-09.\nModel Number: 127 with model VAR in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 127: VAR\nModel Number: 128 with model GluonTS in generation 0 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 128: GluonTS\nModel Number: 129 with model MultivariateMotif in generation 0 of 10\nModel Number: 130 with model Theta in generation 0 of 10\nModel Number: 131 with model VAR in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 131: VAR\nModel Number: 132 with model NVAR in generation 0 of 10\nModel Number: 133 with model Theta in generation 0 of 10\nModel Number: 134 with model DatepartRegression in generation 0 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 134: DatepartRegression\nModel Number: 135 with model Theta in generation 0 of 10\nModel Number: 136 with model MultivariateRegression in generation 0 of 10\nModel Number: 137 with model LastValueNaive in generation 0 of 10\nModel Number: 138 with model GLM in generation 0 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 138: GLM\nModel Number: 139 with model UnivariateMotif in generation 0 of 10\nModel Number: 140 with model VAR in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 140: VAR\nModel Number: 141 with model ETS in generation 0 of 10\nModel Number: 142 with model VECM in generation 0 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 142: VECM\nModel Number: 143 with model SeasonalNaive in generation 0 of 10\nModel Number: 144 with model GLM in generation 0 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 144: GLM\nNew Generation: 1 of 10\nModel Number: 145 with model MultivariateMotif in generation 1 of 10\nModel Number: 146 with model MultivariateMotif in generation 1 of 10\nModel Number: 147 with model MultivariateMotif in generation 1 of 10\nModel Number: 148 with model MultivariateMotif in generation 1 of 10\nModel Number: 149 with model MultivariateRegression in generation 1 of 10\nModel Number: 150 with model MultivariateRegression in generation 1 of 10\nModel Number: 151 with model MultivariateRegression in generation 1 of 10\nModel Number: 152 with model MultivariateRegression in generation 1 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 152: MultivariateRegression\nModel Number: 153 with model ETS in generation 1 of 10\nModel Number: 154 with model ETS in generation 1 of 10\nModel Number: 155 with model ETS in generation 1 of 10\nModel Number: 156 with model ETS in generation 1 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1396: RuntimeWarning: All-NaN slice encountered\n  overwrite_input, interpolation)\n","output_type":"stream"},{"name":"stdout","text":"Template Eval Error: ValueError('Model ETS returned NaN for one or more series. fail_on_forecast_nan=True') in model 156: ETS\nModel Number: 157 with model WindowRegression in generation 1 of 10\nModel Number: 158 with model WindowRegression in generation 1 of 10\nEpoch 1/100\n3/3 [==============================] - 6s 7ms/step - loss: 0.1662\nEpoch 2/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.1574\nEpoch 3/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.1479\nEpoch 4/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.1386\nEpoch 5/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.1276\nEpoch 6/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.1186\nEpoch 7/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.1055\nEpoch 8/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0904\nEpoch 9/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0744\nEpoch 10/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0592\nEpoch 11/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0423\nEpoch 12/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0268\nEpoch 13/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0171\nEpoch 14/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0078\nEpoch 15/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0105\nEpoch 16/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0116\nEpoch 17/100\n3/3 [==============================] - 0s 7ms/step - loss: 0.0166\nEpoch 18/100\n3/3 [==============================] - 0s 7ms/step - loss: 0.0112\nEpoch 19/100\n3/3 [==============================] - 0s 7ms/step - loss: 0.0115\nEpoch 20/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0090\nEpoch 21/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0075\nEpoch 22/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0087\nEpoch 23/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0101\nEpoch 24/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0083\nEpoch 25/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0089\nEpoch 26/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0072\nEpoch 27/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0073\nEpoch 28/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0090\nEpoch 29/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0078\nEpoch 30/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0093\nEpoch 31/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0071\nEpoch 32/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0073\nEpoch 33/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0072\nEpoch 34/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0070\nEpoch 35/100\n3/3 [==============================] - 0s 7ms/step - loss: 0.0087\nEpoch 36/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0080\nEpoch 37/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0081\nEpoch 38/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0087\nEpoch 39/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0072\nEpoch 40/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0078\nEpoch 41/100\n3/3 [==============================] - 0s 7ms/step - loss: 0.0084\nEpoch 42/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0072\nEpoch 43/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0078\nEpoch 44/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0080\nEpoch 45/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0089\nEpoch 46/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0073\nEpoch 47/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0077\nEpoch 48/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0077\nEpoch 49/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0068\nEpoch 50/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0063\nEpoch 51/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0079\nEpoch 52/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0070\nEpoch 53/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0067\nEpoch 54/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0069\nEpoch 55/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0086\nEpoch 56/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0075\nEpoch 57/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0059\nEpoch 58/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0067\nEpoch 59/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0073\nEpoch 60/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0062\nEpoch 61/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0073\nEpoch 62/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0066\nEpoch 63/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0060\nEpoch 64/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0050\nEpoch 65/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0066\nEpoch 66/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0072\nEpoch 67/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0058\nEpoch 68/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0078\nEpoch 69/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0064\nEpoch 70/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0054\nEpoch 71/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0055\nEpoch 72/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0067\nEpoch 73/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0057\nEpoch 74/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0055\nEpoch 75/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0060\nEpoch 76/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0052\nEpoch 77/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0063\nEpoch 78/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0047\nEpoch 79/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0065\nEpoch 80/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0059\nEpoch 81/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0066\nEpoch 82/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0064\nEpoch 83/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0055\nEpoch 84/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0040\nEpoch 85/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0043\nEpoch 86/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0037\nEpoch 87/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0047\nEpoch 88/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0061\nEpoch 89/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0053\nEpoch 90/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0053\nEpoch 91/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0061\nEpoch 92/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0061\nEpoch 93/100\n3/3 [==============================] - 0s 7ms/step - loss: 0.0046\nEpoch 94/100\n3/3 [==============================] - ETA: 0s - loss: 0.003 - 0s 6ms/step - loss: 0.0056\nEpoch 95/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0042\nEpoch 96/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0051\nEpoch 97/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0040\nEpoch 98/100\n3/3 [==============================] - 0s 6ms/step - loss: 0.0055\nEpoch 99/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0051\nEpoch 100/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.0058\nModel Number: 159 with model WindowRegression in generation 1 of 10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000058 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nModel Number: 160 with model NVAR in generation 1 of 10\nModel Number: 161 with model NVAR in generation 1 of 10\nModel Number: 162 with model NVAR in generation 1 of 10\nModel Number: 163 with model NVAR in generation 1 of 10\nModel Number: 164 with model LastValueNaive in generation 1 of 10\nModel Number: 165 with model LastValueNaive in generation 1 of 10\nModel Number: 166 with model LastValueNaive in generation 1 of 10\nModel Number: 167 with model ZeroesNaive in generation 1 of 10\nModel Number: 168 with model ZeroesNaive in generation 1 of 10\nModel Number: 169 with model ZeroesNaive in generation 1 of 10\nModel Number: 170 with model UnivariateMotif in generation 1 of 10\nModel Number: 171 with model UnivariateMotif in generation 1 of 10\nModel Number: 172 with model UnivariateMotif in generation 1 of 10\nModel Number: 173 with model UnivariateMotif in generation 1 of 10\nModel Number: 174 with model Theta in generation 1 of 10\nModel Number: 175 with model Theta in generation 1 of 10\nModel Number: 176 with model Theta in generation 1 of 10\nModel Number: 177 with model Theta in generation 1 of 10\nModel Number: 178 with model GLS in generation 1 of 10\nModel Number: 179 with model GLS in generation 1 of 10\nModel Number: 180 with model GLS in generation 1 of 10\nModel Number: 181 with model DatepartRegression in generation 1 of 10\nModel Number: 182 with model DatepartRegression in generation 1 of 10\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 182: DatepartRegression\nModel Number: 183 with model DatepartRegression in generation 1 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neighbors/_regression.py:470: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n  warnings.warn(empty_warning_msg)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 184 with model SeasonalNaive in generation 1 of 10\nModel Number: 185 with model SeasonalNaive in generation 1 of 10\nModel Number: 186 with model SeasonalNaive in generation 1 of 10\nModel Number: 187 with model SeasonalNaive in generation 1 of 10\nModel Number: 188 with model AverageValueNaive in generation 1 of 10\nModel Number: 189 with model AverageValueNaive in generation 1 of 10\nModel Number: 190 with model AverageValueNaive in generation 1 of 10\nModel Number: 191 with model SectionalMotif in generation 1 of 10\nModel Number: 192 with model SectionalMotif in generation 1 of 10\nModel Number: 193 with model SectionalMotif in generation 1 of 10\nModel Number: 194 with model SectionalMotif in generation 1 of 10\nModel Number: 195 with model UnobservedComponents in generation 1 of 10\nModel Number: 196 with model UnobservedComponents in generation 1 of 10\nModel Number: 197 with model UnobservedComponents in generation 1 of 10\nModel Number: 198 with model FBProphet in generation 1 of 10\nInitial log joint probability = -16.4418\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       133.672     0.0115583       237.218      0.2083      0.9752      123   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199        135.72    0.00383991       15.6934           1           1      247   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       136.904    0.00938787       11.6738           1           1      388   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       137.226   0.000518527        11.853      0.4569           1      508   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     499       137.414    0.00250667       49.6367           1           1      625   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     509       137.435   6.63921e-05       20.2009   6.512e-06       0.001      671  LS failed, Hessian reset \n     596       137.444   4.39083e-08        7.8163      0.3662           1      780   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 199 with model FBProphet in generation 1 of 10\nInitial log joint probability = -16.9547\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       152.169    0.00210114       10.1201           1           1      116   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       152.427   7.14634e-05       6.19833     0.08998     0.08998      228   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     273       152.451   0.000439924       10.5814   3.624e-05       0.001      346  LS failed, Hessian reset \n     299       152.456   1.37101e-06       9.77372      0.6895      0.6895      379   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     315       152.456   9.89375e-08       9.16745           1           1      401   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 200 with model FBProphet in generation 1 of 10\nInitial log joint probability = -2.41026\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       375.095     0.0018995       65.0191      0.1394           1      115   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     175       375.756   0.000111534       14.6507   1.513e-05       0.001      240  LS failed, Hessian reset \n     199       375.788   0.000429036       7.63453           1           1      270   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       376.017    0.00118274        21.164           1           1      386   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       376.115   7.69058e-06       6.96588      0.2314      0.9528      510   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     485       376.163   0.000157378       18.6179   8.669e-06       0.001      645  LS failed, Hessian reset \n     499       376.167   1.94545e-05       5.92032       0.445       0.445      659   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     599       376.177   1.86999e-06       6.98871           1           1      788   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     655        376.18   7.31644e-05        8.4343   9.934e-06       0.001      936  LS failed, Hessian reset \n     693       376.181   3.71425e-06       7.90774   4.677e-07       0.001     1024  LS failed, Hessian reset \n     699       376.181   2.71781e-07       5.96527       0.579       0.579     1030   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     701       376.181   8.38997e-08       5.97008      0.4241      0.4241     1033   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 201 with model FBProphet in generation 1 of 10\nInitial log joint probability = -3.51411\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99        213.59    0.00136815       8.66408           1           1      123   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     104       213.607   0.000605206       17.7259   4.956e-05       0.001      163  LS failed, Hessian reset \n     199       213.648   0.000290949       8.59793      0.5097      0.5097      270   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       213.712    0.00195467       9.62795           1           1      383   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     397       214.002    0.00226788       19.4484   0.0001966       0.001      533  LS failed, Hessian reset \n     399       214.009   0.000276657       13.2596       0.704       0.704      535   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     499       214.035   5.16882e-07       9.05272      0.2585           1      651   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     593       214.037   2.73365e-07       9.49927      0.3725           1      770   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 202 with model GLM in generation 1 of 10\nModel Number: 203 with model GLM in generation 1 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 203: GLM\nModel Number: 204 with model GLM in generation 1 of 10\nTemplate Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 204: GLM\nModel Number: 205 with model GLM in generation 1 of 10\nTemplate Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 205: GLM\nModel Number: 206 with model GluonTS in generation 1 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 206: GluonTS\nModel Number: 207 with model GluonTS in generation 1 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 207: GluonTS\nModel Number: 208 with model GluonTS in generation 1 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 208: GluonTS\nModel Number: 209 with model GluonTS in generation 1 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 209: GluonTS\nModel Number: 210 with model VAR in generation 1 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 210: VAR\nModel Number: 211 with model VAR in generation 1 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 211: VAR\nModel Number: 212 with model VAR in generation 1 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 212: VAR\nModel Number: 213 with model VAR in generation 1 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 213: VAR\nModel Number: 214 with model VECM in generation 1 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 214: VECM\nModel Number: 215 with model VECM in generation 1 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 215: VECM\nModel Number: 216 with model VECM in generation 1 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 216: VECM\nModel Number: 217 with model VECM in generation 1 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 217: VECM\nModel Number: 218 with model ARDL in generation 1 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 218: ARDL\nModel Number: 219 with model ARDL in generation 1 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 219: ARDL\nModel Number: 220 with model ARDL in generation 1 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 220: ARDL\nModel Number: 221 with model ARDL in generation 1 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 221: ARDL\nNew Generation: 2 of 10\nModel Number: 222 with model MultivariateMotif in generation 2 of 10\nModel Number: 223 with model MultivariateMotif in generation 2 of 10\nModel Number: 224 with model MultivariateMotif in generation 2 of 10\nModel Number: 225 with model MultivariateMotif in generation 2 of 10\nModel Number: 226 with model UnivariateMotif in generation 2 of 10\nModel Number: 227 with model UnivariateMotif in generation 2 of 10\nModel Number: 228 with model UnivariateMotif in generation 2 of 10\nModel Number: 229 with model UnivariateMotif in generation 2 of 10\nModel Number: 230 with model MultivariateRegression in generation 2 of 10\nModel Number: 231 with model MultivariateRegression in generation 2 of 10\nTemplate Eval Error: ValueError(\"loss='poisson' requires non-negative y and sum(y) > 0.\") in model 231: MultivariateRegression\nModel Number: 232 with model MultivariateRegression in generation 2 of 10\nTemplate Eval Error: ValueError(\"loss='poisson' requires non-negative y and sum(y) > 0.\") in model 232: MultivariateRegression\nModel Number: 233 with model MultivariateRegression in generation 2 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:17: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n  \"Since version 1.0, \"\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 234 with model ETS in generation 2 of 10\nModel Number: 235 with model ETS in generation 2 of 10\nModel Number: 236 with model ETS in generation 2 of 10\nModel Number: 237 with model ETS in generation 2 of 10\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nModel Number: 238 with model WindowRegression in generation 2 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 238: WindowRegression\nModel Number: 239 with model WindowRegression in generation 2 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 239: WindowRegression\nModel Number: 240 with model WindowRegression in generation 2 of 10\nModel Number: 241 with model NVAR in generation 2 of 10\nModel Number: 242 with model NVAR in generation 2 of 10\nModel Number: 243 with model NVAR in generation 2 of 10\nModel Number: 244 with model NVAR in generation 2 of 10\nModel Number: 245 with model LastValueNaive in generation 2 of 10\nModel Number: 246 with model LastValueNaive in generation 2 of 10\nModel Number: 247 with model LastValueNaive in generation 2 of 10\nModel Number: 248 with model ZeroesNaive in generation 2 of 10\nModel Number: 249 with model ZeroesNaive in generation 2 of 10\nModel Number: 250 with model Theta in generation 2 of 10\nModel Number: 251 with model Theta in generation 2 of 10\nModel Number: 252 with model Theta in generation 2 of 10\nModel Number: 253 with model Theta in generation 2 of 10\nModel Number: 254 with model DatepartRegression in generation 2 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 254: DatepartRegression\nModel Number: 255 with model DatepartRegression in generation 2 of 10\nModel Number: 256 with model DatepartRegression in generation 2 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 256: DatepartRegression\nModel Number: 257 with model GLM in generation 2 of 10\nModel Number: 258 with model GLM in generation 2 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 258: GLM\nModel Number: 259 with model GLM in generation 2 of 10\nModel Number: 260 with model GLM in generation 2 of 10\nModel Number: 261 with model GLS in generation 2 of 10\nModel Number: 262 with model GLS in generation 2 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:296: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n  DomainWarning)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 263 with model GLS in generation 2 of 10\nModel Number: 264 with model SeasonalNaive in generation 2 of 10\nModel Number: 265 with model SeasonalNaive in generation 2 of 10\nModel Number: 266 with model SeasonalNaive in generation 2 of 10\nModel Number: 267 with model SeasonalNaive in generation 2 of 10\nModel Number: 268 with model AverageValueNaive in generation 2 of 10\nModel Number: 269 with model AverageValueNaive in generation 2 of 10\nModel Number: 270 with model AverageValueNaive in generation 2 of 10\nModel Number: 271 with model SectionalMotif in generation 2 of 10\nModel Number: 272 with model SectionalMotif in generation 2 of 10\nModel Number: 273 with model SectionalMotif in generation 2 of 10\nModel Number: 274 with model SectionalMotif in generation 2 of 10\nModel Number: 275 with model UnobservedComponents in generation 2 of 10\nModel Number: 276 with model UnobservedComponents in generation 2 of 10\nModel Number: 277 with model UnobservedComponents in generation 2 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 277: UnobservedComponents\nModel Number: 278 with model FBProphet in generation 2 of 10\nInitial log joint probability = -11.2567\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       89.3832    0.00638495       97.0002           1           1      124   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     111       89.6075   0.000581273       101.307   5.769e-06       0.001      188  LS failed, Hessian reset \n     199       89.8621   0.000249592       109.418           1           1      296   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     236       89.8754   3.32694e-06       97.0239   3.074e-08       0.001      379  LS failed, Hessian reset \n     296       89.8772   9.17271e-09       93.4249      0.4135      0.4135      455   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 279 with model FBProphet in generation 2 of 10\nInitial log joint probability = -7.53333\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99        249.05     0.0244686        44.563      0.7891      0.7891      114   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     172       250.754   0.000335448       15.7148   2.228e-05       0.001      242  LS failed, Hessian reset \n     199       251.271    0.00139475       18.2825       0.444           1      279   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       251.681    0.00432209        9.4271      0.9071      0.9071      402   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     374       252.177    0.00117061       16.3611     0.00017       0.001      547  LS failed, Hessian reset \n     399       252.231    0.00149446       9.00563           1           1      576   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     438       252.268   0.000245657       8.93878   2.536e-05       0.001      657  LS failed, Hessian reset \n     488        252.27    7.8594e-06        8.8742   9.449e-07       0.001      768  LS failed, Hessian reset \n     499        252.27   4.85921e-07       5.41194      0.4982      0.4982      781   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     514        252.27   9.07381e-08       5.78533      0.2546           1      803   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 280 with model FBProphet in generation 2 of 10\nInitial log joint probability = -3.51411\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      67       209.341    0.00132374        92.967   1.947e-05       0.001      146  LS failed, Hessian reset \n      99       209.404   1.75002e-06       68.5916      0.3397      0.3397      195   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     135       209.452   0.000145708       63.5337   2.979e-06       0.001      304  LS failed, Hessian reset \n     179       209.461   2.98144e-06       57.5156   4.796e-08       0.001      399  LS failed, Hessian reset \n     199       209.461   1.08071e-07       35.3977           1           1      425   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     213       209.461   2.65196e-08       46.4196           1           1      443   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 281 with model FBProphet in generation 2 of 10\nInitial log joint probability = -2.88149\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      68       246.958   0.000808495       11.4302   6.427e-05       0.001      117  LS failed, Hessian reset \n      99       247.019   0.000132708       5.12131      0.9412      0.9412      164   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     106       247.056   0.000583218       7.39755   2.493e-05       0.001      210  LS failed, Hessian reset \n     134       247.072   0.000346238       6.47474   6.057e-05       0.001      297  LS failed, Hessian reset \n     199        247.11    0.00010161       5.38029      0.9052      0.9052      390   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     227       247.119   0.000481682       10.2811   6.457e-05       0.001      481  LS failed, Hessian reset \n     263       247.122   1.73377e-05       4.92432   2.591e-06       0.001      568  LS failed, Hessian reset \n     286       247.122   2.23282e-07       4.33548      0.4075           1      606   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 282 with model GluonTS in generation 2 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 282: GluonTS\nModel Number: 283 with model GluonTS in generation 2 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 283: GluonTS\nModel Number: 284 with model GluonTS in generation 2 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 284: GluonTS\nModel Number: 285 with model GluonTS in generation 2 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 285: GluonTS\nModel Number: 286 with model VAR in generation 2 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 286: VAR\nModel Number: 287 with model VAR in generation 2 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 287: VAR\nModel Number: 288 with model VAR in generation 2 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 288: VAR\nModel Number: 289 with model VAR in generation 2 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 289: VAR\nModel Number: 290 with model VECM in generation 2 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 290: VECM\nModel Number: 291 with model VECM in generation 2 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 291: VECM\nModel Number: 292 with model VECM in generation 2 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 292: VECM\nModel Number: 293 with model VECM in generation 2 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 293: VECM\nModel Number: 294 with model ARDL in generation 2 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 294: ARDL\nModel Number: 295 with model ARDL in generation 2 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 295: ARDL\nModel Number: 296 with model ARDL in generation 2 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 296: ARDL\nModel Number: 297 with model ARDL in generation 2 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 297: ARDL\nNew Generation: 3 of 10\nModel Number: 298 with model MultivariateMotif in generation 3 of 10\nModel Number: 299 with model MultivariateMotif in generation 3 of 10\nModel Number: 300 with model MultivariateMotif in generation 3 of 10\nModel Number: 301 with model MultivariateMotif in generation 3 of 10\nModel Number: 302 with model UnivariateMotif in generation 3 of 10\nModel Number: 303 with model UnivariateMotif in generation 3 of 10\nModel Number: 304 with model UnivariateMotif in generation 3 of 10\nModel Number: 305 with model UnivariateMotif in generation 3 of 10\nModel Number: 306 with model NVAR in generation 3 of 10\nModel Number: 307 with model NVAR in generation 3 of 10\nModel Number: 308 with model NVAR in generation 3 of 10\nModel Number: 309 with model NVAR in generation 3 of 10\nModel Number: 310 with model MultivariateRegression in generation 3 of 10\nModel Number: 311 with model MultivariateRegression in generation 3 of 10\nModel Number: 312 with model MultivariateRegression in generation 3 of 10\nModel Number: 313 with model MultivariateRegression in generation 3 of 10\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=-2)]: Done 300 out of 300 | elapsed:    0.3s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 314 with model ETS in generation 3 of 10\nModel Number: 315 with model ETS in generation 3 of 10\nModel Number: 316 with model ETS in generation 3 of 10\nModel Number: 317 with model ETS in generation 3 of 10\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\nModel Number: 318 with model WindowRegression in generation 3 of 10\nModel Number: 319 with model WindowRegression in generation 3 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 320 with model WindowRegression in generation 3 of 10\nModel Number: 321 with model LastValueNaive in generation 3 of 10\nModel Number: 322 with model LastValueNaive in generation 3 of 10\nModel Number: 323 with model LastValueNaive in generation 3 of 10\nModel Number: 324 with model ZeroesNaive in generation 3 of 10\nModel Number: 325 with model ZeroesNaive in generation 3 of 10\nModel Number: 326 with model ZeroesNaive in generation 3 of 10\nModel Number: 327 with model Theta in generation 3 of 10\nModel Number: 328 with model Theta in generation 3 of 10\nModel Number: 329 with model Theta in generation 3 of 10\nModel Number: 330 with model Theta in generation 3 of 10\nModel Number: 331 with model DatepartRegression in generation 3 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 331: DatepartRegression\nModel Number: 332 with model DatepartRegression in generation 3 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 332: DatepartRegression\nModel Number: 333 with model DatepartRegression in generation 3 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 333: DatepartRegression\nModel Number: 334 with model GLM in generation 3 of 10\nModel Number: 335 with model GLM in generation 3 of 10\nTemplate Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 335: GLM\nModel Number: 336 with model GLM in generation 3 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:296: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n  DomainWarning)\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:428: RuntimeWarning: divide by zero encountered in true_divide\n  endog_mu = self._clean(endog / mu)\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:428: RuntimeWarning: overflow encountered in true_divide\n  endog_mu = self._clean(endog / mu)\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:134: RuntimeWarning: divide by zero encountered in true_divide\n  return 1. / (self.link.deriv(mu)**2 * self.variance(mu))\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:296: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n  DomainWarning)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 337 with model GLM in generation 3 of 10\nModel Number: 338 with model FBProphet in generation 3 of 10\nInitial log joint probability = -8.8886\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       144.616    0.00255273       6.33886           1           1      118   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     129        144.65   0.000637661       7.19608   7.496e-05       0.001      194  LS failed, Hessian reset \n     155       144.651   1.18343e-05       5.98777   1.933e-06       0.001      279  LS failed, Hessian reset \n     176       144.651   1.50797e-07       4.57848      0.3024      0.3024      324   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 339 with model FBProphet in generation 3 of 10\nInitial log joint probability = -9.6422\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       92.0132   0.000145935       4.19533      0.7815      0.7815      119   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     188       92.0445   0.000273667       4.82647   4.972e-05       0.001      258  LS failed, Hessian reset \n     199       92.0453   2.46962e-05       5.34161      0.9085      0.9085      270   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       92.0458   9.45964e-07       4.92161      0.9896      0.9896      399   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       92.0464   1.13897e-06       4.11991      0.5415      0.5415      521   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     441       92.0465   7.49536e-06       6.29391   1.636e-06       0.001      603  LS failed, Hessian reset \n     460       92.0465   9.68364e-08       3.63661      0.9332      0.9332      628   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 340 with model FBProphet in generation 3 of 10\nInitial log joint probability = -19.7881\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       46.5909   0.000637101       3.43099      0.4908      0.4908      120   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     162       46.7483    0.00433914       4.87311       0.001       0.001      269  LS failed, Hessian reset \n     199       46.7977   1.18785e-06       3.82712      0.2433      0.2433      320   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     221       46.7978   5.70501e-08       3.22477           1           1      354   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 341 with model FBProphet in generation 3 of 10\nInitial log joint probability = -41.9767\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99        137.31     0.0201411       17.7616           1           1      121   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       137.707    0.00240949       8.43335      0.4891           1      237   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     202       137.707   0.000251213       10.9322   2.959e-05       0.001      287  LS failed, Hessian reset \n     284        137.71    8.0999e-08       7.63548      0.7546      0.7546      387   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 342 with model GLS in generation 3 of 10\nModel Number: 343 with model GLS in generation 3 of 10\nModel Number: 344 with model GLS in generation 3 of 10\nModel Number: 345 with model SeasonalNaive in generation 3 of 10\nModel Number: 346 with model SeasonalNaive in generation 3 of 10\nModel Number: 347 with model SeasonalNaive in generation 3 of 10\nModel Number: 348 with model SeasonalNaive in generation 3 of 10\nModel Number: 349 with model AverageValueNaive in generation 3 of 10\nModel Number: 350 with model AverageValueNaive in generation 3 of 10\nModel Number: 351 with model AverageValueNaive in generation 3 of 10\nModel Number: 352 with model UnobservedComponents in generation 3 of 10\nModel Number: 353 with model UnobservedComponents in generation 3 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 353: UnobservedComponents\nModel Number: 354 with model UnobservedComponents in generation 3 of 10\nModel Number: 355 with model SectionalMotif in generation 3 of 10\nModel Number: 356 with model SectionalMotif in generation 3 of 10\nModel Number: 357 with model SectionalMotif in generation 3 of 10\nModel Number: 358 with model SectionalMotif in generation 3 of 10\nModel Number: 359 with model GluonTS in generation 3 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 359: GluonTS\nModel Number: 360 with model GluonTS in generation 3 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 360: GluonTS\nModel Number: 361 with model GluonTS in generation 3 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 361: GluonTS\nModel Number: 362 with model GluonTS in generation 3 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 362: GluonTS\nModel Number: 363 with model VAR in generation 3 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 363: VAR\nModel Number: 364 with model VAR in generation 3 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 364: VAR\nModel Number: 365 with model VAR in generation 3 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 365: VAR\nModel Number: 366 with model VAR in generation 3 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 366: VAR\nModel Number: 367 with model VECM in generation 3 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 367: VECM\nModel Number: 368 with model VECM in generation 3 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 368: VECM\nModel Number: 369 with model VECM in generation 3 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 369: VECM\nModel Number: 370 with model VECM in generation 3 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 370: VECM\nModel Number: 371 with model ARDL in generation 3 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 371: ARDL\nModel Number: 372 with model ARDL in generation 3 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 372: ARDL\nModel Number: 373 with model ARDL in generation 3 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 373: ARDL\nModel Number: 374 with model ARDL in generation 3 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 374: ARDL\nNew Generation: 4 of 10\nModel Number: 375 with model MultivariateMotif in generation 4 of 10\nModel Number: 376 with model MultivariateMotif in generation 4 of 10\nModel Number: 377 with model MultivariateMotif in generation 4 of 10\nModel Number: 378 with model MultivariateMotif in generation 4 of 10\nModel Number: 379 with model UnivariateMotif in generation 4 of 10\nModel Number: 380 with model UnivariateMotif in generation 4 of 10\nModel Number: 381 with model UnivariateMotif in generation 4 of 10\nModel Number: 382 with model UnivariateMotif in generation 4 of 10\nModel Number: 383 with model NVAR in generation 4 of 10\nModel Number: 384 with model NVAR in generation 4 of 10\nModel Number: 385 with model NVAR in generation 4 of 10\nModel Number: 386 with model NVAR in generation 4 of 10\nModel Number: 387 with model MultivariateRegression in generation 4 of 10\nModel Number: 388 with model MultivariateRegression in generation 4 of 10\nModel Number: 389 with model MultivariateRegression in generation 4 of 10\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.2s\n[Parallel(n_jobs=-2)]: Done 300 out of 300 | elapsed:    0.3s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 300 out of 300 | elapsed:    0.0s finished\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 390 with model MultivariateRegression in generation 4 of 10\nModel Number: 391 with model ETS in generation 4 of 10\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nModel Number: 392 with model ETS in generation 4 of 10\nModel Number: 393 with model ETS in generation 4 of 10\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nModel Number: 394 with model ETS in generation 4 of 10\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nModel Number: 395 with model WindowRegression in generation 4 of 10\nModel Number: 396 with model WindowRegression in generation 4 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 397 with model WindowRegression in generation 4 of 10\nModel Number: 398 with model LastValueNaive in generation 4 of 10\nModel Number: 399 with model LastValueNaive in generation 4 of 10\nModel Number: 400 with model LastValueNaive in generation 4 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 401 with model ZeroesNaive in generation 4 of 10\nModel Number: 402 with model ZeroesNaive in generation 4 of 10\nModel Number: 403 with model ZeroesNaive in generation 4 of 10\nModel Number: 404 with model SectionalMotif in generation 4 of 10\nModel Number: 405 with model SectionalMotif in generation 4 of 10\nModel Number: 406 with model SectionalMotif in generation 4 of 10\nModel Number: 407 with model SectionalMotif in generation 4 of 10\nModel Number: 408 with model SeasonalNaive in generation 4 of 10\nModel Number: 409 with model SeasonalNaive in generation 4 of 10\nModel Number: 410 with model SeasonalNaive in generation 4 of 10\nModel Number: 411 with model SeasonalNaive in generation 4 of 10\nModel Number: 412 with model Theta in generation 4 of 10\nModel Number: 413 with model Theta in generation 4 of 10\nModel Number: 414 with model Theta in generation 4 of 10\nModel Number: 415 with model Theta in generation 4 of 10\nModel Number: 416 with model FBProphet in generation 4 of 10\nInitial log joint probability = -9.69977\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       195.825     0.0260162        90.513      0.1507           1      125   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199        211.82     0.0873191       310.743           1           1      254   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       219.934     0.0323827       270.184           1           1      381   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       225.471   0.000562659       28.2553           1           1      505   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     499       229.077    0.00760834       150.381           1           1      631   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     599       231.844    0.00682149       274.842      0.3824           1      751   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     699       235.899    0.00279144       69.1814           1           1      878   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     799       236.936    0.00157202        46.352           1           1     1008   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     899       237.434    0.00178178       197.975      0.2157      0.2157     1126   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     999       237.913   0.000223327       19.2212           1           1     1243   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1099       238.238    0.00124517       43.7723           1           1     1371   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1199        238.98      0.017075       112.287           1           1     1498   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1299       239.771     0.0012478       33.1707           1           1     1624   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1399       240.102   0.000148543       42.5892     0.02256      0.8044     1739   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1499       240.336   0.000260941       15.2933           1           1     1859   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1599       240.656    0.00199206       20.9337           1           1     1980   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1699       241.174     0.0015992       22.7888           1           1     2100   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1799       241.578    0.00396576       91.3319      0.4778      0.4778     2234   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1899       242.029    0.00499039       93.0627           1           1     2355   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1999       242.247   0.000145128       22.7348      0.5269      0.5269     2479   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2099       242.386    0.00720974       33.0577       1.482      0.1482     2605   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2199        242.75    0.00338686        18.226           1           1     2718   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2259       242.858    4.7337e-06       10.9585   1.241e-07       0.001     2838  LS failed, Hessian reset \n    2299       242.937   0.000190684       40.9373           1           1     2897   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2399       243.035   0.000240709        11.673           1           1     3022   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2499       243.196   1.37279e-05        7.3713           1           1     3145   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2599       243.286   0.000276404       19.6917           1           1     3268   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2699       243.322    0.00029573       49.5653           1           1     3389   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2783       243.426    1.7322e-06       6.29855   1.399e-07       0.001     3565  LS failed, Hessian reset \n    2799       243.437   0.000176157       14.3039           1           1     3585   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2886       243.513   5.73277e-06       11.1885   1.165e-07       0.001     3740  LS failed, Hessian reset \n    2899       243.525   4.35046e-05       29.5278      0.3992     0.03992     3757   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2999         243.6   5.08022e-05       55.4361       0.187       0.187     3876   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3099       243.744   0.000369517       107.212      0.1484      0.9747     4002   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3199       243.987     0.0428147       58.2258           1           1     4126   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3299        244.43   0.000309453       12.5305           1           1     4253   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3399       244.811    0.00127628       227.386      0.4058           1     4378   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3499       245.388    0.00269001       195.224      0.2673           1     4508   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3599       246.065    0.00041219       82.1791      0.1025           1     4635   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3699       246.247   0.000727853       73.6048       0.776       0.776     4755   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3799        246.33   2.96393e-05       39.4869      0.5649      0.5649     4888   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3899       246.423    0.00121933       38.6173      0.9713     0.09713     5012   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3999       246.622     0.0048433       140.097           1           1     5146   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4099        246.85   3.08872e-05       6.68333           1           1     5271   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4199       246.996   0.000880827       126.104      0.1359      0.1359     5399   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4299       247.182   6.00119e-05       78.1908      0.5317      0.5317     5522   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4399       247.319    0.00451458        25.627           1           1     5635   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4499       247.582    0.00214302       57.0542           1           1     5767   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4599        247.72   0.000668157       49.8079      0.3877           1     5882   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4699       247.798   0.000621539       145.302      0.3931           1     5998   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4799       247.925   0.000491686       12.4966       1.134      0.1134     6120   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4887       247.992   2.80543e-06       14.4196   4.059e-07       0.001     6271  LS failed, Hessian reset \n    4899       247.996   1.39736e-05       8.10531           1           1     6285   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4999       248.026   2.61402e-05        14.532           1           1     6404   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5007       248.028   1.05599e-06       6.92004   1.283e-07       0.001     6458  LS failed, Hessian reset \n    5072       248.039   1.42336e-06       8.82978   1.222e-07       0.001     6577  LS failed, Hessian reset \n    5099       248.043   3.19083e-05       9.71031           1           1     6612   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5199       248.104   4.39688e-05        161.97    0.002829           1     6752   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5299       248.144   0.000336087       11.1981           1           1     6888   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5390       248.158   3.05036e-06       17.6876   2.912e-07       0.001     7044  LS failed, Hessian reset \n    5399       248.161   0.000149822        12.761           1           1     7056   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5499       248.176   0.000630728       20.8797     0.00854      0.8004     7193   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5599       248.257    0.00225911       195.576           1           1     7311   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5647       248.571   3.64105e-06       15.7031   7.477e-08       0.001     7419  LS failed, Hessian reset \n    5699       248.724   9.03276e-05       45.2086           1           1     7484   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5799       248.992   2.18255e-05        27.082           1           1     7604   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5899       249.076   0.000192273       29.2714      0.4485           1     7724   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5999       249.131    0.00232777       52.1734           1           1     7843   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6099        249.19   0.000267258       9.33409           1           1     7970   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6199       249.248   9.13427e-05       54.5926           1           1     8104   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6299       249.273   0.000471274        60.385           1           1     8231   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6323       249.282   2.76179e-06       15.1034   7.912e-08       0.001     8296  LS failed, Hessian reset \n    6399       249.305   9.27872e-05        4.8956           1           1     8390   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6499       249.329    0.00011947       33.6067           1           1     8517   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6511       249.335   1.12868e-06       7.20707   8.775e-08       0.001     8575  LS failed, Hessian reset \n    6599       249.351   2.87623e-05       8.61686           1           1     8691   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6638       249.353   1.60159e-06       6.24557   7.191e-08       0.001     8790  LS failed, Hessian reset \n    6699       249.363   0.000257399       33.7143      0.4951      0.4951     8869   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6799        249.47    0.00165515        106.01      0.4411     0.04411     8986   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6899         249.8    0.00268644       291.591           1           1     9111   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6999       250.129   0.000651991       106.341      0.8538      0.8538     9227   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7099       250.398    0.00271295       32.6262           1           1     9354   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7199        250.66   0.000253285       28.0591           1           1     9478   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7299       250.751    0.00113363       87.9523           1           1     9597   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7337       250.838   3.22989e-06        26.758   1.458e-07       0.001     9694  LS failed, Hessian reset \n    7399       250.936    0.00154157       239.363           1           1     9769   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7499           251   0.000542629       9.35399          10           1     9900   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7599        251.03   1.49671e-05       18.1238      0.2375      0.2375    10018   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7699       251.087   0.000423421       29.3239           1           1    10134   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7799        251.19   0.000222787       9.53713      0.6855      0.6855    10255   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7899       251.316   0.000984309       173.958      0.3437     0.03437    10378   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7999        251.39   0.000327927       22.2968           1           1    10498   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8099       251.496   0.000507256       36.0724      0.1964           1    10623   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8199       251.561   0.000118254       65.5089           1           1    10748   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8299       251.596   0.000148997       18.1418           1           1    10880   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8399       251.629     4.446e-05       22.7404      0.0669      0.8117    11009   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8443       251.648    9.3735e-07       8.63061   9.705e-08       0.001    11107  LS failed, Hessian reset \n    8499       251.664   5.57519e-06       8.93445           1           1    11170   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8552        251.67   4.43714e-06       16.0546   1.017e-06       0.001    11282  LS failed, Hessian reset \n    8599       251.673   0.000121135       47.7553      0.3109      0.6311    11348   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8699       251.712   0.000329259       22.8072           1           1    11468   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8799       251.739   3.39105e-05         7.285      0.3962     0.03962    11595   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8899       251.754   0.000459971       63.1811           1           1    11717   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8999       251.885    0.00504916       182.978     0.04448      0.6914    11840   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9099        252.11   0.000508648       82.0797      0.4788      0.4788    11978   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9199       252.651     0.0101249       416.701           1           1    12106   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9299       253.147   0.000962927       52.7939          10           1    12231   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9399        253.45    0.00518155       64.9833           1           1    12358   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9499       253.825    0.00644273       55.8407           1           1    12484   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9599       254.149     0.0012983       70.1787           1           1    12599   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9699       254.375    0.00168582       34.4002      0.2067      0.2067    12718   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9799       254.543    0.00339354       173.543           1           1    12831   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9899       254.719    0.00250905       253.545      0.2507      0.2507    12950   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9999       254.792   8.83281e-05        111.14           1           1    13066   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n   10000       254.793   0.000245315       146.903           1           1    13067   \nOptimization terminated normally: \n  Maximum number of iterations hit, may not be at an optima\nModel Number: 417 with model FBProphet in generation 4 of 10\nInitial log joint probability = -8.15183\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       192.878   3.74262e-05       2.81909           1           1      126   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     181        192.91   0.000524511       4.12251   0.0001497       0.001      262  LS failed, Hessian reset \n     199       192.914   0.000303812       4.43947           1           1      283   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     243       192.918   7.92514e-08       3.07901      0.1405           1      351   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 418 with model FBProphet in generation 4 of 10\nInitial log joint probability = -3.99297\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       222.628     0.0233567       13.3375           1           1      122   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       223.254    0.00260075       4.54368      0.5698      0.5698      241   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     286       223.288   2.43725e-07       4.31978      0.2483           1      347   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 419 with model FBProphet in generation 4 of 10\nInitial log joint probability = -4.42062\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       252.975   0.000488492       3.95457      0.2518           1      125   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       253.022    8.0418e-06       3.11658      0.4118           1      248   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     233       253.022   5.95294e-05       3.56965   1.341e-05       0.001      338  LS failed, Hessian reset \n     288       253.023   3.92697e-07       3.18553     0.05777      0.6739      407   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 420 with model DatepartRegression in generation 4 of 10\nEpoch 1/200\n5/5 [==============================] - 7s 6ms/step - loss: 100.1105\nEpoch 2/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.1006\nEpoch 3/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.5931\nEpoch 4/200\n5/5 [==============================] - 0s 5ms/step - loss: 103.2675\nEpoch 5/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.6599\nEpoch 6/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.9506\nEpoch 7/200\n5/5 [==============================] - 0s 5ms/step - loss: 101.6513\nEpoch 8/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.7944\nEpoch 9/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.0031\nEpoch 10/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.8435\nEpoch 11/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.5148\nEpoch 12/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.8821\nEpoch 13/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.5592\nEpoch 14/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.7361\nEpoch 15/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.1335\nEpoch 16/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.4868\nEpoch 17/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.9492\nEpoch 18/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.5453\nEpoch 19/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.8571\nEpoch 20/200\n5/5 [==============================] - 0s 6ms/step - loss: 100.3080\nEpoch 21/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7039\nEpoch 22/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.5359\nEpoch 23/200\n5/5 [==============================] - 0s 6ms/step - loss: 97.3912\nEpoch 24/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.1306\nEpoch 25/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.7991\nEpoch 26/200\n5/5 [==============================] - 0s 5ms/step - loss: 97.4592\nEpoch 27/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.9178\nEpoch 28/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.0217\nEpoch 29/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.0610\nEpoch 30/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.7680\nEpoch 31/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.4846\nEpoch 32/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.1351\nEpoch 33/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.4592\nEpoch 34/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.1494\nEpoch 35/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.7241\nEpoch 36/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.3493\nEpoch 37/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.4826\nEpoch 38/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.9407\nEpoch 39/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.9050\nEpoch 40/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.9408\nEpoch 41/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.3814\nEpoch 42/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.0999\nEpoch 43/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.0879\nEpoch 44/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.1653\nEpoch 45/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.7767\nEpoch 46/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.9227\nEpoch 47/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.8328\nEpoch 48/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.8165\nEpoch 49/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.7174\nEpoch 50/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.0905\nEpoch 51/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.0726\nEpoch 52/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9784\nEpoch 53/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.0326\nEpoch 54/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.8738\nEpoch 55/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.7052\nEpoch 56/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.2802\nEpoch 57/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.3904\nEpoch 58/200\n5/5 [==============================] - 0s 6ms/step - loss: 100.1089\nEpoch 59/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7803\nEpoch 60/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6022\nEpoch 61/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.8614\nEpoch 62/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.5392\nEpoch 63/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.4297\nEpoch 64/200\n5/5 [==============================] - 0s 6ms/step - loss: 101.0321\nEpoch 65/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.1905\nEpoch 66/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.4459\nEpoch 67/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9312\nEpoch 68/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6016\nEpoch 69/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.2565\nEpoch 70/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6521\nEpoch 71/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.8543\nEpoch 72/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.8572\nEpoch 73/200\n5/5 [==============================] - 0s 6ms/step - loss: 100.5366\nEpoch 74/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.7572\nEpoch 75/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.2689\nEpoch 76/200\n5/5 [==============================] - 0s 5ms/step - loss: 97.4634\nEpoch 77/200\n5/5 [==============================] - 0s 5ms/step - loss: 100.5770\nEpoch 78/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6303\nEpoch 79/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.4004\nEpoch 80/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.3132\nEpoch 81/200\n5/5 [==============================] - 0s 12ms/step - loss: 99.2699\nEpoch 82/200\n5/5 [==============================] - 0s 13ms/step - loss: 99.4415\nEpoch 83/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3785\nEpoch 84/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.7117\nEpoch 85/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6242\nEpoch 86/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0153\nEpoch 87/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.9866\nEpoch 88/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0938\nEpoch 89/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3091\nEpoch 90/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7209\nEpoch 91/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.6866\nEpoch 92/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9107\nEpoch 93/200\n5/5 [==============================] - 0s 6ms/step - loss: 100.1776\nEpoch 94/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7552\nEpoch 95/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7904\nEpoch 96/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3657\nEpoch 97/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0010\nEpoch 98/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.4498\nEpoch 99/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9231\nEpoch 100/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.6204\nEpoch 101/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.2858\nEpoch 102/200\n5/5 [==============================] - 0s 6ms/step - loss: 97.6436\nEpoch 103/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.6248\nEpoch 104/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.9612\nEpoch 105/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7995\nEpoch 106/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.2072\nEpoch 107/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.6643\nEpoch 108/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0689\nEpoch 109/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.6549\nEpoch 110/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9012\nEpoch 111/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0971\nEpoch 112/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.1832\nEpoch 113/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.1513\nEpoch 114/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0943\nEpoch 115/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.3187\nEpoch 116/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7402\nEpoch 117/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.5344\nEpoch 118/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.8731\nEpoch 119/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3136\nEpoch 120/200\n5/5 [==============================] - 0s 6ms/step - loss: 97.8397\nEpoch 121/200\n5/5 [==============================] - 0s 6ms/step - loss: 101.1746\nEpoch 122/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.6667\nEpoch 123/200\n5/5 [==============================] - 0s 6ms/step - loss: 100.0607\nEpoch 124/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.5662\nEpoch 125/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.2884\nEpoch 126/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0551\nEpoch 127/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.5389\nEpoch 128/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6077\nEpoch 129/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.6665\nEpoch 130/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3588\nEpoch 131/200\n5/5 [==============================] - 0s 6ms/step - loss: 97.9513\nEpoch 132/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0245\nEpoch 133/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.1071\nEpoch 134/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.5009\nEpoch 135/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.8204\nEpoch 136/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0373\nEpoch 137/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.5803\nEpoch 138/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.8752\nEpoch 139/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6872\nEpoch 140/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.2424\nEpoch 141/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.4457\nEpoch 142/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.0366\nEpoch 143/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9752\nEpoch 144/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.8790\nEpoch 145/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9823\nEpoch 146/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.2974\nEpoch 147/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.1603\nEpoch 148/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9933\nEpoch 149/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.8249\nEpoch 150/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9807\nEpoch 151/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.2106\nEpoch 152/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9497\nEpoch 153/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.8175\nEpoch 154/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.9445\nEpoch 155/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.5975\nEpoch 156/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9631\nEpoch 157/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3393\nEpoch 158/200\n5/5 [==============================] - 0s 5ms/step - loss: 97.9259\nEpoch 159/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9772\nEpoch 160/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3005\nEpoch 161/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.0107\nEpoch 162/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9853\nEpoch 163/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.1796\nEpoch 164/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.9477\nEpoch 165/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0997\nEpoch 166/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.4543\nEpoch 167/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.9657\nEpoch 168/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.6548\nEpoch 169/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.2978\nEpoch 170/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3847\nEpoch 171/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.3182\nEpoch 172/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.2470\nEpoch 173/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9030\nEpoch 174/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6740\nEpoch 175/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.9794\nEpoch 176/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.3510\nEpoch 177/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.6953\nEpoch 178/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.0412\nEpoch 179/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.0602\nEpoch 180/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.4969\nEpoch 181/200\n5/5 [==============================] - 0s 6ms/step - loss: 99.1675\nEpoch 182/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7232\nEpoch 183/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.8273\nEpoch 184/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7739\nEpoch 185/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.6836\nEpoch 186/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.7345\nEpoch 187/200\n5/5 [==============================] - 0s 6ms/step - loss: 98.4242\nEpoch 188/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.4514\nEpoch 189/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.7265\nEpoch 190/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.1354\nEpoch 191/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.6222\nEpoch 192/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.0800\nEpoch 193/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.2473\nEpoch 194/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.1149\nEpoch 195/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.6413\nEpoch 196/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.4162\nEpoch 197/200\n5/5 [==============================] - 0s 5ms/step - loss: 99.4854\nEpoch 198/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.9748\nEpoch 199/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.8173\nEpoch 200/200\n5/5 [==============================] - 0s 5ms/step - loss: 98.0805\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 420: DatepartRegression\nModel Number: 421 with model DatepartRegression in generation 4 of 10\nModel Number: 422 with model DatepartRegression in generation 4 of 10\nModel Number: 423 with model GLM in generation 4 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 423: GLM\nModel Number: 424 with model GLM in generation 4 of 10\nModel Number: 425 with model GLM in generation 4 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 425: GLM\nModel Number: 426 with model GLM in generation 4 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:1227: RuntimeWarning: divide by zero encountered in true_divide\n  endog_mu = self._clean(endog / mu)\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:296: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n  DomainWarning)\n","output_type":"stream"},{"name":"stdout","text":"Template Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 426: GLM\nModel Number: 427 with model AverageValueNaive in generation 4 of 10\nModel Number: 428 with model AverageValueNaive in generation 4 of 10\nModel Number: 429 with model AverageValueNaive in generation 4 of 10\nModel Number: 430 with model GLS in generation 4 of 10\nModel Number: 431 with model GLS in generation 4 of 10\nModel Number: 432 with model GLS in generation 4 of 10\nModel Number: 433 with model UnobservedComponents in generation 4 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 433: UnobservedComponents\nModel Number: 434 with model UnobservedComponents in generation 4 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 434: UnobservedComponents\nModel Number: 435 with model UnobservedComponents in generation 4 of 10\nModel Number: 436 with model GluonTS in generation 4 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 436: GluonTS\nModel Number: 437 with model GluonTS in generation 4 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 437: GluonTS\nModel Number: 438 with model GluonTS in generation 4 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 438: GluonTS\nModel Number: 439 with model VAR in generation 4 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 439: VAR\nModel Number: 440 with model VAR in generation 4 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 440: VAR\nModel Number: 441 with model VAR in generation 4 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 441: VAR\nModel Number: 442 with model VAR in generation 4 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 442: VAR\nModel Number: 443 with model VECM in generation 4 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 443: VECM\nModel Number: 444 with model VECM in generation 4 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 444: VECM\nModel Number: 445 with model VECM in generation 4 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 445: VECM\nModel Number: 446 with model VECM in generation 4 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 446: VECM\nModel Number: 447 with model ARDL in generation 4 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 447: ARDL\nModel Number: 448 with model ARDL in generation 4 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 448: ARDL\nModel Number: 449 with model ARDL in generation 4 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 449: ARDL\nModel Number: 450 with model ARDL in generation 4 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 450: ARDL\nNew Generation: 5 of 10\nModel Number: 451 with model SectionalMotif in generation 5 of 10\nModel Number: 452 with model SectionalMotif in generation 5 of 10\nModel Number: 453 with model SectionalMotif in generation 5 of 10\nModel Number: 454 with model SectionalMotif in generation 5 of 10\nModel Number: 455 with model MultivariateMotif in generation 5 of 10\nModel Number: 456 with model MultivariateMotif in generation 5 of 10\nModel Number: 457 with model MultivariateMotif in generation 5 of 10\nModel Number: 458 with model MultivariateMotif in generation 5 of 10\nModel Number: 459 with model UnivariateMotif in generation 5 of 10\nModel Number: 460 with model UnivariateMotif in generation 5 of 10\nModel Number: 461 with model UnivariateMotif in generation 5 of 10\nModel Number: 462 with model UnivariateMotif in generation 5 of 10\nModel Number: 463 with model NVAR in generation 5 of 10\nModel Number: 464 with model NVAR in generation 5 of 10\nModel Number: 465 with model NVAR in generation 5 of 10\nModel Number: 466 with model NVAR in generation 5 of 10\nModel Number: 467 with model MultivariateRegression in generation 5 of 10\nModel Number: 468 with model MultivariateRegression in generation 5 of 10\nModel Number: 469 with model MultivariateRegression in generation 5 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 470 with model MultivariateRegression in generation 5 of 10\nModel Number: 471 with model ETS in generation 5 of 10\nModel Number: 472 with model ETS in generation 5 of 10\nModel Number: 473 with model ETS in generation 5 of 10\nModel Number: 474 with model ETS in generation 5 of 10\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\nModel Number: 475 with model WindowRegression in generation 5 of 10\nModel Number: 476 with model WindowRegression in generation 5 of 10\nModel Number: 477 with model WindowRegression in generation 5 of 10\nModel Number: 478 with model ZeroesNaive in generation 5 of 10\nModel Number: 479 with model ZeroesNaive in generation 5 of 10\nModel Number: 480 with model LastValueNaive in generation 5 of 10\nModel Number: 481 with model LastValueNaive in generation 5 of 10\nModel Number: 482 with model LastValueNaive in generation 5 of 10\nModel Number: 483 with model DatepartRegression in generation 5 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 483: DatepartRegression\nModel Number: 484 with model DatepartRegression in generation 5 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 484: DatepartRegression\nModel Number: 485 with model DatepartRegression in generation 5 of 10\nModel Number: 486 with model SeasonalNaive in generation 5 of 10\nModel Number: 487 with model SeasonalNaive in generation 5 of 10\nModel Number: 488 with model SeasonalNaive in generation 5 of 10\nModel Number: 489 with model SeasonalNaive in generation 5 of 10\nModel Number: 490 with model Theta in generation 5 of 10\nModel Number: 491 with model Theta in generation 5 of 10\nModel Number: 492 with model Theta in generation 5 of 10\nModel Number: 493 with model Theta in generation 5 of 10\nModel Number: 494 with model FBProphet in generation 5 of 10\nInitial log joint probability = -11.2567\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       89.5495   0.000171292       148.672    0.007774           1      117   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     165       89.8167   0.000119017       133.625   8.567e-07       0.001      233  LS failed, Hessian reset \n     199        89.836    9.6215e-05       129.721      0.8817      0.8817      271   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     206       89.8478   6.48232e-05       148.702   4.968e-07       0.001      319  LS failed, Hessian reset \n     280       89.8661   5.69738e-09       127.963      0.3336      0.3336      404   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 495 with model FBProphet in generation 5 of 10\nInitial log joint probability = -10.6962\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       75.9904   0.000592538       6.24026       0.439       0.439      126   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     134        75.998   0.000360283       7.66877   3.924e-05       0.001      203  LS failed, Hessian reset \n     199       75.9992   2.27358e-05       8.42526          10           1      289   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     203       75.9993   1.36429e-05       7.53869    1.59e-06       0.001      326  LS failed, Hessian reset \n     257       75.9995   4.14285e-08       7.77621        0.48        0.48      401   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 496 with model FBProphet in generation 5 of 10\nInitial log joint probability = -2.7525\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       381.242     0.0158173       91.4362           1           1      123   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       384.181    0.00162062       67.9757           1           1      240   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       384.293   3.18455e-07       74.2881      0.3787           1      374   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     360       384.296   2.23522e-07       60.7553   2.979e-09       0.001      510  LS failed, Hessian reset \n     370       384.296    3.3131e-09       60.0462      0.1354      0.1354      523   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 497 with model FBProphet in generation 5 of 10\nInitial log joint probability = -3.0528\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       223.697    0.00822838       6.13121           1           1      112   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       223.844    0.00330561       2.47374      0.4757      0.4757      226   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299         224.1     0.0315188       9.31406           1           1      336   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       224.429     0.0245015       1.27062           1           1      448   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     499       224.555    0.00850369       5.94421           1           1      566   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     599       224.695   0.000491321       3.06456      0.7552      0.7552      671   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     699       224.802    0.00330014       1.82369           1           1      778   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     799        224.88    0.00154852       2.60151      0.4516      0.4516      884   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     899       224.922   0.000503523       1.99611         0.3      0.7024     1001   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     999       225.117     0.0207141       9.34447      0.2946      0.9864     1113   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1099       225.534     0.0359129       4.36858      0.8044      0.8044     1224   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1199       225.957    0.00179084       1.87477           1           1     1326   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1299       226.104    0.00283132       1.83731      0.7156      0.7156     1431   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1399       226.177    0.00116721       0.91122           1           1     1542   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1499       226.257     0.0029614       3.08777      0.3605      0.3605     1656   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1599        226.33    0.00128887      0.505666           1           1     1768   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1699        226.39    0.00440139       1.46112       4.262      0.4262     1886   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1799       226.445    0.00354559       3.05865      0.2039      0.5996     1998   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1899        226.46    4.6027e-05      0.526088      0.4226      0.4226     2118   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1948        226.47    9.2319e-05       1.36601   2.866e-05       0.001     2214  LS failed, Hessian reset \n    1999       226.481    0.00158425       1.29266           1           1     2279   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2011       226.484    5.1083e-05      0.729251   7.514e-05       0.001     2330  LS failed, Hessian reset \n    2099       226.489   3.12613e-05      0.617559   5.881e-05       0.001     2470  LS failed, Hessian reset \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2199       226.501   0.000958682      0.705956       3.319      0.3319     2583   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2299       226.506   8.64635e-05      0.377993       1.558      0.1558     2702   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2399       226.518    0.00064564       2.99556      0.6195      0.6195     2826   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2499       226.562    0.00321804       1.86877           1           1     2934   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2599       226.603    0.00239511      0.791001           1           1     3042   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2699        226.63   0.000485989      0.500249           1           1     3154   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2799       226.633    0.00111236      0.718102           1           1     3276   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2834       226.639   0.000125627       1.38368    7.15e-05       0.001     3352  LS failed, Hessian reset \n    2899       226.645   0.000544172      0.696813           1           1     3434   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2999       226.647   0.000453479       0.54248           1           1     3566   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3099        226.65   0.000769316       1.48301           1           1     3676   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3199       226.656    0.00240637       0.36118           1           1     3786   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3299       226.663    0.00177412       0.82798      0.4819      0.4819     3902   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3399       226.667   0.000107283      0.554149      0.7378      0.7378     4017   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3426       226.668   3.45111e-05      0.572704    0.000116       0.001     4100  LS failed, Hessian reset \n    3452       226.668   3.74274e-06      0.230042      0.2415           1     4138   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 498 with model GLM in generation 5 of 10\nModel Number: 499 with model GLM in generation 5 of 10\nTemplate Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 499: GLM\nModel Number: 500 with model GLM in generation 5 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:296: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n  DomainWarning)\n","output_type":"stream"},{"name":"stdout","text":"Template Eval Error: Exception('Transformer Detrend failed on inverse') in model 500: GLM\nModel Number: 501 with model GLM in generation 5 of 10\nModel Number: 502 with model AverageValueNaive in generation 5 of 10\nModel Number: 503 with model AverageValueNaive in generation 5 of 10\nModel Number: 504 with model AverageValueNaive in generation 5 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:296: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n  DomainWarning)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 505 with model GLS in generation 5 of 10\nModel Number: 506 with model GLS in generation 5 of 10\nModel Number: 507 with model GLS in generation 5 of 10\nModel Number: 508 with model UnobservedComponents in generation 5 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 508: UnobservedComponents\nModel Number: 509 with model UnobservedComponents in generation 5 of 10\nModel Number: 510 with model UnobservedComponents in generation 5 of 10\nModel Number: 511 with model GluonTS in generation 5 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 511: GluonTS\nModel Number: 512 with model GluonTS in generation 5 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 512: GluonTS\nModel Number: 513 with model GluonTS in generation 5 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 513: GluonTS\nModel Number: 514 with model GluonTS in generation 5 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 514: GluonTS\nModel Number: 515 with model VAR in generation 5 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 515: VAR\nModel Number: 516 with model VAR in generation 5 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 516: VAR\nModel Number: 517 with model VAR in generation 5 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 517: VAR\nModel Number: 518 with model VAR in generation 5 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 518: VAR\nModel Number: 519 with model VECM in generation 5 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 519: VECM\nModel Number: 520 with model VECM in generation 5 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 520: VECM\nModel Number: 521 with model VECM in generation 5 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 521: VECM\nModel Number: 522 with model VECM in generation 5 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 522: VECM\nModel Number: 523 with model ARDL in generation 5 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 523: ARDL\nModel Number: 524 with model ARDL in generation 5 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 524: ARDL\nModel Number: 525 with model ARDL in generation 5 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 525: ARDL\nModel Number: 526 with model ARDL in generation 5 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 526: ARDL\nNew Generation: 6 of 10\nModel Number: 527 with model SectionalMotif in generation 6 of 10\nModel Number: 528 with model SectionalMotif in generation 6 of 10\nModel Number: 529 with model SectionalMotif in generation 6 of 10\nModel Number: 530 with model SectionalMotif in generation 6 of 10\nModel Number: 531 with model MultivariateMotif in generation 6 of 10\nModel Number: 532 with model MultivariateMotif in generation 6 of 10\nModel Number: 533 with model MultivariateMotif in generation 6 of 10\nModel Number: 534 with model MultivariateMotif in generation 6 of 10\nModel Number: 535 with model UnivariateMotif in generation 6 of 10\nModel Number: 536 with model UnivariateMotif in generation 6 of 10\nModel Number: 537 with model UnivariateMotif in generation 6 of 10\nModel Number: 538 with model UnivariateMotif in generation 6 of 10\nModel Number: 539 with model NVAR in generation 6 of 10\nModel Number: 540 with model NVAR in generation 6 of 10\nModel Number: 541 with model NVAR in generation 6 of 10\nModel Number: 542 with model NVAR in generation 6 of 10\nModel Number: 543 with model WindowRegression in generation 6 of 10\nModel Number: 544 with model WindowRegression in generation 6 of 10\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000042 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000049 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000044 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000048 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000045 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000046 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000047 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000053 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -infModel Number: 545 with model WindowRegression in generation 6 of 10\nModel Number: 546 with model MultivariateRegression in generation 6 of 10\nModel Number: 547 with model MultivariateRegression in generation 6 of 10\nEpoch 1/100\n5/5 [==============================] - 6s 14ms/step - loss: 0.1530\nEpoch 2/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.1332\nEpoch 3/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.1107\nEpoch 4/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0839\nEpoch 5/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0495\nEpoch 6/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0165\nEpoch 7/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0045\nEpoch 8/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0092\nEpoch 9/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0064\nEpoch 10/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0019\nEpoch 11/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0023\nEpoch 12/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0020\nEpoch 13/100\n5/5 [==============================] - 0s 16ms/step - loss: 0.0014\nEpoch 14/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0012\nEpoch 15/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0017\nEpoch 16/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0012\nEpoch 17/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0011\nEpoch 18/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0013\nEpoch 19/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0013\nEpoch 20/100\n5/5 [==============================] - 0s 13ms/step - loss: 9.2535e-04\nEpoch 21/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0010\nEpoch 22/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0015\nEpoch 23/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0015\nEpoch 24/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0018\nEpoch 25/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0011\nEpoch 26/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0014\nEpoch 27/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0010\nEpoch 28/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0014\nEpoch 29/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0015\nEpoch 30/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0012\nEpoch 31/100\n5/5 [==============================] - 0s 14ms/step - loss: 9.5561e-04\nEpoch 32/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0010\nEpoch 33/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0011\nEpoch 34/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0013\nEpoch 35/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0012\nEpoch 36/100\n5/5 [==============================] - 0s 13ms/step - loss: 8.5254e-04\nEpoch 37/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0013\nEpoch 38/100\n5/5 [==============================] - 0s 13ms/step - loss: 9.5860e-04\nEpoch 39/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0013\nEpoch 40/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0011\nEpoch 41/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0011\nEpoch 42/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0013\nEpoch 43/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0013\nEpoch 44/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0012\nEpoch 45/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0016\nEpoch 46/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0011\nEpoch 47/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0018\nEpoch 48/100\n5/5 [==============================] - 0s 15ms/step - loss: 0.0012\nEpoch 49/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0016\nEpoch 50/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0011\nEpoch 51/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0015\nEpoch 52/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0013\nEpoch 53/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0013\nEpoch 54/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0015\nEpoch 55/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0012\nEpoch 56/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0014\nEpoch 57/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0011\nEpoch 58/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0011\nEpoch 59/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0013\nEpoch 60/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0010\nEpoch 61/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0010\nEpoch 62/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0013\nEpoch 63/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0012\nEpoch 64/100\n5/5 [==============================] - 0s 13ms/step - loss: 9.0762e-04\nEpoch 65/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0010\nEpoch 66/100\n5/5 [==============================] - 0s 13ms/step - loss: 9.2744e-04\nEpoch 67/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0013\nEpoch 68/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0012\nEpoch 69/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0013\nEpoch 70/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0010\nEpoch 71/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0012\nEpoch 72/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0011\nEpoch 73/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0014\nEpoch 74/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0012\nEpoch 75/100\n5/5 [==============================] - 0s 13ms/step - loss: 8.8049e-04\nEpoch 76/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0012\nEpoch 77/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0011\nEpoch 78/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0012\nEpoch 79/100\n5/5 [==============================] - 0s 14ms/step - loss: 9.3916e-04\nEpoch 80/100\n5/5 [==============================] - 0s 12ms/step - loss: 8.2582e-04\nEpoch 81/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0013\nEpoch 82/100\n5/5 [==============================] - 0s 12ms/step - loss: 7.9628e-04\nEpoch 83/100\n5/5 [==============================] - 0s 12ms/step - loss: 0.0013\nEpoch 84/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0011\nEpoch 85/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0011\nEpoch 86/100\n5/5 [==============================] - 0s 29ms/step - loss: 9.2650e-04\nEpoch 87/100\n5/5 [==============================] - 0s 29ms/step - loss: 9.8772e-04\nEpoch 88/100\n5/5 [==============================] - 0s 27ms/step - loss: 9.8187e-04\nEpoch 89/100\n5/5 [==============================] - 0s 30ms/step - loss: 0.0014\nEpoch 90/100\n5/5 [==============================] - 0s 37ms/step - loss: 9.1659e-04\nEpoch 91/100\n5/5 [==============================] - 0s 19ms/step - loss: 0.0016\nEpoch 92/100\n5/5 [==============================] - 0s 41ms/step - loss: 8.5244e-04\nEpoch 93/100\n5/5 [==============================] - 0s 14ms/step - loss: 0.0011\nEpoch 94/100\n5/5 [==============================] - 0s 14ms/step - loss: 9.0385e-04\nEpoch 95/100\n5/5 [==============================] - 0s 13ms/step - loss: 7.4737e-04\nEpoch 96/100\n5/5 [==============================] - 0s 14ms/step - loss: 9.4054e-04\nEpoch 97/100\n5/5 [==============================] - 0s 14ms/step - loss: 8.9440e-04\nEpoch 98/100\n5/5 [==============================] - 0s 13ms/step - loss: 7.6154e-04\nEpoch 99/100\n5/5 [==============================] - 0s 13ms/step - loss: 0.0011\nEpoch 100/100\n5/5 [==============================] - 0s 13ms/step - loss: 9.1922e-04\nTemplate Eval Error: ValueError('Model MultivariateRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 547: MultivariateRegression\nModel Number: 548 with model MultivariateRegression in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 548: MultivariateRegression\nModel Number: 549 with model MultivariateRegression in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 549: MultivariateRegression\nModel Number: 550 with model ETS in generation 6 of 10\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\nModel Number: 551 with model ETS in generation 6 of 10\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\nModel Number: 552 with model ETS in generation 6 of 10\nModel Number: 553 with model ETS in generation 6 of 10\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nModel Number: 554 with model ZeroesNaive in generation 6 of 10\nModel Number: 555 with model ZeroesNaive in generation 6 of 10\nModel Number: 556 with model ZeroesNaive in generation 6 of 10\nModel Number: 557 with model LastValueNaive in generation 6 of 10\nModel Number: 558 with model LastValueNaive in generation 6 of 10\nModel Number: 559 with model LastValueNaive in generation 6 of 10\nModel Number: 560 with model DatepartRegression in generation 6 of 10\nModel Number: 561 with model DatepartRegression in generation 6 of 10\nModel Number: 562 with model DatepartRegression in generation 6 of 10\nModel Number: 563 with model SeasonalNaive in generation 6 of 10\nModel Number: 564 with model SeasonalNaive in generation 6 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 565 with model SeasonalNaive in generation 6 of 10\nModel Number: 566 with model SeasonalNaive in generation 6 of 10\nModel Number: 567 with model Theta in generation 6 of 10\nModel Number: 568 with model Theta in generation 6 of 10\nModel Number: 569 with model Theta in generation 6 of 10\nModel Number: 570 with model Theta in generation 6 of 10\nModel Number: 571 with model FBProphet in generation 6 of 10\nInitial log joint probability = -31.1351\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      72       129.078   9.17289e-09       63.6709      0.3934      0.3934      113   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 572 with model FBProphet in generation 6 of 10\nInitial log joint probability = -94.5726\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      82      -13.4209   7.23991e-05       90.3655   8.056e-07       0.001      139  LS failed, Hessian reset \n      99      -13.4085   6.14672e-06       71.7732      0.7022      0.7022      158   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     128       -13.408   4.97346e-09       90.7356      0.5361      0.5361      195   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 573 with model FBProphet in generation 6 of 10\nInitial log joint probability = -9.53407\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      90       92.2037   8.69492e-09       100.691      0.1391      0.1391      121   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 574 with model FBProphet in generation 6 of 10\nInitial log joint probability = -13.4818\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       101.768   4.02088e-07       99.1031      0.5373      0.5373      125   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     115       101.768   5.48869e-09       95.6135      0.1916      0.1916      145   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 575 with model GLM in generation 6 of 10\nModel Number: 576 with model GLM in generation 6 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:1444: RuntimeWarning: invalid value encountered in log\n  endog * np.log(endog / mu) + (mu - endog))\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 577 with model GLM in generation 6 of 10\nTemplate Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 577: GLM\nModel Number: 578 with model GLM in generation 6 of 10\nModel Number: 579 with model UnobservedComponents in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 579: UnobservedComponents\nModel Number: 580 with model UnobservedComponents in generation 6 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:296: DomainWarning: The inverse_power link function does not respect the domain of the Gamma family.\n  DomainWarning)\n/opt/conda/lib/python3.7/site-packages/autots/models/base.py:344: RuntimeWarning: overflow encountered in square\n  self.squared_errors = full_errors ** 2\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 581 with model UnobservedComponents in generation 6 of 10\nModel Number: 582 with model AverageValueNaive in generation 6 of 10\nModel Number: 583 with model AverageValueNaive in generation 6 of 10\nModel Number: 584 with model AverageValueNaive in generation 6 of 10\nModel Number: 585 with model GLS in generation 6 of 10\nModel Number: 586 with model GLS in generation 6 of 10\nModel Number: 587 with model GluonTS in generation 6 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 587: GluonTS\nModel Number: 588 with model GluonTS in generation 6 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 588: GluonTS\nModel Number: 589 with model GluonTS in generation 6 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 589: GluonTS\nModel Number: 590 with model GluonTS in generation 6 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 590: GluonTS\nModel Number: 591 with model VAR in generation 6 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 591: VAR\nModel Number: 592 with model VAR in generation 6 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 592: VAR\nModel Number: 593 with model VAR in generation 6 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 593: VAR\nModel Number: 594 with model VAR in generation 6 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 594: VAR\nModel Number: 595 with model VECM in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 595: VECM\nModel Number: 596 with model VECM in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 596: VECM\nModel Number: 597 with model VECM in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 597: VECM\nModel Number: 598 with model ARDL in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 598: ARDL\nModel Number: 599 with model ARDL in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 599: ARDL\nModel Number: 600 with model ARDL in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 600: ARDL\nModel Number: 601 with model ARDL in generation 6 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 601: ARDL\nNew Generation: 7 of 10\nModel Number: 602 with model SectionalMotif in generation 7 of 10\nModel Number: 603 with model SectionalMotif in generation 7 of 10\nModel Number: 604 with model SectionalMotif in generation 7 of 10\nModel Number: 605 with model SectionalMotif in generation 7 of 10\nModel Number: 606 with model MultivariateMotif in generation 7 of 10\nModel Number: 607 with model MultivariateMotif in generation 7 of 10\nModel Number: 608 with model MultivariateMotif in generation 7 of 10\nModel Number: 609 with model MultivariateMotif in generation 7 of 10\nModel Number: 610 with model UnivariateMotif in generation 7 of 10\nModel Number: 611 with model UnivariateMotif in generation 7 of 10\nModel Number: 612 with model UnivariateMotif in generation 7 of 10\nModel Number: 613 with model UnivariateMotif in generation 7 of 10\nModel Number: 614 with model NVAR in generation 7 of 10\nModel Number: 615 with model NVAR in generation 7 of 10\nModel Number: 616 with model NVAR in generation 7 of 10\nModel Number: 617 with model NVAR in generation 7 of 10\nModel Number: 618 with model WindowRegression in generation 7 of 10\nTemplate Eval Error: ValueError('Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required.') in model 618: WindowRegression\nModel Number: 619 with model WindowRegression in generation 7 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 619: WindowRegression\nModel Number: 620 with model WindowRegression in generation 7 of 10\nTemplate Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\") in model 620: WindowRegression\nModel Number: 621 with model MultivariateRegression in generation 7 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 621: MultivariateRegression\nModel Number: 622 with model MultivariateRegression in generation 7 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neighbors/_regression.py:470: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n  warnings.warn(empty_warning_msg)\n[Parallel(n_jobs=-2)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=-2)]: Done  44 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=-2)]: Done 194 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=-2)]: Done 444 tasks      | elapsed:    0.7s\n[Parallel(n_jobs=-2)]: Done 794 tasks      | elapsed:    1.3s\n[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:    1.6s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 444 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 794 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 623 with model MultivariateRegression in generation 7 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 623: MultivariateRegression\nModel Number: 624 with model MultivariateRegression in generation 7 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 624: MultivariateRegression\nModel Number: 625 with model ETS in generation 7 of 10\nModel Number: 626 with model ETS in generation 7 of 10\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\nModel Number: 627 with model ETS in generation 7 of 10\nModel Number: 628 with model ETS in generation 7 of 10\nModel Number: 629 with model ZeroesNaive in generation 7 of 10\nModel Number: 630 with model ZeroesNaive in generation 7 of 10\nModel Number: 631 with model ZeroesNaive in generation 7 of 10\nModel Number: 632 with model LastValueNaive in generation 7 of 10\nModel Number: 633 with model LastValueNaive in generation 7 of 10\nModel Number: 634 with model LastValueNaive in generation 7 of 10\nModel Number: 635 with model DatepartRegression in generation 7 of 10\nModel Number: 636 with model DatepartRegression in generation 7 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 636: DatepartRegression\nModel Number: 637 with model DatepartRegression in generation 7 of 10\nModel Number: 638 with model FBProphet in generation 7 of 10\nInitial log joint probability = -9.53407\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       212.518     0.0626368       174.159           1           1      115   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       226.835     0.0130821       57.6191           1           1      229   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       227.828    0.00383445       24.3208           1           1      358   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       228.132    0.00244273       42.1395      0.6891      0.6891      487   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     499       228.324    0.00388758       23.4634       0.825       0.825      609   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     532       228.337   4.35713e-06       2.44333   8.253e-07       0.001      692  LS failed, Hessian reset \n     553       228.366   1.16543e-05       8.42128    1.21e-06       0.001      761  LS failed, Hessian reset \n     599       228.414   0.000842136       13.7414           1           1      814   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     699        228.62    0.00842921        6.0335           1           1      936   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     763       228.767   2.37629e-05        16.686   1.312e-06       0.001     1055  LS failed, Hessian reset \n     799       228.856    0.00100532       4.12026           1           1     1099   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     899        229.02   0.000898926        24.256      0.1795           1     1231   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     979       229.087    9.8543e-06       6.19017   7.453e-07       0.001     1369  LS failed, Hessian reset \n     999       229.092   2.04693e-05       4.11378      0.2859           1     1398   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1099       229.167    0.00029722        3.7514           1           1     1530   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1173        229.26   3.22262e-05       19.3643   2.852e-06       0.001     1662  LS failed, Hessian reset \n    1199       229.354   0.000222499        8.3919     0.03953           1     1693   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1233       229.376   2.46574e-05       9.10208   9.787e-06       0.001     1780  LS failed, Hessian reset \n    1299       229.401   0.000322127       7.77718      0.3413           1     1858   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1383       229.512   1.04658e-05       9.18387   1.215e-06       0.001     2005  LS failed, Hessian reset \n    1399       229.523   9.05603e-05       6.21553           1           1     2026   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1462       229.565   5.12004e-06        4.5729   1.114e-06       0.001     2164  LS failed, Hessian reset \n    1473       229.575   2.70996e-06       2.33772   1.003e-06       0.001     2227  LS failed, Hessian reset \n    1483       229.578   2.95454e-06       2.08739   6.823e-07       0.001     2286  LS failed, Hessian reset \n    1499       229.586    0.00447648       11.6882           1           1     2308   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1547       229.597   3.33331e-06        2.8664   1.236e-06       0.001     2412  LS failed, Hessian reset \n    1599        229.61   0.000194201       2.28704      0.7624      0.7624     2472   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1699       229.661     0.0129846       16.5188       1.284      0.1284     2608   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1799       229.892     0.0026092       15.2659           1           1     2725   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1899       230.371    0.00057896       9.82439           1           1     2843   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1999        230.59     0.0024824       15.2166           1           1     2964   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2099       230.808   0.000328719       4.27806       1.274      0.1274     3092   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2150       230.815   1.21543e-05       11.8393   5.525e-07       0.001     3186  LS failed, Hessian reset \n    2199       230.818   7.16238e-05       2.91257           1           1     3251   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2299       230.837    0.00148934       5.99187           1           1     3380   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2399       230.848   2.19121e-05       1.49395       1.672      0.1672     3514   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2401       230.848   2.93117e-06       1.75193   2.215e-06       0.001     3560  LS failed, Hessian reset \n    2424       230.848   2.63456e-06       2.88802   7.479e-07       0.001     3633  LS failed, Hessian reset \n    2435       230.849   9.23684e-06       8.52102   1.318e-06       0.001     3686  LS failed, Hessian reset \n    2453       230.849   1.98494e-07       1.78379     0.03315           1     3714   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 639 with model FBProphet in generation 7 of 10\nInitial log joint probability = -9.6422\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       221.627     0.0193748       79.6913           1           1      127   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       226.884      0.117221       341.451           1           1      247   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       231.134    0.00624076       61.3888           1           1      362   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       231.904    0.00609653       74.5361      0.1923           1      485   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     499       234.089     0.0132494       169.454           1           1      604   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     599       234.911   0.000493794       85.0164           1           1      728   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     699       235.083   0.000444929       31.1426           1           1      842   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     799       235.337   0.000699977       45.0781           1           1      965   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     899       235.609    0.00437874       157.536           1           1     1094   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     999       235.734   0.000229826       48.1519           1           1     1217   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1099       235.823   0.000475984       52.5757           1           1     1338   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1199        235.97    0.00461203       57.9333           1           1     1450   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1299       236.307   0.000242221       40.3796     0.06085           1     1576   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1399        236.43   0.000120755       21.8585      0.7551      0.7551     1693   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1499       236.506   0.000462201       50.8714           1           1     1828   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1599       236.593    0.00150055       24.2275           1           1     1957   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1699       236.653    0.00174562       17.1229           1           1     2096   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1799       236.703   1.82035e-05        13.952      0.9926      0.9926     2228   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1899       236.787   0.000314272       40.6427      0.3966      0.3966     2353   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1999       236.815   0.000491386       42.4599           1           1     2477   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2099       236.992   0.000516836       43.1546      0.7011      0.7011     2598   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2199       237.189    0.00107391       40.9659      0.3256      0.3256     2722   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2299       237.245   0.000620883       8.14644           1           1     2844   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2399       237.265   0.000367233       35.7628           1           1     2959   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2499       237.285   1.14098e-05       6.61049      0.2822      0.2822     3081   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2599       237.315   3.55885e-05       7.98801           1           1     3214   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2699       237.349   0.000793596       16.8111           1           1     3348   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2799        237.54    0.00321721       74.0077           1           1     3471   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2899        237.82   0.000738825       106.343      0.6388      0.6388     3598   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2999       238.107   6.52737e-05        85.259      0.2102      0.2102     3722   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3099       238.263   0.000651843       23.7496      0.1919           1     3847   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3199       238.312   4.64357e-06       5.38266           1           1     3978   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3299        238.42   0.000212447        6.9962      0.8357      0.8357     4124   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3399       238.469    0.00137572       53.8041      0.1515           1     4253   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3499       238.518   8.86998e-06       32.5355   4.501e-07       0.001     4419  LS failed, Hessian reset \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3599        238.54   0.000181809       6.83403           1           1     4551   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3699       238.565   6.78915e-05       26.8042           1           1     4680   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3799       238.573   7.95862e-05       5.63516           1           1     4801   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3899       238.591    0.00147083       32.0983      0.2662           1     4919   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    3995        238.65   3.93054e-06       17.9479   4.607e-07       0.001     5084  LS failed, Hessian reset \n    3999       238.654   0.000755207       44.1137           1           1     5089   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4099       238.682   0.000218689       8.96435           1           1     5221   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4146       238.686   2.04102e-06       10.4492   1.093e-07       0.001     5322  LS failed, Hessian reset \n    4199       238.688     1.549e-05       4.02091       3.979      0.3979     5398   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4299       238.703   6.91832e-05       10.5005           1           1     5524   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4399       238.716    0.00142327       19.3277           1           1     5652   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4499       238.832   0.000452917        26.012           1           1     5785   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4599       238.871   3.31944e-05       6.67404      0.9297      0.9297     5909   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4699       238.895   0.000283232        14.816       3.316      0.8549     6037   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4745       238.898   1.25266e-06       6.78449   1.102e-07       0.001     6137  LS failed, Hessian reset \n    4799         238.9   1.87497e-06       4.20626      0.3956      0.3956     6211   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4899       238.926   5.97522e-05        28.176       0.221           1     6335   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    4999       238.941   8.82607e-05       5.12276      0.8237      0.8237     6462   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5043       238.943    2.4877e-06       12.9681   3.353e-07       0.001     6554  LS failed, Hessian reset \n    5099       238.945   3.09049e-05       5.58202       2.236     0.08612     6623   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5109       238.946   1.18667e-06       7.03062   2.139e-07       0.001     6678  LS failed, Hessian reset \n    5186       238.948   2.64333e-06       15.0149   2.511e-07       0.001     6829  LS failed, Hessian reset \n    5199       238.949   7.33957e-06       6.26707      0.4226      0.4226     6847   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5299       238.959    0.00034871       47.1109      0.4917           1     6962   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5399        239.04   0.000364758        124.77     0.07036           1     7090   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5491       239.084   2.63006e-05       16.4595   2.326e-06       0.001     7253  LS failed, Hessian reset \n    5499       239.089   7.84691e-05       91.1511      0.1565      0.1565     7265   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5584       239.102   2.60718e-06         14.55   2.869e-07       0.001     7414  LS failed, Hessian reset \n    5599       239.107   3.74045e-05       89.3773      0.5044      0.5044     7431   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5699       239.112   1.27992e-05       9.66065      0.4928      0.4928     7558   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5799       239.119   4.46482e-05       31.8923           1           1     7689   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5884        239.13   1.15062e-06       6.12824   1.005e-07       0.001     7838  LS failed, Hessian reset \n    5899       239.131   6.90665e-05       7.31409           1           1     7859   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    5999       239.136   0.000254391       14.0876      0.3999      0.3999     7976   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6099       239.162   8.70308e-05       7.09678      0.7551      0.7551     8105   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6199       239.167    2.4258e-05       11.2392      0.8504      0.8504     8237   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6299       239.172   0.000131619       22.7614     0.05743           1     8374   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6399       239.179   3.18398e-05       24.8693      0.4631      0.4631     8501   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6493       239.183   1.18183e-06       7.23212   1.186e-07       0.001     8656  LS failed, Hessian reset \n    6499       239.184   9.84651e-05       16.4317      0.3572           1     8665   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6599       239.187   7.33443e-06       5.28485      0.1261           1     8795   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6671       239.188   1.02726e-06       4.79667    9.17e-08       0.001     8926  LS failed, Hessian reset \n    6699        239.19   6.69676e-06       9.01508     0.04363           1     8963   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6799       239.196    0.00033006       35.6356     0.07183           1     9087   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6899       239.498    0.00200017       22.0094           1           1     9207   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    6956       239.586   2.19777e-06       15.2931   1.555e-07       0.001     9326  LS failed, Hessian reset \n    6999       239.605    4.9992e-05        19.504           1           1     9376   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7035       239.652   1.85428e-06       7.94277   7.943e-08       0.001     9471  LS failed, Hessian reset \n    7099        239.68   0.000361811       94.3818      0.4748           1     9557   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7199       239.746   0.000347283       7.26063           1           1     9683   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7204       239.747   1.45756e-06       6.20598   7.891e-08       0.001     9729  LS failed, Hessian reset \n    7299       239.771   0.000351856       22.7287           1           1     9858   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7318       239.772   7.74947e-07       5.46533   1.478e-07       0.001     9931  LS failed, Hessian reset \n    7375       239.777   1.02967e-06       6.63439       1e-07       0.001    10054  LS failed, Hessian reset \n    7399       239.781   0.000268015       24.6408           1           1    10085   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7499       239.793   0.000275911       27.3017           1           1    10218   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7599       239.813    0.00021619        16.825           1           1    10344   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7602       239.813   1.00382e-06       5.73364    8.88e-08       0.001    10387  LS failed, Hessian reset \n    7699       239.818   0.000140228       23.3744           1           1    10512   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7799        239.83   5.80349e-06       3.82194      0.1239           1    10649   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7862       239.841   4.06613e-06       23.3831   8.931e-08       0.001    10768  LS failed, Hessian reset \n    7899       239.853   1.04628e-05       11.5829      0.4955      0.4955    10815   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    7999       239.858   1.66483e-06       4.13909      0.2214      0.2214    10946   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8056       239.859   1.06452e-06       4.48249   7.773e-08       0.001    11058  LS failed, Hessian reset \n    8099        239.86   2.99831e-06       7.62676      0.6409      0.6409    11110   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8199       239.862   0.000632121       11.1644           1           1    11246   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8299       239.894     0.0201392       84.4451           1           1    11362   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8399       239.987    0.00130299       48.1017      0.1338           1    11483   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8499       240.043   0.000718937        80.355       1.052      0.1052    11618   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8586       240.059   2.45238e-06       11.8075   4.777e-07       0.001    11757  LS failed, Hessian reset \n    8599        240.06   8.04146e-05        7.3095           1           1    11770   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8699       240.078   9.39277e-05       23.6549           1           1    11901   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8799       240.087   0.000402228       13.1852      0.1519           1    12024   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8899         240.1   0.000168518       14.8888      0.8502      0.8502    12151   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    8924       240.103    1.7208e-06       9.73417   8.318e-08       0.001    12232  LS failed, Hessian reset \n    8999       240.112   0.000181277       20.3057      0.3786           1    12337   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9050       240.116   5.81363e-06       18.2297   1.136e-06       0.001    12464  LS failed, Hessian reset \n    9099       240.118   4.55868e-05       9.69439           1           1    12535   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9199       240.121   1.67968e-05       23.6234      0.8655      0.8655    12658   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9299       240.122   1.31596e-05       21.7012      0.5678      0.5678    12789   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9302       240.122   9.83236e-07       5.01027   7.762e-08       0.001    12832  LS failed, Hessian reset \n    9363       240.123   1.29339e-05       19.4164   2.511e-06       0.001    12938  LS failed, Hessian reset \n    9399       240.123   6.56859e-06       4.68417           1           1    12978   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9499       240.156    0.00454527       105.943           1           1    13097   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9599       240.263   0.000358443       84.7475      0.4115      0.4115    13223   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9699       240.325   0.000953436       70.9302      0.0441           1    13340   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9794       240.445   1.69402e-06       12.8288   9.626e-08       0.001    13502  LS failed, Hessian reset \n    9799       240.462   0.000122263       12.7852      0.3907           1    13510   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9856       240.481   1.97185e-06       15.3989   1.394e-07       0.001    13617  LS failed, Hessian reset \n    9899       240.491   6.76299e-05       7.48438       2.576      0.2576    13676   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    9975       240.508    1.2677e-06       8.98036   2.037e-07       0.001    13812  LS failed, Hessian reset \n    9999       240.511   1.58743e-05       9.82206      0.8356      0.8356    13838   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n   10000       240.511   6.87811e-05       12.7395           1           1    13839   \nOptimization terminated normally: \n  Maximum number of iterations hit, may not be at an optima\nModel Number: 640 with model FBProphet in generation 7 of 10\nInitial log joint probability = -22.6959\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       41.3917   2.81531e-07       93.0197      0.7895       0.205      125   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     132       41.3918   6.66723e-08       99.7535    7.16e-10       0.001      205  LS failed, Hessian reset \n     135       41.3918   8.84438e-09       85.3608      0.6296      0.6296      208   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 641 with model FBProphet in generation 7 of 10\nInitial log joint probability = -11.5947\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      97       92.8823   9.32689e-09       100.226      0.1891      0.1891      120   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 642 with model SeasonalNaive in generation 7 of 10\nModel Number: 643 with model SeasonalNaive in generation 7 of 10\nModel Number: 644 with model SeasonalNaive in generation 7 of 10\nModel Number: 645 with model SeasonalNaive in generation 7 of 10\nModel Number: 646 with model Theta in generation 7 of 10\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f= -9.50922D-01    |proj g|=  4.14112D-01\n\nAt iterate    5    f= -9.51484D-01    |proj g|=  4.10427D-07\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      5      8      1     0     0   4.104D-07  -9.515D-01\n  F = -0.95148435811520060     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \nModel Number: 647 with model Theta in generation 7 of 10\n","output_type":"stream"},{"name":"stderr","text":" This problem is unconstrained.\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 648 with model Theta in generation 7 of 10\nModel Number: 649 with model Theta in generation 7 of 10\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  4.10204D+00    |proj g|=  4.08109D-03\n\nAt iterate    5    f=  4.10049D+00    |proj g|=  5.12923D-07\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      5      6      1     0     0   5.129D-07   4.100D+00\n  F =   4.1004921318301504     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \nModel Number: 650 with model GLM in generation 7 of 10\n","output_type":"stream"},{"name":"stderr","text":" This problem is unconstrained.\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/links.py:188: RuntimeWarning: overflow encountered in exp\n  t = np.exp(-z)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 651 with model GLM in generation 7 of 10\nModel Number: 652 with model GLM in generation 7 of 10\nModel Number: 653 with model GLM in generation 7 of 10\nTemplate Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 653: GLM\nModel Number: 654 with model UnobservedComponents in generation 7 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/links.py:188: RuntimeWarning: overflow encountered in exp\n  t = np.exp(-z)\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/links.py:517: RuntimeWarning: overflow encountered in exp\n  return np.exp(z)\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:134: RuntimeWarning: invalid value encountered in multiply\n  return 1. / (self.link.deriv(mu)**2 * self.variance(mu))\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:1199: RuntimeWarning: invalid value encountered in multiply\n  - self._offset_exposure)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 655 with model UnobservedComponents in generation 7 of 10\nModel Number: 656 with model UnobservedComponents in generation 7 of 10\nModel Number: 657 with model AverageValueNaive in generation 7 of 10\nModel Number: 658 with model AverageValueNaive in generation 7 of 10\nModel Number: 659 with model AverageValueNaive in generation 7 of 10\nModel Number: 660 with model GLS in generation 7 of 10\nModel Number: 661 with model GLS in generation 7 of 10\nModel Number: 662 with model GLS in generation 7 of 10\nModel Number: 663 with model GluonTS in generation 7 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 663: GluonTS\nModel Number: 664 with model GluonTS in generation 7 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 664: GluonTS\nModel Number: 665 with model GluonTS in generation 7 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 665: GluonTS\nModel Number: 666 with model GluonTS in generation 7 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 666: GluonTS\nModel Number: 667 with model VAR in generation 7 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 667: VAR\nModel Number: 668 with model VAR in generation 7 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 668: VAR\nModel Number: 669 with model VAR in generation 7 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 669: VAR\nModel Number: 670 with model VAR in generation 7 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 670: VAR\nModel Number: 671 with model VECM in generation 7 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 671: VECM\nModel Number: 672 with model VECM in generation 7 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 672: VECM\nModel Number: 673 with model VECM in generation 7 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 673: VECM\nModel Number: 674 with model VECM in generation 7 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 674: VECM\nModel Number: 675 with model ARDL in generation 7 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 675: ARDL\nModel Number: 676 with model ARDL in generation 7 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 676: ARDL\nModel Number: 677 with model ARDL in generation 7 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 677: ARDL\nModel Number: 678 with model ARDL in generation 7 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 678: ARDL\nNew Generation: 8 of 10\nModel Number: 679 with model SectionalMotif in generation 8 of 10\nModel Number: 680 with model SectionalMotif in generation 8 of 10\nTemplate Eval Error: Exception('Transformer MaxAbsScaler failed on fit') in model 680: SectionalMotif\nModel Number: 681 with model SectionalMotif in generation 8 of 10\nModel Number: 682 with model SectionalMotif in generation 8 of 10\nTemplate Eval Error: Exception('Transformer RobustScaler failed on fit') in model 682: SectionalMotif\nModel Number: 683 with model NVAR in generation 8 of 10\nModel Number: 684 with model NVAR in generation 8 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:1187: RuntimeWarning: All-NaN slice encountered\n  max_abs = np.nanmax(np.abs(X), axis=0)\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1120: RuntimeWarning: All-NaN slice encountered\n  overwrite_input=overwrite_input)\n/opt/conda/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1376: RuntimeWarning: All-NaN slice encountered\n  overwrite_input=overwrite_input, interpolation=interpolation\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 685 with model NVAR in generation 8 of 10\nModel Number: 686 with model NVAR in generation 8 of 10\nModel Number: 687 with model MultivariateMotif in generation 8 of 10\nModel Number: 688 with model MultivariateMotif in generation 8 of 10\nModel Number: 689 with model MultivariateMotif in generation 8 of 10\nModel Number: 690 with model MultivariateMotif in generation 8 of 10\nModel Number: 691 with model UnivariateMotif in generation 8 of 10\nModel Number: 692 with model UnivariateMotif in generation 8 of 10\nModel Number: 693 with model UnivariateMotif in generation 8 of 10\nModel Number: 694 with model UnivariateMotif in generation 8 of 10\nModel Number: 695 with model WindowRegression in generation 8 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 695: WindowRegression\nModel Number: 696 with model WindowRegression in generation 8 of 10\nModel Number: 697 with model WindowRegression in generation 8 of 10\nModel Number: 698 with model MultivariateRegression in generation 8 of 10\nModel Number: 699 with model MultivariateRegression in generation 8 of 10\nModel Number: 700 with model MultivariateRegression in generation 8 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 700: MultivariateRegression\nModel Number: 701 with model MultivariateRegression in generation 8 of 10\nModel Number: 702 with model ETS in generation 8 of 10\nModel Number: 703 with model ETS in generation 8 of 10\nModel Number: 704 with model ETS in generation 8 of 10\nModel Number: 705 with model ETS in generation 8 of 10\nModel Number: 706 with model ZeroesNaive in generation 8 of 10\nModel Number: 707 with model ZeroesNaive in generation 8 of 10\nModel Number: 708 with model ZeroesNaive in generation 8 of 10\nModel Number: 709 with model LastValueNaive in generation 8 of 10\nModel Number: 710 with model LastValueNaive in generation 8 of 10\nModel Number: 711 with model LastValueNaive in generation 8 of 10\nModel Number: 712 with model SeasonalNaive in generation 8 of 10\nModel Number: 713 with model SeasonalNaive in generation 8 of 10\nModel Number: 714 with model SeasonalNaive in generation 8 of 10\nModel Number: 715 with model SeasonalNaive in generation 8 of 10\nModel Number: 716 with model FBProphet in generation 8 of 10\nInitial log joint probability = -8.33088\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99        104.42     0.0020074       103.209           1           1      120   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       104.723   4.78974e-07       102.647      0.6872      0.6872      244   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     215       104.723   6.37664e-09       103.371      0.3603      0.3603      266   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 717 with model FBProphet in generation 8 of 10\nInitial log joint probability = -3.27508\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       303.572     0.0206281       437.806      0.5123      0.5123      123   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       311.096    0.00415011        57.471           1           1      237   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       313.183    0.00288909       20.5404           1           1      355   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     399       314.117    0.00476583       123.958      0.6471      0.6471      474   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     499       315.139    0.00114578       10.9893           1           1      597   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     599       315.629    0.00660194       41.7432           1           1      722   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     699       315.844     0.0003711       19.0281      0.4409      0.1429      852   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     799       316.115    0.00131975       26.6704      0.1182           1      984   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     899       316.266   0.000463351       25.9842           1           1     1113   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     999       316.346   0.000661322       9.55148           1           1     1237   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1099       316.378   0.000347268       36.8175       0.434       0.434     1377   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1112       316.384   1.00526e-05       14.8916   5.183e-07       0.001     1433  LS failed, Hessian reset \n    1199       316.429   0.000637445       31.2041      0.2373      0.7603     1541   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1299       316.477   0.000622491       69.7105      0.4089           1     1668   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1314       316.486   7.58322e-06       9.15862   3.827e-07       0.001     1726  LS failed, Hessian reset \n    1399       316.519   1.06784e-05       5.56176      0.9699      0.9699     1827   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1499       316.565   0.000454244       70.4577           1           1     1950   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1599       316.592    5.6571e-05       6.48452           1           1     2075   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1699       316.608   4.12477e-05       6.08074      0.4618      0.4618     2200   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1777       316.646    1.0989e-05       6.97411   3.257e-07       0.001     2334  LS failed, Hessian reset \n    1799       316.662   6.27902e-05       20.7379     0.03989     0.03989     2363   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1899       316.682   0.000322281       16.8219           1           1     2499   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    1990       316.694   1.55423e-05       6.72755   3.177e-07       0.001     2644  LS failed, Hessian reset \n    1999       316.698   1.79892e-05       6.81218       0.375       0.375     2656   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2085       316.703   0.000131625       12.2072   2.505e-05       0.001     2792  LS failed, Hessian reset \n    2099       316.704    0.00011316       7.20216           1           1     2808   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2199       316.707   0.000822873       11.2667           1           1     2926   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2263       316.739   6.09424e-05       11.4988   5.858e-06       0.001     3046  LS failed, Hessian reset \n    2299        316.77   0.000130927       11.5097      0.6246      0.6246     3093   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2399       316.798   4.01915e-05       6.23355      0.7854      0.7854     3212   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2499       316.806     0.0001425       11.8571           1           1     3340   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2599        316.81   1.45626e-06       4.85389           1           1     3454   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2699        316.82    0.00214573       13.9256           1           1     3575   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2799        316.83   6.70503e-05       8.37566           1           1     3704   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n    2814       316.831   3.90928e-06       6.43958   5.175e-07       0.001     3761  LS failed, Hessian reset \n    2898       316.831   3.71087e-07       5.47589           1           1     3859   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\nModel Number: 718 with model FBProphet in generation 8 of 10\nInitial log joint probability = -4.15787\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      54       165.739   9.04031e-06       93.7952   9.188e-08       0.001      109  LS failed, Hessian reset \n      99        165.74    3.1436e-08       92.6799      0.6636      0.6636      162   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     117        165.74   8.86455e-09       89.9222      0.5457      0.5457      183   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 719 with model FBProphet in generation 8 of 10\nInitial log joint probability = -3.02225\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       216.586   8.28029e-05       136.897      0.9146      0.9146      117   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     144       216.599   5.34577e-05       146.343   3.844e-07       0.001      203  LS failed, Hessian reset \n     199        216.61   4.67509e-07       120.006      0.8697      0.8697      267   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     224        216.61   7.50548e-09        107.97      0.6967      0.6967      295   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 720 with model DatepartRegression in generation 8 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 720: DatepartRegression\nModel Number: 721 with model DatepartRegression in generation 8 of 10\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 721: DatepartRegression\nModel Number: 722 with model DatepartRegression in generation 8 of 10\nTemplate Eval Error: Exception('Transformer MinMaxScaler failed on fit') in model 722: DatepartRegression\nModel Number: 723 with model Theta in generation 8 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neighbors/_regression.py:470: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n  warnings.warn(empty_warning_msg)\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:461: RuntimeWarning: All-NaN slice encountered\n  data_min = np.nanmin(X, axis=0)\n/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:462: RuntimeWarning: All-NaN slice encountered\n  data_max = np.nanmax(X, axis=0)\n/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:1692: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['Timestamp', 'str']. An error will be raised in 1.2.\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 724 with model Theta in generation 8 of 10\nModel Number: 725 with model Theta in generation 8 of 10\nModel Number: 726 with model GLM in generation 8 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 726: GLM\nModel Number: 727 with model GLM in generation 8 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 727: GLM\nModel Number: 728 with model GLM in generation 8 of 10\nModel Number: 729 with model GLM in generation 8 of 10\nTemplate Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 729: GLM\nModel Number: 730 with model UnobservedComponents in generation 8 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 730: UnobservedComponents\nModel Number: 731 with model UnobservedComponents in generation 8 of 10\nModel Number: 732 with model UnobservedComponents in generation 8 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor supplied\") in model 732: UnobservedComponents\nModel Number: 733 with model AverageValueNaive in generation 8 of 10\nModel Number: 734 with model AverageValueNaive in generation 8 of 10\nModel Number: 735 with model AverageValueNaive in generation 8 of 10\nModel Number: 736 with model GLS in generation 8 of 10\nModel Number: 737 with model GLS in generation 8 of 10\nModel Number: 738 with model GLS in generation 8 of 10\nModel Number: 739 with model GluonTS in generation 8 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 739: GluonTS\nModel Number: 740 with model GluonTS in generation 8 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 740: GluonTS\nModel Number: 741 with model GluonTS in generation 8 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 741: GluonTS\nModel Number: 742 with model GluonTS in generation 8 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 742: GluonTS\nModel Number: 743 with model VAR in generation 8 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 743: VAR\nModel Number: 744 with model VAR in generation 8 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 744: VAR\nModel Number: 745 with model VAR in generation 8 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 745: VAR\nModel Number: 746 with model VECM in generation 8 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 746: VECM\nModel Number: 747 with model VECM in generation 8 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 747: VECM\nModel Number: 748 with model VECM in generation 8 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 748: VECM\nModel Number: 749 with model VECM in generation 8 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 749: VECM\nModel Number: 750 with model ARDL in generation 8 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 750: ARDL\nModel Number: 751 with model ARDL in generation 8 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 751: ARDL\nModel Number: 752 with model ARDL in generation 8 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 752: ARDL\nModel Number: 753 with model ARDL in generation 8 of 10\nTemplate Eval Error: ImportError(\"cannot import name 'ARDL' from 'statsmodels.tsa.api' (/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/api.py)\") in model 753: ARDL\nNew Generation: 9 of 10\nModel Number: 754 with model SectionalMotif in generation 9 of 10\nModel Number: 755 with model SectionalMotif in generation 9 of 10\nModel Number: 756 with model SectionalMotif in generation 9 of 10\nModel Number: 757 with model SectionalMotif in generation 9 of 10\nModel Number: 758 with model NVAR in generation 9 of 10\nModel Number: 759 with model NVAR in generation 9 of 10\nModel Number: 760 with model NVAR in generation 9 of 10\nModel Number: 761 with model NVAR in generation 9 of 10\nModel Number: 762 with model MultivariateMotif in generation 9 of 10\nModel Number: 763 with model MultivariateMotif in generation 9 of 10\nModel Number: 764 with model MultivariateMotif in generation 9 of 10\nModel Number: 765 with model MultivariateMotif in generation 9 of 10\nModel Number: 766 with model UnivariateMotif in generation 9 of 10\nModel Number: 767 with model UnivariateMotif in generation 9 of 10\nModel Number: 768 with model UnivariateMotif in generation 9 of 10\nModel Number: 769 with model UnivariateMotif in generation 9 of 10\nModel Number: 770 with model WindowRegression in generation 9 of 10\nModel Number: 771 with model WindowRegression in generation 9 of 10\nTemplate Eval Error: ValueError('Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required.') in model 771: WindowRegression\nModel Number: 772 with model WindowRegression in generation 9 of 10\nModel Number: 773 with model MultivariateRegression in generation 9 of 10\nModel Number: 774 with model MultivariateRegression in generation 9 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but not future_regressor supplied.\") in model 774: MultivariateRegression\nModel Number: 775 with model MultivariateRegression in generation 9 of 10\nModel Number: 776 with model MultivariateRegression in generation 9 of 10\nModel Number: 777 with model ETS in generation 9 of 10\nModel Number: 778 with model ETS in generation 9 of 10\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nModel Number: 779 with model ETS in generation 9 of 10\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nModel Number: 780 with model ETS in generation 9 of 10\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\nModel Number: 781 with model ZeroesNaive in generation 9 of 10\nModel Number: 782 with model ZeroesNaive in generation 9 of 10\nModel Number: 783 with model FBProphet in generation 9 of 10\nInitial log joint probability = -15.1861\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n       1      -15.1861             0       145.717       1e-12       0.001       21   \nOptimization terminated with error: \n  Line search failed to achieve a sufficient decrease, no more progress can be made\n\nInitial log joint probability = -64.6968\nIteration  1. Log joint probability =     130.94. Improved by 195.637.\nIteration  2. Log joint probability =    174.635. Improved by 43.6948.\nIteration  3. Log joint probability =    221.199. Improved by 46.5646.\nIteration  4. Log joint probability =    224.954. Improved by 3.75515.\nIteration  5. Log joint probability =    244.502. Improved by 19.5476.\nIteration  6. Log joint probability =    248.808. Improved by 4.3056.\nIteration  7. Log joint probability =    251.012. Improved by 2.20478.\nIteration  8. Log joint probability =    252.382. Improved by 1.37002.\nIteration  9. Log joint probability =    254.686. Improved by 2.30406.\nIteration 10. Log joint probability =    258.219. Improved by 3.53281.\nIteration 11. Log joint probability =    258.288. Improved by 0.068666.\nIteration 12. Log joint probability =    259.203. Improved by 0.915158.\nIteration 13. Log joint probability =    259.596. Improved by 0.392741.\nIteration 14. Log joint probability =    259.751. Improved by 0.155161.\nIteration 15. Log joint probability =    259.783. Improved by 0.0318027.\nIteration 16. Log joint probability =     260.01. Improved by 0.22689.\nIteration 17. Log joint probability =    260.341. Improved by 0.331399.\nIteration 18. Log joint probability =    260.454. Improved by 0.112808.\nIteration 19. Log joint probability =    260.544. Improved by 0.0897304.\nIteration 20. Log joint probability =    260.611. Improved by 0.0669549.\nIteration 21. Log joint probability =    260.625. Improved by 0.0140466.\nIteration 22. Log joint probability =    260.644. Improved by 0.0189762.\nIteration 23. Log joint probability =    260.644. Improved by 0.000806.\nIteration 24. Log joint probability =    260.673. Improved by 0.0290188.\nIteration 25. Log joint probability =    260.704. Improved by 0.0302802.\nIteration 26. Log joint probability =    260.723. Improved by 0.0191249.\nIteration 27. Log joint probability =    260.739. Improved by 0.0164103.\nIteration 28. Log joint probability =    260.743. Improved by 0.0037302.\nIteration 29. Log joint probability =    260.745. Improved by 0.00241577.\nIteration 30. Log joint probability =     260.75. Improved by 0.00443294.\nIteration 31. Log joint probability =    260.752. Improved by 0.00179781.\nIteration 32. Log joint probability =    260.756. Improved by 0.00480829.\nIteration 33. Log joint probability =    260.758. Improved by 0.0015056.\nIteration 34. Log joint probability =    260.763. Improved by 0.00558705.\nIteration 35. Log joint probability =    260.774. Improved by 0.0109273.\nIteration 36. Log joint probability =    260.775. Improved by 0.000118543.\nIteration 37. Log joint probability =    260.777. Improved by 0.00253046.\nIteration 38. Log joint probability =    260.778. Improved by 0.000872495.\nIteration 39. Log joint probability =     260.78. Improved by 0.00231814.\nIteration 40. Log joint probability =    260.781. Improved by 0.000347316.\nIteration 41. Log joint probability =    260.784. Improved by 0.00314017.\nIteration 42. Log joint probability =    260.789. Improved by 0.00499202.\nIteration 43. Log joint probability =    260.789. Improved by 0.000704395.\nIteration 44. Log joint probability =    260.791. Improved by 0.00110324.\nIteration 45. Log joint probability =    260.791. Improved by 0.00018335.\nIteration 46. Log joint probability =    260.792. Improved by 0.00162215.\nIteration 47. Log joint probability =    260.795. Improved by 0.00231321.\nIteration 48. Log joint probability =    260.795. Improved by 0.000616033.\nIteration 49. Log joint probability =    260.796. Improved by 0.000270571.\nIteration 50. Log joint probability =    260.796. Improved by 0.000504959.\nIteration 51. Log joint probability =    260.796. Improved by 0.000293415.\nIteration 52. Log joint probability =    260.797. Improved by 0.000421875.\nIteration 53. Log joint probability =    260.797. Improved by 0.000321642.\nIteration 54. Log joint probability =    260.798. Improved by 0.000510262.\nIteration 55. Log joint probability =    260.798. Improved by 0.00021898.\nIteration 56. Log joint probability =    260.798. Improved by 0.000658303.\nIteration 57. Log joint probability =    260.799. Improved by 9.89236e-05.\nIteration 58. Log joint probability =    260.799. Improved by 0.000729239.\nIteration 59. Log joint probability =    260.799. Improved by 8.84455e-05.\nIteration 60. Log joint probability =      260.8. Improved by 0.000664496.\nIteration 61. Log joint probability =      260.8. Improved by 0.000170107.\nIteration 62. Log joint probability =    260.801. Improved by 0.000474751.\nIteration 63. Log joint probability =    260.801. Improved by 0.000544119.\nIteration 64. Log joint probability =    260.803. Improved by 0.0013966.\nIteration 65. Log joint probability =    260.803. Improved by 1.60339e-05.\nIteration 66. Log joint probability =    260.803. Improved by 0.000367148.\nIteration 67. Log joint probability =    260.803. Improved by 4.48509e-05.\nIteration 68. Log joint probability =    260.803. Improved by 0.000321263.\nIteration 69. Log joint probability =    260.803. Improved by 0.000114365.\nIteration 70. Log joint probability =    260.804. Improved by 0.000239141.\nIteration 71. Log joint probability =    260.804. Improved by 0.000197165.\nIteration 72. Log joint probability =    260.804. Improved by 0.000165929.\nIteration 73. Log joint probability =    260.804. Improved by 0.000247483.\nIteration 74. Log joint probability =    260.804. Improved by 0.000160325.\nIteration 75. Log joint probability =    260.805. Improved by 0.000216526.\nIteration 76. Log joint probability =    260.805. Improved by 0.000228744.\nIteration 77. Log joint probability =    260.805. Improved by 8.60446e-05.\nIteration 78. Log joint probability =    260.805. Improved by 0.000416163.\nIteration 79. Log joint probability =    260.806. Improved by 0.000525516.\nIteration 80. Log joint probability =    260.806. Improved by 0.000204893.\nIteration 81. Log joint probability =    260.806. Improved by 0.000299086.\nIteration 82. Log joint probability =    260.807. Improved by 5.90056e-05.\nIteration 83. Log joint probability =    260.807. Improved by 3.97515e-05.\nIteration 84. Log joint probability =    260.807. Improved by 6.41914e-05.\nIteration 85. Log joint probability =    260.807. Improved by 3.50345e-05.\nIteration 86. Log joint probability =    260.807. Improved by 5.69697e-05.\nIteration 87. Log joint probability =    260.807. Improved by 3.94844e-05.\nIteration 88. Log joint probability =    260.807. Improved by 4.73529e-05.\nIteration 89. Log joint probability =    260.807. Improved by 7.09284e-05.\nIteration 90. Log joint probability =    260.807. Improved by 7.76124e-06.\nIteration 91. Log joint probability =    260.807. Improved by 0.000113873.\nIteration 92. Log joint probability =    260.807. Improved by 0.000168925.\nIteration 93. Log joint probability =    260.807. Improved by 6.02477e-05.\nIteration 94. Log joint probability =    260.807. Improved by 2.27578e-05.\nIteration 95. Log joint probability =    260.807. Improved by 3.6696e-05.\nIteration 96. Log joint probability =    260.807. Improved by 1.25717e-05.\nIteration 97. Log joint probability =    260.807. Improved by 1.80452e-05.\nIteration 98. Log joint probability =    260.807. Improved by 4.9731e-06.\nIteration 99. Log joint probability =    260.807. Improved by 9.2693e-06.\nIteration 100. Log joint probability =    260.807. Improved by 2.56444e-06.\nIteration 101. Log joint probability =    260.807. Improved by 2.06164e-07.\nIteration 102. Log joint probability =    260.807. Improved by 2.86461e-06.\nIteration 103. Log joint probability =    260.807. Improved by 1.01671e-07.\nIteration 104. Log joint probability =    260.807. Improved by 2.30322e-06.\nIteration 105. Log joint probability =    260.807. Improved by 1.59298e-06.\nIteration 106. Log joint probability =    260.807. Improved by 8.29338e-07.\nIteration 107. Log joint probability =    260.807. Improved by 3.24574e-06.\nIteration 108. Log joint probability =    260.807. Improved by 6.06146e-06.\nIteration 109. Log joint probability =    260.807. Improved by 1.2402e-06.\nIteration 110. Log joint probability =    260.807. Improved by 5.76772e-07.\nIteration 111. Log joint probability =    260.807. Improved by 1.89813e-07.\nIteration 112. Log joint probability =    260.807. Improved by 6.10819e-07.\nIteration 113. Log joint probability =    260.807. Improved by 9.62941e-08.\nIteration 114. Log joint probability =    260.807. Improved by 7.87531e-07.\nIteration 115. Log joint probability =    260.807. Improved by 1.06285e-06.\nIteration 116. Log joint probability =    260.807. Improved by 3.92689e-07.\nIteration 117. Log joint probability =    260.807. Improved by 2.02086e-08.\nIteration 118. Log joint probability =    260.807. Improved by 3.02991e-07.\nIteration 119. Log joint probability =    260.807. Improved by 1.25589e-07.\nIteration 120. Log joint probability =    260.807. Improved by 2.04462e-07.\nIteration 121. Log joint probability =    260.807. Improved by 2.44923e-07.\nIteration 122. Log joint probability =    260.807. Improved by 9.9814e-08.\nIteration 123. Log joint probability =    260.807. Improved by 3.67622e-07.\nIteration 124. Log joint probability =    260.807. Improved by 5.72089e-07.\nIteration 125. Log joint probability =    260.807. Improved by 1.59137e-07.\nIteration 126. Log joint probability =    260.807. Improved by 3.10157e-07.\nIteration 127. Log joint probability =    260.807. Improved by 3.02926e-08.\nIteration 128. Log joint probability =    260.807. Improved by 9.12284e-08.\nIteration 129. Log joint probability =    260.807. Improved by 1.50324e-07.\nIteration 130. Log joint probability =    260.807. Improved by 2.48538e-08.\nIteration 131. Log joint probability =    260.807. Improved by 1.83386e-08.\nIteration 132. Log joint probability =    260.807. Improved by 3.32076e-08.\nIteration 133. Log joint probability =    260.807. Improved by 1.03375e-08.\nIteration 134. Log joint probability =    260.807. Improved by 3.80592e-08.\nIteration 135. Log joint probability =    260.807. Improved by 1.27275e-08.\nIteration 136. Log joint probability =    260.807. Improved by 3.14544e-08.\nIteration 137. Log joint probability =    260.807. Improved by 2.08727e-08.\nIteration 138. Log joint probability =    260.807. Improved by 2.61801e-08.\nIteration 139. Log joint probability =    260.807. Improved by 2.78013e-08.\nIteration 140. Log joint probability =    260.807. Improved by 1.21919e-08.\nIteration 141. Log joint probability =    260.807. Improved by 4.32556e-08.\nIteration 142. Log joint probability =    260.807. Improved by 9.60597e-10.\nTemplate Eval Error: Exception('Transformer MaxAbsScaler failed on inverse') in model 783: FBProphet\nModel Number: 784 with model FBProphet in generation 9 of 10\nInitial log joint probability = -8.06778\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       108.372    0.00265794       99.7462       0.191           1      119   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     175       108.751   5.51503e-09       96.3722      0.2874      0.2874      216   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 785 with model FBProphet in generation 9 of 10\nInitial log joint probability = -8.33088\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n       1      -8.33088             0       125.435       1e-12       0.001       22   \nOptimization terminated with error: \n  Line search failed to achieve a sufficient decrease, no more progress can be made\n\nInitial log joint probability = 45.8537\nIteration  1. Log joint probability =     60.122. Improved by 14.2684.\nIteration  2. Log joint probability =    67.5828. Improved by 7.46079.\nIteration  3. Log joint probability =    91.9657. Improved by 24.3828.\nIteration  4. Log joint probability =    91.9762. Improved by 0.010555.\nIteration  5. Log joint probability =    92.2275. Improved by 0.2513.\nIteration  6. Log joint probability =    93.2896. Improved by 1.06207.\nIteration  7. Log joint probability =    93.6165. Improved by 0.326952.\nIteration  8. Log joint probability =     96.951. Improved by 3.33447.\nIteration  9. Log joint probability =    97.9397. Improved by 0.988701.\nIteration 10. Log joint probability =    98.8668. Improved by 0.92713.\nIteration 11. Log joint probability =    99.1118. Improved by 0.244929.\nIteration 12. Log joint probability =    99.1129. Improved by 0.00114245.\nIteration 13. Log joint probability =    99.1551. Improved by 0.0422256.\nIteration 14. Log joint probability =    99.1568. Improved by 0.00162629.\nIteration 15. Log joint probability =    99.1977. Improved by 0.0409214.\nIteration 16. Log joint probability =    99.1998. Improved by 0.00210716.\nIteration 17. Log joint probability =    99.2394. Improved by 0.0396482.\nIteration 18. Log joint probability =     99.242. Improved by 0.00258421.\nIteration 19. Log joint probability =    99.2804. Improved by 0.038406.\nIteration 20. Log joint probability =    99.2835. Improved by 0.00305657.\nIteration 21. Log joint probability =    99.3207. Improved by 0.0371945.\nIteration 22. Log joint probability =    99.3242. Improved by 0.00352337.\nIteration 23. Log joint probability =    99.3602. Improved by 0.0360139.\nIteration 24. Log joint probability =    99.3642. Improved by 0.00398373.\nIteration 25. Log joint probability =    99.3991. Improved by 0.0348641.\nIteration 26. Log joint probability =    99.4035. Improved by 0.00443674.\nIteration 27. Log joint probability =    99.4373. Improved by 0.0337453.\nIteration 28. Log joint probability =    99.4421. Improved by 0.00488148.\nIteration 29. Log joint probability =    99.4748. Improved by 0.0326576.\nIteration 30. Log joint probability =    99.4801. Improved by 0.00531698.\nIteration 31. Log joint probability =    99.5117. Improved by 0.0316011.\nIteration 32. Log joint probability =    99.5175. Improved by 0.00574226.\nIteration 33. Log joint probability =     99.548. Improved by 0.0305762.\nIteration 34. Log joint probability =    99.5542. Improved by 0.0061563.\nIteration 35. Log joint probability =    99.5838. Improved by 0.0295832.\nIteration 36. Log joint probability =    99.5903. Improved by 0.00655804.\nIteration 37. Log joint probability =    99.6189. Improved by 0.0286225.\nIteration 38. Log joint probability =    99.6259. Improved by 0.00694635.\nIteration 39. Log joint probability =    99.6536. Improved by 0.0276944.\nIteration 40. Log joint probability =    99.6609. Improved by 0.00732006.\nIteration 41. Log joint probability =    99.6877. Improved by 0.0267997.\nIteration 42. Log joint probability =    99.6954. Improved by 0.00767791.\nIteration 43. Log joint probability =    99.7213. Improved by 0.0259389.\nIteration 44. Log joint probability =    99.7293. Improved by 0.00801858.\nIteration 45. Log joint probability =    99.7545. Improved by 0.0251126.\nIteration 46. Log joint probability =    99.7628. Improved by 0.00834064.\nIteration 47. Log joint probability =    99.7871. Improved by 0.0243218.\nIteration 48. Log joint probability =    99.7958. Improved by 0.00864255.\nIteration 49. Log joint probability =    99.8193. Improved by 0.0235674.\nIteration 50. Log joint probability =    99.8283. Improved by 0.00892263.\nIteration 51. Log joint probability =    99.8511. Improved by 0.0228504.\nIteration 52. Log joint probability =    99.8603. Improved by 0.00917905.\nIteration 53. Log joint probability =    99.8825. Improved by 0.0221719.\nIteration 54. Log joint probability =    99.8919. Improved by 0.00940977.\nIteration 55. Log joint probability =    99.9134. Improved by 0.0215335.\nIteration 56. Log joint probability =     99.923. Improved by 0.00961254.\nIteration 57. Log joint probability =    99.9439. Improved by 0.0209365.\nIteration 58. Log joint probability =    99.9537. Improved by 0.00978482.\nIteration 59. Log joint probability =    99.9741. Improved by 0.0203827.\nIteration 60. Log joint probability =     99.984. Improved by 0.0099237.\nIteration 61. Log joint probability =    100.004. Improved by 0.0198742.\nIteration 62. Log joint probability =    100.014. Improved by 0.0100259.\nIteration 63. Log joint probability =    100.033. Improved by 0.0194132.\nIteration 64. Log joint probability =    100.043. Improved by 0.0100874.\nIteration 65. Log joint probability =    100.062. Improved by 0.0190024.\nIteration 66. Log joint probability =    100.073. Improved by 0.0101038.\nIteration 67. Log joint probability =    100.091. Improved by 0.0186448.\nIteration 68. Log joint probability =    100.101. Improved by 0.0100694.\nIteration 69. Log joint probability =     100.12. Improved by 0.0183443.\nIteration 70. Log joint probability =     100.13. Improved by 0.0099774.\nIteration 71. Log joint probability =    100.148. Improved by 0.0181054.\nIteration 72. Log joint probability =    100.158. Improved by 0.00981932.\nIteration 73. Log joint probability =    100.175. Improved by 0.0179338.\nIteration 74. Log joint probability =    100.185. Improved by 0.009584.\nIteration 75. Log joint probability =    100.203. Improved by 0.017837.\nIteration 76. Log joint probability =    100.212. Improved by 0.00925649.\nIteration 77. Log joint probability =     100.23. Improved by 0.0178254.\nIteration 78. Log joint probability =    100.239. Improved by 0.00881576.\nIteration 79. Log joint probability =    100.257. Improved by 0.017915.\nIteration 80. Log joint probability =    100.265. Improved by 0.00823052.\nIteration 81. Log joint probability =    100.283. Improved by 0.0181341.\nIteration 82. Log joint probability =     100.29. Improved by 0.00745134.\nIteration 83. Log joint probability =    100.309. Improved by 0.0185489.\nIteration 84. Log joint probability =    100.315. Improved by 0.00640873.\nIteration 85. Log joint probability =    100.335. Improved by 0.0194959.\nIteration 86. Log joint probability =    100.343. Improved by 0.00812172.\nIteration 87. Log joint probability =    100.372. Improved by 0.0291851.\nIteration 88. Log joint probability =    100.413. Improved by 0.0405929.\nIteration 89. Log joint probability =    100.414. Improved by 0.00131271.\nIteration 90. Log joint probability =    100.424. Improved by 0.0100156.\nIteration 91. Log joint probability =    100.425. Improved by 0.00130888.\nIteration 92. Log joint probability =    100.435. Improved by 0.00933706.\nIteration 93. Log joint probability =    100.436. Improved by 0.00139907.\nIteration 94. Log joint probability =    100.445. Improved by 0.00873933.\nIteration 95. Log joint probability =    100.446. Improved by 0.001516.\nIteration 96. Log joint probability =    100.455. Improved by 0.00821707.\nIteration 97. Log joint probability =    100.456. Improved by 0.00164744.\nIteration 98. Log joint probability =    100.464. Improved by 0.0077523.\nIteration 99. Log joint probability =    100.466. Improved by 0.00178681.\nIteration 100. Log joint probability =    100.473. Improved by 0.00733169.\nIteration 101. Log joint probability =    100.475. Improved by 0.00193006.\nIteration 102. Log joint probability =    100.482. Improved by 0.00694572.\nIteration 103. Log joint probability =    100.484. Improved by 0.00207473.\nIteration 104. Log joint probability =    100.491. Improved by 0.00658739.\nIteration 105. Log joint probability =    100.493. Improved by 0.00221932.\nIteration 106. Log joint probability =    100.499. Improved by 0.00625148.\nIteration 107. Log joint probability =    100.502. Improved by 0.00236293.\nIteration 108. Log joint probability =    100.508. Improved by 0.00593402.\nIteration 109. Log joint probability =     100.51. Improved by 0.00250504.\nIteration 110. Log joint probability =    100.516. Improved by 0.00563191.\nIteration 111. Log joint probability =    100.518. Improved by 0.00264539.\nIteration 112. Log joint probability =    100.524. Improved by 0.00534273.\nIteration 113. Log joint probability =    100.526. Improved by 0.00278387.\nIteration 114. Log joint probability =    100.531. Improved by 0.00506455.\nIteration 115. Log joint probability =    100.534. Improved by 0.00292045.\nIteration 116. Log joint probability =    100.539. Improved by 0.00479582.\nIteration 117. Log joint probability =    100.542. Improved by 0.0030552.\nIteration 118. Log joint probability =    100.547. Improved by 0.00453527.\nIteration 119. Log joint probability =     100.55. Improved by 0.00318818.\nIteration 120. Log joint probability =    100.554. Improved by 0.00428187.\nIteration 121. Log joint probability =    100.558. Improved by 0.00331952.\nIteration 122. Log joint probability =    100.562. Improved by 0.00403475.\nIteration 123. Log joint probability =    100.565. Improved by 0.00344932.\nIteration 124. Log joint probability =    100.569. Improved by 0.00379319.\nIteration 125. Log joint probability =    100.572. Improved by 0.00357771.\nIteration 126. Log joint probability =    100.576. Improved by 0.00355658.\nIteration 127. Log joint probability =     100.58. Improved by 0.0037048.\nIteration 128. Log joint probability =    100.583. Improved by 0.00332441.\nIteration 129. Log joint probability =    100.587. Improved by 0.0038307.\nIteration 130. Log joint probability =     100.59. Improved by 0.00309623.\nIteration 131. Log joint probability =    100.594. Improved by 0.00395554.\nIteration 132. Log joint probability =    100.597. Improved by 0.00287167.\nIteration 133. Log joint probability =    100.601. Improved by 0.00407941.\nIteration 134. Log joint probability =    100.604. Improved by 0.00265041.\nIteration 135. Log joint probability =    100.608. Improved by 0.0042024.\nIteration 136. Log joint probability =     100.61. Improved by 0.00243214.\nIteration 137. Log joint probability =    100.614. Improved by 0.00432461.\nIteration 138. Log joint probability =    100.617. Improved by 0.00221664.\nIteration 139. Log joint probability =    100.621. Improved by 0.00444613.\nIteration 140. Log joint probability =    100.623. Improved by 0.00200367.\nIteration 141. Log joint probability =    100.628. Improved by 0.00456702.\nIteration 142. Log joint probability =    100.629. Improved by 0.00179305.\nIteration 143. Log joint probability =    100.634. Improved by 0.00468737.\nIteration 144. Log joint probability =    100.636. Improved by 0.00158461.\nIteration 145. Log joint probability =    100.641. Improved by 0.00480723.\nIteration 146. Log joint probability =    100.642. Improved by 0.00137819.\nIteration 147. Log joint probability =    100.647. Improved by 0.00492667.\nIteration 148. Log joint probability =    100.648. Improved by 0.00117366.\nIteration 149. Log joint probability =    100.653. Improved by 0.00504574.\nIteration 150. Log joint probability =    100.654. Improved by 0.000970894.\nIteration 151. Log joint probability =    100.659. Improved by 0.00516449.\nIteration 152. Log joint probability =     100.66. Improved by 0.00076979.\nIteration 153. Log joint probability =    100.665. Improved by 0.00528297.\nIteration 154. Log joint probability =    100.666. Improved by 0.000570247.\nIteration 155. Log joint probability =    100.671. Improved by 0.00540123.\nIteration 156. Log joint probability =    100.672. Improved by 0.000372176.\nIteration 157. Log joint probability =    100.677. Improved by 0.00551929.\nIteration 158. Log joint probability =    100.677. Improved by 0.000175496.\nIteration 159. Log joint probability =    100.683. Improved by 0.0056372.\nIteration 160. Log joint probability =    100.707. Improved by 0.0244164.\nIteration 161. Log joint probability =    100.713. Improved by 0.00572615.\nIteration 162. Log joint probability =    100.715. Improved by 0.0017229.\nIteration 163. Log joint probability =     100.72. Improved by 0.00489875.\nIteration 164. Log joint probability =     100.72. Improved by 0.000218286.\nIteration 165. Log joint probability =     100.72. Improved by 0.000487424.\nIteration 166. Log joint probability =    100.721. Improved by 0.000216274.\nIteration 167. Log joint probability =    100.721. Improved by 0.000488244.\nIteration 168. Log joint probability =    100.721. Improved by 0.000214267.\nIteration 169. Log joint probability =    100.722. Improved by 0.000489061.\nIteration 170. Log joint probability =    100.722. Improved by 0.000212265.\nIteration 171. Log joint probability =    100.723. Improved by 0.000489875.\nIteration 172. Log joint probability =    100.723. Improved by 0.000210267.\nIteration 173. Log joint probability =    100.723. Improved by 0.000490687.\nIteration 174. Log joint probability =    100.723. Improved by 0.000208274.\nIteration 175. Log joint probability =    100.724. Improved by 0.000491496.\nIteration 176. Log joint probability =    100.724. Improved by 0.000206285.\nIteration 177. Log joint probability =    100.725. Improved by 0.000492304.\nIteration 178. Log joint probability =    100.725. Improved by 0.000204301.\nIteration 179. Log joint probability =    100.725. Improved by 0.00049181.\nIteration 180. Log joint probability =    100.726. Improved by 0.000205211.\nIteration 181. Log joint probability =    100.726. Improved by 0.000486703.\nIteration 182. Log joint probability =    100.726. Improved by 0.000211676.\nIteration 183. Log joint probability =    100.727. Improved by 0.000478524.\nIteration 184. Log joint probability =    100.727. Improved by 0.000219418.\nIteration 185. Log joint probability =    100.727. Improved by 0.000469611.\nIteration 186. Log joint probability =    100.728. Improved by 0.000227161.\nIteration 187. Log joint probability =    100.728. Improved by 0.0004607.\nIteration 188. Log joint probability =    100.728. Improved by 0.000234904.\nIteration 189. Log joint probability =    100.729. Improved by 0.00045179.\nIteration 190. Log joint probability =    100.729. Improved by 0.000242648.\nIteration 191. Log joint probability =    100.729. Improved by 0.000442881.\nIteration 192. Log joint probability =     100.73. Improved by 0.000250393.\nIteration 193. Log joint probability =     100.73. Improved by 0.000433973.\nIteration 194. Log joint probability =     100.73. Improved by 0.000258139.\nIteration 195. Log joint probability =    100.731. Improved by 0.000425067.\nIteration 196. Log joint probability =    100.731. Improved by 0.000265886.\nIteration 197. Log joint probability =    100.732. Improved by 0.000416161.\nIteration 198. Log joint probability =    100.732. Improved by 0.000273633.\nIteration 199. Log joint probability =    100.732. Improved by 0.000407258.\nIteration 200. Log joint probability =    100.732. Improved by 0.000281382.\nIteration 201. Log joint probability =    100.733. Improved by 0.000398355.\nIteration 202. Log joint probability =    100.733. Improved by 0.000289131.\nIteration 203. Log joint probability =    100.734. Improved by 0.000389453.\nIteration 204. Log joint probability =    100.734. Improved by 0.000296881.\nIteration 205. Log joint probability =    100.734. Improved by 0.000380553.\nIteration 206. Log joint probability =    100.735. Improved by 0.000304632.\nIteration 207. Log joint probability =    100.735. Improved by 0.000371653.\nIteration 208. Log joint probability =    100.735. Improved by 0.000312385.\nIteration 209. Log joint probability =    100.736. Improved by 0.000362755.\nIteration 210. Log joint probability =    100.736. Improved by 0.000320138.\nIteration 211. Log joint probability =    100.736. Improved by 0.000353857.\nIteration 212. Log joint probability =    100.737. Improved by 0.000327892.\nIteration 213. Log joint probability =    100.737. Improved by 0.000344961.\nIteration 214. Log joint probability =    100.737. Improved by 0.000335647.\nIteration 215. Log joint probability =    100.738. Improved by 0.000336066.\nIteration 216. Log joint probability =    100.738. Improved by 0.000343403.\nIteration 217. Log joint probability =    100.738. Improved by 0.000327171.\nIteration 218. Log joint probability =    100.739. Improved by 0.000351161.\nIteration 219. Log joint probability =    100.739. Improved by 0.000318278.\nIteration 220. Log joint probability =    100.739. Improved by 0.000358919.\nIteration 221. Log joint probability =     100.74. Improved by 0.000309385.\nIteration 222. Log joint probability =     100.74. Improved by 0.000366679.\nIteration 223. Log joint probability =     100.74. Improved by 0.000300494.\nIteration 224. Log joint probability =    100.741. Improved by 0.00037444.\nIteration 225. Log joint probability =    100.741. Improved by 0.000291603.\nIteration 226. Log joint probability =    100.741. Improved by 0.000382202.\nIteration 227. Log joint probability =    100.742. Improved by 0.000282713.\nIteration 228. Log joint probability =    100.742. Improved by 0.000389965.\nIteration 229. Log joint probability =    100.742. Improved by 0.000273823.\nIteration 230. Log joint probability =    100.743. Improved by 0.000397729.\nIteration 231. Log joint probability =    100.743. Improved by 0.000264935.\nIteration 232. Log joint probability =    100.743. Improved by 0.000405495.\nIteration 233. Log joint probability =    100.744. Improved by 0.000256047.\nIteration 234. Log joint probability =    100.744. Improved by 0.000413262.\nIteration 235. Log joint probability =    100.744. Improved by 0.00024716.\nIteration 236. Log joint probability =    100.745. Improved by 0.00042103.\nIteration 237. Log joint probability =    100.745. Improved by 0.000238274.\nIteration 238. Log joint probability =    100.745. Improved by 0.000428799.\nIteration 239. Log joint probability =    100.746. Improved by 0.000229388.\nIteration 240. Log joint probability =    100.746. Improved by 0.00043657.\nIteration 241. Log joint probability =    100.746. Improved by 0.000220503.\nIteration 242. Log joint probability =    100.747. Improved by 0.000444341.\nIteration 243. Log joint probability =    100.747. Improved by 0.000211619.\nIteration 244. Log joint probability =    100.747. Improved by 0.000452115.\nIteration 245. Log joint probability =    100.748. Improved by 0.000202735.\nIteration 246. Log joint probability =    100.748. Improved by 0.000459889.\nIteration 247. Log joint probability =    100.748. Improved by 0.000193852.\nIteration 248. Log joint probability =    100.749. Improved by 0.000467665.\nIteration 249. Log joint probability =    100.749. Improved by 0.000184969.\nIteration 250. Log joint probability =    100.749. Improved by 0.000475443.\nIteration 251. Log joint probability =    100.749. Improved by 0.000176087.\nIteration 252. Log joint probability =     100.75. Improved by 0.000483222.\nIteration 253. Log joint probability =     100.75. Improved by 0.000167205.\nIteration 254. Log joint probability =    100.751. Improved by 0.000491002.\nIteration 255. Log joint probability =    100.751. Improved by 0.000158324.\nIteration 256. Log joint probability =    100.751. Improved by 0.000498783.\nIteration 257. Log joint probability =    100.751. Improved by 0.000147686.\nIteration 258. Log joint probability =    100.752. Improved by 0.000509859.\nIteration 259. Log joint probability =    100.752. Improved by 0.000134969.\nIteration 260. Log joint probability =    100.753. Improved by 0.000522248.\nIteration 261. Log joint probability =    100.753. Improved by 0.000121486.\nIteration 262. Log joint probability =    100.753. Improved by 0.000534636.\nIteration 263. Log joint probability =    100.753. Improved by 0.000108006.\nIteration 264. Log joint probability =    100.754. Improved by 0.000547023.\nIteration 265. Log joint probability =    100.754. Improved by 9.45278e-05.\nIteration 266. Log joint probability =    100.755. Improved by 0.00055941.\nIteration 267. Log joint probability =    100.755. Improved by 8.10523e-05.\nIteration 268. Log joint probability =    100.755. Improved by 0.000571797.\nIteration 269. Log joint probability =    100.755. Improved by 6.75792e-05.\nIteration 270. Log joint probability =    100.756. Improved by 0.000584183.\nIteration 271. Log joint probability =    100.756. Improved by 5.41084e-05.\nIteration 272. Log joint probability =    100.757. Improved by 0.000596568.\nIteration 273. Log joint probability =    100.757. Improved by 4.06399e-05.\nIteration 274. Log joint probability =    100.757. Improved by 0.000608953.\nIteration 275. Log joint probability =    100.757. Improved by 2.71737e-05.\nIteration 276. Log joint probability =    100.758. Improved by 0.000621338.\nIteration 277. Log joint probability =    100.758. Improved by 1.37097e-05.\nIteration 278. Log joint probability =    100.758. Improved by 0.000633723.\nIteration 279. Log joint probability =    100.758. Improved by 2.47999e-07.\nIteration 280. Log joint probability =    100.759. Improved by 0.000646107.\nIteration 281. Log joint probability =    100.762. Improved by 0.00258667.\nIteration 282. Log joint probability =    100.763. Improved by 0.00129491.\nIteration 283. Log joint probability =    100.764. Improved by 0.000700001.\nIteration 284. Log joint probability =    100.764. Improved by 0.000213776.\nIteration 285. Log joint probability =    100.764. Improved by 5.28494e-05.\nIteration 286. Log joint probability =    100.764. Improved by 0.000182845.\nIteration 287. Log joint probability =    100.764. Improved by 4.10334e-05.\nIteration 288. Log joint probability =    100.764. Improved by 2.22111e-05.\nIteration 289. Log joint probability =    100.764. Improved by 3.73997e-05.\nIteration 290. Log joint probability =    100.764. Improved by 1.50669e-05.\nIteration 291. Log joint probability =    100.764. Improved by 3.55342e-06.\nIteration 292. Log joint probability =    100.764. Improved by 1.10278e-05.\nIteration 293. Log joint probability =    100.764. Improved by 3.25418e-06.\nIteration 294. Log joint probability =    100.764. Improved by 5.38022e-07.\nIteration 295. Log joint probability =    100.764. Improved by 9.4775e-08.\nIteration 296. Log joint probability =    100.764. Improved by 5.198e-07.\nIteration 297. Log joint probability =    100.764. Improved by 1.12996e-07.\nIteration 298. Log joint probability =    100.764. Improved by 5.01578e-07.\nIteration 299. Log joint probability =    100.764. Improved by 1.31218e-07.\nIteration 300. Log joint probability =    100.764. Improved by 4.83355e-07.\nIteration 301. Log joint probability =    100.764. Improved by 1.49439e-07.\nIteration 302. Log joint probability =    100.764. Improved by 4.65132e-07.\nIteration 303. Log joint probability =    100.764. Improved by 1.67661e-07.\nIteration 304. Log joint probability =    100.764. Improved by 4.4691e-07.\nIteration 305. Log joint probability =    100.764. Improved by 1.85882e-07.\nIteration 306. Log joint probability =    100.764. Improved by 4.28688e-07.\nIteration 307. Log joint probability =    100.764. Improved by 2.04103e-07.\nIteration 308. Log joint probability =    100.764. Improved by 4.10465e-07.\nIteration 309. Log joint probability =    100.764. Improved by 2.22325e-07.\nIteration 310. Log joint probability =    100.764. Improved by 3.92243e-07.\nIteration 311. Log joint probability =    100.764. Improved by 2.40546e-07.\nIteration 312. Log joint probability =    100.764. Improved by 3.7402e-07.\nIteration 313. Log joint probability =    100.764. Improved by 2.58768e-07.\nIteration 314. Log joint probability =    100.764. Improved by 3.55798e-07.\nIteration 315. Log joint probability =    100.764. Improved by 2.76989e-07.\nIteration 316. Log joint probability =    100.764. Improved by 3.3746e-07.\nIteration 317. Log joint probability =    100.764. Improved by 2.9559e-07.\nIteration 318. Log joint probability =    100.764. Improved by 3.1819e-07.\nIteration 319. Log joint probability =    100.764. Improved by 3.15378e-07.\nIteration 320. Log joint probability =    100.764. Improved by 2.98401e-07.\nIteration 321. Log joint probability =    100.764. Improved by 3.35166e-07.\nIteration 322. Log joint probability =    100.764. Improved by 2.78612e-07.\nIteration 323. Log joint probability =    100.764. Improved by 3.54953e-07.\nIteration 324. Log joint probability =    100.764. Improved by 2.58824e-07.\nIteration 325. Log joint probability =    100.764. Improved by 3.725e-07.\nIteration 326. Log joint probability =    100.764. Improved by 2.43669e-07.\nIteration 327. Log joint probability =    100.764. Improved by 3.8731e-07.\nIteration 328. Log joint probability =    100.764. Improved by 2.29049e-07.\nIteration 329. Log joint probability =    100.764. Improved by 4.0193e-07.\nIteration 330. Log joint probability =    100.764. Improved by 2.14429e-07.\nIteration 331. Log joint probability =    100.764. Improved by 4.16549e-07.\nIteration 332. Log joint probability =    100.764. Improved by 1.99808e-07.\nIteration 333. Log joint probability =    100.764. Improved by 4.31168e-07.\nIteration 334. Log joint probability =    100.764. Improved by 1.85188e-07.\nIteration 335. Log joint probability =    100.764. Improved by 4.45788e-07.\nIteration 336. Log joint probability =    100.764. Improved by 1.70567e-07.\nIteration 337. Log joint probability =    100.764. Improved by 4.60407e-07.\nIteration 338. Log joint probability =    100.764. Improved by 1.55947e-07.\nIteration 339. Log joint probability =    100.764. Improved by 4.75027e-07.\nIteration 340. Log joint probability =    100.764. Improved by 1.41326e-07.\nIteration 341. Log joint probability =    100.764. Improved by 4.89646e-07.\nIteration 342. Log joint probability =    100.764. Improved by 1.26706e-07.\nIteration 343. Log joint probability =    100.764. Improved by 5.04265e-07.\nIteration 344. Log joint probability =    100.764. Improved by 1.12085e-07.\nIteration 345. Log joint probability =    100.764. Improved by 5.18885e-07.\nIteration 346. Log joint probability =    100.764. Improved by 9.74651e-08.\nIteration 347. Log joint probability =    100.764. Improved by 5.33504e-07.\nIteration 348. Log joint probability =    100.764. Improved by 8.28443e-08.\nIteration 349. Log joint probability =    100.764. Improved by 5.48124e-07.\nIteration 350. Log joint probability =    100.764. Improved by 6.82238e-08.\nIteration 351. Log joint probability =    100.764. Improved by 5.62744e-07.\nIteration 352. Log joint probability =    100.764. Improved by 5.36034e-08.\nIteration 353. Log joint probability =    100.764. Improved by 5.77363e-07.\nIteration 354. Log joint probability =    100.764. Improved by 3.8983e-08.\nIteration 355. Log joint probability =    100.764. Improved by 5.91982e-07.\nIteration 356. Log joint probability =    100.764. Improved by 2.43626e-08.\nIteration 357. Log joint probability =    100.764. Improved by 6.06602e-07.\nIteration 358. Log joint probability =    100.764. Improved by 9.7421e-09.\nModel Number: 786 with model FBProphet in generation 9 of 10\nInitial log joint probability = -11.1279\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n       1      -11.1279             0       122.986       1e-12       0.001       19   \nOptimization terminated with error: \n  Line search failed to achieve a sufficient decrease, no more progress can be made\n\nInitial log joint probability = -5.34504\nIteration  1. Log joint probability =   -0.82498. Improved by 4.52006.\nIteration  2. Log joint probability =    81.3683. Improved by 82.1933.\nIteration  3. Log joint probability =     101.82. Improved by 20.452.\nIteration  4. Log joint probability =    102.092. Improved by 0.27199.\nIteration  5. Log joint probability =     105.62. Improved by 3.52742.\nIteration  6. Log joint probability =    106.197. Improved by 0.577067.\nIteration  7. Log joint probability =    107.324. Improved by 1.12691.\nIteration  8. Log joint probability =    107.336. Improved by 0.0118743.\nIteration  9. Log joint probability =    107.382. Improved by 0.0466685.\nIteration 10. Log joint probability =    107.393. Improved by 0.0110889.\nIteration 11. Log joint probability =    107.435. Improved by 0.0420009.\nIteration 12. Log joint probability =    107.447. Improved by 0.0121195.\nIteration 13. Log joint probability =    107.485. Improved by 0.0379651.\nIteration 14. Log joint probability =    107.498. Improved by 0.0126934.\nIteration 15. Log joint probability =    107.533. Improved by 0.0345976.\nIteration 16. Log joint probability =    107.546. Improved by 0.0130681.\nIteration 17. Log joint probability =    107.577. Improved by 0.0313456.\nIteration 18. Log joint probability =    107.591. Improved by 0.0135037.\nIteration 19. Log joint probability =    107.619. Improved by 0.0282028.\nIteration 20. Log joint probability =    107.633. Improved by 0.0137634.\nIteration 21. Log joint probability =    107.658. Improved by 0.0256336.\nIteration 22. Log joint probability =    107.672. Improved by 0.013655.\nIteration 23. Log joint probability =    107.695. Improved by 0.0235055.\nIteration 24. Log joint probability =    107.709. Improved by 0.0133048.\nIteration 25. Log joint probability =     107.73. Improved by 0.0217345.\nIteration 26. Log joint probability =    107.743. Improved by 0.0122533.\nIteration 27. Log joint probability =    107.764. Improved by 0.0212664.\nIteration 28. Log joint probability =    107.775. Improved by 0.0105966.\nIteration 29. Log joint probability =    107.796. Improved by 0.0210019.\nIteration 30. Log joint probability =    107.804. Improved by 0.00894059.\nIteration 31. Log joint probability =    107.825. Improved by 0.0208088.\nIteration 32. Log joint probability =    107.833. Improved by 0.00734443.\nIteration 33. Log joint probability =    107.853. Improved by 0.0206856.\nIteration 34. Log joint probability =    107.859. Improved by 0.00580576.\nIteration 35. Log joint probability =     107.88. Improved by 0.02063.\nIteration 36. Log joint probability =    107.884. Improved by 0.0043227.\nIteration 37. Log joint probability =    107.905. Improved by 0.0206402.\nIteration 38. Log joint probability =    107.908. Improved by 0.00289454.\nIteration 39. Log joint probability =    107.928. Improved by 0.020716.\nIteration 40. Log joint probability =     107.93. Improved by 0.00152305.\nIteration 41. Log joint probability =    107.951. Improved by 0.0208609.\nIteration 42. Log joint probability =    107.951. Improved by 0.00021664.\nIteration 43. Log joint probability =    107.972. Improved by 0.0210899.\nIteration 44. Log joint probability =    108.051. Improved by 0.079369.\nIteration 45. Log joint probability =    108.057. Improved by 0.0051023.\nIteration 46. Log joint probability =    108.062. Improved by 0.00533129.\nIteration 47. Log joint probability =    108.066. Improved by 0.00430047.\nIteration 48. Log joint probability =    108.072. Improved by 0.00597874.\nIteration 49. Log joint probability =    108.076. Improved by 0.00425814.\nIteration 50. Log joint probability =    108.082. Improved by 0.00597204.\nIteration 51. Log joint probability =    108.087. Improved by 0.00466311.\nIteration 52. Log joint probability =    108.092. Improved by 0.00492514.\nIteration 53. Log joint probability =    108.095. Improved by 0.00323965.\nIteration 54. Log joint probability =    108.101. Improved by 0.00597459.\nIteration 55. Log joint probability =    108.103. Improved by 0.00193065.\nIteration 56. Log joint probability =     108.11. Improved by 0.00696295.\nIteration 57. Log joint probability =    108.111. Improved by 0.000733485.\nIteration 58. Log joint probability =    108.119. Improved by 0.00785184.\nIteration 59. Log joint probability =    108.161. Improved by 0.0426292.\nIteration 60. Log joint probability =     108.18. Improved by 0.0182826.\nIteration 61. Log joint probability =     108.18. Improved by 0.000917162.\nIteration 62. Log joint probability =    108.181. Improved by 0.000999795.\nIteration 63. Log joint probability =    108.182. Improved by 0.00102451.\nIteration 64. Log joint probability =    108.183. Improved by 0.000871173.\nIteration 65. Log joint probability =    108.184. Improved by 0.00113831.\nIteration 66. Log joint probability =    108.185. Improved by 0.000742696.\nIteration 67. Log joint probability =    108.186. Improved by 0.00125208.\nIteration 68. Log joint probability =    108.187. Improved by 0.000614356.\nIteration 69. Log joint probability =    108.188. Improved by 0.00136583.\nIteration 70. Log joint probability =    108.189. Improved by 0.000486146.\nIteration 71. Log joint probability =     108.19. Improved by 0.00147957.\nIteration 72. Log joint probability =    108.191. Improved by 0.000355752.\nIteration 73. Log joint probability =    108.192. Improved by 0.00159784.\nIteration 74. Log joint probability =    108.193. Improved by 0.000220201.\nIteration 75. Log joint probability =    108.194. Improved by 0.00172226.\nIteration 76. Log joint probability =    108.194. Improved by 8.16518e-05.\nIteration 77. Log joint probability =    108.196. Improved by 0.00184668.\nIteration 78. Log joint probability =    108.206. Improved by 0.00969525.\nIteration 79. Log joint probability =    108.207. Improved by 0.000993492.\nIteration 80. Log joint probability =    108.212. Improved by 0.00460772.\nIteration 81. Log joint probability =    108.212. Improved by 0.000914031.\nIteration 82. Log joint probability =    108.215. Improved by 0.00212221.\nIteration 83. Log joint probability =    108.215. Improved by 0.000401696.\nIteration 84. Log joint probability =    108.216. Improved by 0.00125584.\nIteration 85. Log joint probability =    108.217. Improved by 0.000544693.\nIteration 86. Log joint probability =    108.217. Improved by 5.8979e-05.\nIteration 87. Log joint probability =    108.217. Improved by 1.11858e-08.\nIteration 88. Log joint probability =    108.217. Improved by 5.60433e-05.\nIteration 89. Log joint probability =    108.217. Improved by 2.93345e-06.\nIteration 90. Log joint probability =    108.217. Improved by 5.31074e-05.\nIteration 91. Log joint probability =    108.217. Improved by 5.85574e-06.\nIteration 92. Log joint probability =    108.217. Improved by 5.01716e-05.\nIteration 93. Log joint probability =    108.217. Improved by 8.77806e-06.\nIteration 94. Log joint probability =    108.217. Improved by 4.72357e-05.\nIteration 95. Log joint probability =    108.217. Improved by 1.17004e-05.\nIteration 96. Log joint probability =    108.217. Improved by 4.42999e-05.\nIteration 97. Log joint probability =    108.217. Improved by 1.46228e-05.\nIteration 98. Log joint probability =    108.217. Improved by 4.1364e-05.\nIteration 99. Log joint probability =    108.217. Improved by 1.75451e-05.\nIteration 100. Log joint probability =    108.217. Improved by 3.84281e-05.\nIteration 101. Log joint probability =    108.217. Improved by 2.04676e-05.\nIteration 102. Log joint probability =    108.217. Improved by 3.54921e-05.\nIteration 103. Log joint probability =    108.217. Improved by 2.339e-05.\nIteration 104. Log joint probability =    108.217. Improved by 3.25562e-05.\nIteration 105. Log joint probability =    108.217. Improved by 2.63124e-05.\nIteration 106. Log joint probability =    108.217. Improved by 2.96202e-05.\nIteration 107. Log joint probability =    108.217. Improved by 2.92349e-05.\nIteration 108. Log joint probability =    108.217. Improved by 2.66842e-05.\nIteration 109. Log joint probability =    108.217. Improved by 3.16807e-05.\nIteration 110. Log joint probability =    108.218. Improved by 2.46746e-05.\nIteration 111. Log joint probability =    108.218. Improved by 3.20579e-05.\nIteration 112. Log joint probability =    108.218. Improved by 2.59299e-05.\nIteration 113. Log joint probability =    108.218. Improved by 2.97548e-05.\nIteration 114. Log joint probability =    108.218. Improved by 2.91915e-05.\nIteration 115. Log joint probability =    108.218. Improved by 2.56592e-05.\nIteration 116. Log joint probability =    108.218. Improved by 3.41565e-05.\nIteration 117. Log joint probability =    108.218. Improved by 2.06808e-05.\nIteration 118. Log joint probability =    108.218. Improved by 3.91213e-05.\nIteration 119. Log joint probability =    108.218. Improved by 1.57026e-05.\nIteration 120. Log joint probability =    108.218. Improved by 4.4086e-05.\nIteration 121. Log joint probability =    108.218. Improved by 1.07244e-05.\nIteration 122. Log joint probability =    108.218. Improved by 4.90506e-05.\nIteration 123. Log joint probability =    108.218. Improved by 5.74639e-06.\nIteration 124. Log joint probability =    108.218. Improved by 5.40151e-05.\nIteration 125. Log joint probability =    108.218. Improved by 7.68487e-07.\nIteration 126. Log joint probability =    108.218. Improved by 5.89795e-05.\nIteration 127. Log joint probability =    108.218. Improved by 0.000261311.\nIteration 128. Log joint probability =    108.218. Improved by 0.000112413.\nIteration 129. Log joint probability =    108.219. Improved by 0.000150828.\nIteration 130. Log joint probability =    108.219. Improved by 3.85891e-05.\nIteration 131. Log joint probability =    108.219. Improved by 1.50448e-05.\nIteration 132. Log joint probability =    108.219. Improved by 2.84331e-05.\nIteration 133. Log joint probability =    108.219. Improved by 1.45836e-05.\nIteration 134. Log joint probability =    108.219. Improved by 1.43739e-05.\nIteration 135. Log joint probability =    108.219. Improved by 3.05304e-08.\nIteration 136. Log joint probability =    108.219. Improved by 1.81029e-06.\nIteration 137. Log joint probability =    108.219. Improved by 7.47438e-06.\nIteration 138. Log joint probability =    108.219. Improved by 4.93853e-06.\nIteration 139. Log joint probability =    108.219. Improved by 2.356e-06.\nIteration 140. Log joint probability =    108.219. Improved by 1.17886e-06.\nIteration 141. Log joint probability =    108.219. Improved by 1.85463e-06.\nIteration 142. Log joint probability =    108.219. Improved by 5.37935e-07.\nIteration 143. Log joint probability =    108.219. Improved by 1.25149e-06.\nIteration 144. Log joint probability =    108.219. Improved by 3.31856e-07.\nIteration 145. Log joint probability =    108.219. Improved by 2.56901e-07.\nIteration 146. Log joint probability =    108.219. Improved by 1.6478e-07.\nIteration 147. Log joint probability =    108.219. Improved by 9.09616e-08.\nIteration 148. Log joint probability =    108.219. Improved by 1.20379e-07.\nIteration 149. Log joint probability =    108.219. Improved by 6.55561e-09.\nModel Number: 787 with model LastValueNaive in generation 9 of 10\nModel Number: 788 with model LastValueNaive in generation 9 of 10\nModel Number: 789 with model LastValueNaive in generation 9 of 10\nModel Number: 790 with model SeasonalNaive in generation 9 of 10\nModel Number: 791 with model SeasonalNaive in generation 9 of 10\nModel Number: 792 with model SeasonalNaive in generation 9 of 10\nModel Number: 793 with model SeasonalNaive in generation 9 of 10\nModel Number: 794 with model DatepartRegression in generation 9 of 10\nModel Number: 795 with model DatepartRegression in generation 9 of 10\nTemplate Eval Error: ValueError('Model DatepartRegression returned NaN for one or more series. fail_on_forecast_nan=True') in model 795: DatepartRegression\nModel Number: 796 with model DatepartRegression in generation 9 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/neighbors/_regression.py:470: UserWarning: One or more samples have no neighbors within specified radius; predicting NaN.\n  warnings.warn(empty_warning_msg)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 797 with model Theta in generation 9 of 10\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f= -1.84619D+00    |proj g|=  1.01612D+00\n\nAt iterate    5    f= -1.84675D+00    |proj g|=  1.64138D-05\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      5      9      1     0     0   1.641D-05  -1.847D+00\n  F =  -1.8467477936625325     \n\nCONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \nModel Number: 798 with model Theta in generation 9 of 10\n","output_type":"stream"},{"name":"stderr","text":" This problem is unconstrained.\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 799 with model Theta in generation 9 of 10\nModel Number: 800 with model Theta in generation 9 of 10\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  5.93922D+00    |proj g|=  3.41447D-04\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      3      4      1     0     0   8.662D-07   5.939D+00\n  F =   5.9385121986663041     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \nModel Number: 801 with model GLM in generation 9 of 10\n","output_type":"stream"},{"name":"stderr","text":" This problem is unconstrained.\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 802 with model GLM in generation 9 of 10\nModel Number: 803 with model GLM in generation 9 of 10\nTemplate Eval Error: TypeError(\"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\") in model 803: GLM\nModel Number: 804 with model AverageValueNaive in generation 9 of 10\nModel Number: 805 with model AverageValueNaive in generation 9 of 10\nModel Number: 806 with model UnobservedComponents in generation 9 of 10\nModel Number: 807 with model UnobservedComponents in generation 9 of 10\nModel Number: 808 with model UnobservedComponents in generation 9 of 10\nModel Number: 809 with model GLS in generation 9 of 10\nModel Number: 810 with model GLS in generation 9 of 10\nModel Number: 811 with model GLS in generation 9 of 10\nModel Number: 812 with model GluonTS in generation 9 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 812: GluonTS\nModel Number: 813 with model GluonTS in generation 9 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 813: GluonTS\nModel Number: 814 with model GluonTS in generation 9 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 814: GluonTS\nModel Number: 815 with model GluonTS in generation 9 of 10\nTemplate Eval Error: ImportError('GluonTS installation not found or installed version is incompatible with AutoTS.') in model 815: GluonTS\nModel Number: 816 with model VAR in generation 9 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 816: VAR\nModel Number: 817 with model VAR in generation 9 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 817: VAR\nModel Number: 818 with model VAR in generation 9 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 818: VAR\nModel Number: 819 with model VAR in generation 9 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VAR') in model 819: VAR\nModel Number: 820 with model VECM in generation 9 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 820: VECM\nModel Number: 821 with model VECM in generation 9 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 821: VECM\nModel Number: 822 with model VECM in generation 9 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 822: VECM\nModel Number: 823 with model VECM in generation 9 of 10\nTemplate Eval Error: ValueError('Only gave one variable to VECM') in model 823: VECM\nModel Number: 824 with model ARDL in generation 9 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 824: ARDL\nModel Number: 825 with model ARDL in generation 9 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but future_regressor not supplied\") in model 825: ARDL\nModel Number: 826 with model ARDL in generation 9 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 826: ARDL\nModel Number: 827 with model ARDL in generation 9 of 10\nTemplate Eval Error: Exception('Transformer Detrend failed on fit') in model 827: ARDL\nNew Generation: 10 of 10\nModel Number: 828 with model SectionalMotif in generation 10 of 10\nModel Number: 829 with model SectionalMotif in generation 10 of 10\nModel Number: 830 with model SectionalMotif in generation 10 of 10\nTemplate Eval Error: ValueError(\"regression_type=='User' but no future_regressor supplied\") in model 830: SectionalMotif\nModel Number: 831 with model SectionalMotif in generation 10 of 10\nTemplate Eval Error: ValueError('Model SectionalMotif returned NaN for one or more series. fail_on_forecast_nan=True') in model 831: SectionalMotif\nModel Number: 832 with model NVAR in generation 10 of 10\nModel Number: 833 with model NVAR in generation 10 of 10\nModel Number: 834 with model NVAR in generation 10 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1396: RuntimeWarning: All-NaN slice encountered\n  overwrite_input, interpolation)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 835 with model NVAR in generation 10 of 10\nModel Number: 836 with model MultivariateMotif in generation 10 of 10\nModel Number: 837 with model MultivariateMotif in generation 10 of 10\nModel Number: 838 with model MultivariateMotif in generation 10 of 10\nModel Number: 839 with model MultivariateMotif in generation 10 of 10\nModel Number: 840 with model UnivariateMotif in generation 10 of 10\nModel Number: 841 with model UnivariateMotif in generation 10 of 10\n\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000041 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000040 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nTemplate Eval Error: ValueError('Model UnivariateMotif returned NaN for one or more series. fail_on_forecast_nan=True') in model 841: UnivariateMotif\nModel Number: 842 with model UnivariateMotif in generation 10 of 10\nModel Number: 843 with model UnivariateMotif in generation 10 of 10\nModel Number: 844 with model WindowRegression in generation 10 of 10\nModel Number: 845 with model WindowRegression in generation 10 of 10\nTemplate Eval Error: ValueError(\"regression_type='User' but no future_regressor passed\") in model 845: WindowRegression\nModel Number: 846 with model WindowRegression in generation 10 of 10\nModel Number: 847 with model MultivariateRegression in generation 10 of 10\nModel Number: 848 with model MultivariateRegression in generation 10 of 10\nModel Number: 849 with model MultivariateRegression in generation 10 of 10\nModel Number: 850 with model MultivariateRegression in generation 10 of 10\nModel Number: 851 with model ETS in generation 10 of 10\nModel Number: 852 with model ETS in generation 10 of 10\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\nModel Number: 853 with model ETS in generation 10 of 10\nModel Number: 854 with model ETS in generation 10 of 10\nModel Number: 855 with model ZeroesNaive in generation 10 of 10\nModel Number: 856 with model ZeroesNaive in generation 10 of 10\nModel Number: 857 with model ZeroesNaive in generation 10 of 10\nModel Number: 858 with model FBProphet in generation 10 of 10\nInitial log joint probability = -17.5741\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n       1      -17.5741             0       116.242       1e-12       0.001       19   \nOptimization terminated with error: \n  Line search failed to achieve a sufficient decrease, no more progress can be made\n\nInitial log joint probability = 5.53744\nIteration  1. Log joint probability =    19.3856. Improved by 13.8481.\nIteration  2. Log joint probability =    36.9549. Improved by 17.5693.\nIteration  3. Log joint probability =    39.6254. Improved by 2.67055.\nIteration  4. Log joint probability =    40.8164. Improved by 1.19096.\nIteration  5. Log joint probability =    41.6114. Improved by 0.795034.\nIteration  6. Log joint probability =    42.5289. Improved by 0.917503.\nIteration  7. Log joint probability =    44.0057. Improved by 1.47673.\nIteration  8. Log joint probability =    44.2226. Improved by 0.216977.\nIteration  9. Log joint probability =    44.3122. Improved by 0.0895713.\nIteration 10. Log joint probability =    44.3334. Improved by 0.0212085.\nIteration 11. Log joint probability =    44.3543. Improved by 0.0208457.\nIteration 12. Log joint probability =     48.423. Improved by 4.06871.\nIteration 13. Log joint probability =    48.4309. Improved by 0.0079219.\nIteration 14. Log joint probability =    48.4331. Improved by 0.00216451.\nIteration 15. Log joint probability =    48.4347. Improved by 0.00166592.\nIteration 16. Log joint probability =    48.4349. Improved by 0.00016713.\nIteration 17. Log joint probability =    48.4356. Improved by 0.000716439.\nIteration 18. Log joint probability =     48.436. Improved by 0.00039839.\nIteration 19. Log joint probability =    48.4362. Improved by 0.000170479.\nIteration 20. Log joint probability =    48.4364. Improved by 0.000259926.\nIteration 21. Log joint probability =    48.4366. Improved by 0.00010802.\nIteration 22. Log joint probability =    48.4366. Improved by 2.36297e-05.\nIteration 23. Log joint probability =    48.4367. Improved by 0.000134273.\nIteration 24. Log joint probability =    48.4368. Improved by 4.90894e-05.\nIteration 25. Log joint probability =    48.4368. Improved by 2.38512e-05.\nIteration 26. Log joint probability =    48.4368. Improved by 1.60847e-05.\nIteration 27. Log joint probability =    48.4368. Improved by 7.06738e-06.\nIteration 28. Log joint probability =    48.4368. Improved by 3.47295e-06.\nIteration 29. Log joint probability =    48.4368. Improved by 2.44931e-07.\nIteration 30. Log joint probability =    48.4368. Improved by 1.68501e-06.\nIteration 31. Log joint probability =    48.4368. Improved by 2.88046e-07.\nIteration 32. Log joint probability =    48.4368. Improved by 8.37427e-07.\nIteration 33. Log joint probability =    48.4368. Improved by 4.8719e-07.\nIteration 34. Log joint probability =    48.4368. Improved by 2.06133e-07.\nIteration 35. Log joint probability =    48.4368. Improved by 6.40995e-08.\nIteration 36. Log joint probability =    48.4368. Improved by 7.09408e-08.\nIteration 37. Log joint probability =    48.4368. Improved by 3.79909e-08.\nIteration 38. Log joint probability =    48.4368. Improved by 4.50961e-08.\nIteration 39. Log joint probability =    48.4368. Improved by 2.48009e-08.\nIteration 40. Log joint probability =    48.4368. Improved by 5.1279e-09.\nModel Number: 859 with model FBProphet in generation 10 of 10\nInitial log joint probability = -2.41026\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      94       322.083   1.55941e-05       444.345    4.26e-08       0.001      155  LS failed, Hessian reset \n      99       322.088   1.73071e-06       289.817      0.4341      0.4341      160   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     136       322.091   3.86236e-09       396.233    0.001559           1      221   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 860 with model FBProphet in generation 10 of 10\nInitial log joint probability = -2.09376\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      83       443.105   7.92071e-09       4985.01      0.5814      0.5814      104   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\nModel Number: 861 with model FBProphet in generation 10 of 10\nInitial log joint probability = -22.9958\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n       1      -22.9958             0       127.476       1e-12       0.001       23   \nOptimization terminated with error: \n  Line search failed to achieve a sufficient decrease, no more progress can be made\n\nInitial log joint probability = -116.208\nIteration  1. Log joint probability =     73.522. Improved by 189.73.\nIteration  2. Log joint probability =    100.931. Improved by 27.4093.\nIteration  3. Log joint probability =    142.153. Improved by 41.2218.\nIteration  4. Log joint probability =    142.519. Improved by 0.366064.\nIteration  5. Log joint probability =     142.87. Improved by 0.350569.\nIteration  6. Log joint probability =    143.041. Improved by 0.171171.\nIteration  7. Log joint probability =    143.439. Improved by 0.398145.\nIteration  8. Log joint probability =    143.738. Improved by 0.298603.\nIteration  9. Log joint probability =    143.766. Improved by 0.0281532.\nIteration 10. Log joint probability =    143.934. Improved by 0.168158.\nIteration 11. Log joint probability =    144.153. Improved by 0.218964.\nIteration 12. Log joint probability =    144.658. Improved by 0.505591.\nIteration 13. Log joint probability =    144.769. Improved by 0.110533.\nIteration 14. Log joint probability =    146.184. Improved by 1.4153.\nIteration 15. Log joint probability =     146.68. Improved by 0.495681.\nIteration 16. Log joint probability =    147.743. Improved by 1.06288.\nIteration 17. Log joint probability =    147.925. Improved by 0.182.\nIteration 18. Log joint probability =    163.324. Improved by 15.3988.\nIteration 19. Log joint probability =    163.412. Improved by 0.0881093.\nIteration 20. Log joint probability =    168.636. Improved by 5.22378.\nIteration 21. Log joint probability =    170.114. Improved by 1.47817.\nIteration 22. Log joint probability =    170.119. Improved by 0.00490834.\nIteration 23. Log joint probability =    170.411. Improved by 0.292363.\nIteration 24. Log joint probability =    171.093. Improved by 0.68176.\nIteration 25. Log joint probability =    171.392. Improved by 0.299352.\nIteration 26. Log joint probability =    171.816. Improved by 0.42433.\nIteration 27. Log joint probability =    172.138. Improved by 0.321597.\nIteration 28. Log joint probability =    172.183. Improved by 0.0449682.\nIteration 29. Log joint probability =    172.684. Improved by 0.500884.\nIteration 30. Log joint probability =    173.655. Improved by 0.971309.\nIteration 31. Log joint probability =    173.871. Improved by 0.215908.\nIteration 32. Log joint probability =    174.078. Improved by 0.206911.\nIteration 33. Log joint probability =    174.151. Improved by 0.0726565.\nIteration 34. Log joint probability =    174.438. Improved by 0.287508.\nIteration 35. Log joint probability =     174.67. Improved by 0.232286.\nIteration 36. Log joint probability =    174.815. Improved by 0.144686.\nIteration 37. Log joint probability =    175.233. Improved by 0.418003.\nIteration 38. Log joint probability =    175.266. Improved by 0.0329887.\nIteration 39. Log joint probability =    175.292. Improved by 0.025827.\nIteration 40. Log joint probability =    175.403. Improved by 0.110847.\nIteration 41. Log joint probability =    175.418. Improved by 0.0155009.\nIteration 42. Log joint probability =    175.484. Improved by 0.0657591.\nIteration 43. Log joint probability =    175.503. Improved by 0.0187173.\nIteration 44. Log joint probability =    175.513. Improved by 0.0100759.\nIteration 45. Log joint probability =    175.524. Improved by 0.0109019.\nIteration 46. Log joint probability =    175.524. Improved by 0.000667036.\nIteration 47. Log joint probability =    175.531. Improved by 0.00620507.\nIteration 48. Log joint probability =    175.534. Improved by 0.00294015.\nIteration 49. Log joint probability =    175.537. Improved by 0.00302774.\nIteration 50. Log joint probability =    175.539. Improved by 0.001915.\nIteration 51. Log joint probability =    175.539. Improved by 0.000571974.\nIteration 52. Log joint probability =    175.539. Improved by 0.000112082.\nIteration 53. Log joint probability =     175.54. Improved by 0.000490177.\nIteration 54. Log joint probability =     175.54. Improved by 0.000247804.\nIteration 55. Log joint probability =     175.54. Improved by 6.56119e-05.\nIteration 56. Log joint probability =     175.54. Improved by 3.97097e-05.\nIteration 57. Log joint probability =     175.54. Improved by 2.71061e-05.\nIteration 58. Log joint probability =     175.54. Improved by 3.17446e-05.\nIteration 59. Log joint probability =     175.54. Improved by 1.26622e-05.\nIteration 60. Log joint probability =     175.54. Improved by 5.6419e-06.\nIteration 61. Log joint probability =     175.54. Improved by 1.45267e-05.\nIteration 62. Log joint probability =     175.54. Improved by 6.28045e-06.\nIteration 63. Log joint probability =     175.54. Improved by 3.4737e-06.\nIteration 64. Log joint probability =     175.54. Improved by 4.65008e-07.\nIteration 65. Log joint probability =     175.54. Improved by 1.8985e-06.\nIteration 66. Log joint probability =     175.54. Improved by 8.64458e-07.\nIteration 67. Log joint probability =     175.54. Improved by 4.21789e-07.\nIteration 68. Log joint probability =     175.54. Improved by 2.94733e-10.\nModel Number: 862 with model LastValueNaive in generation 10 of 10\nTemplate Eval Error: ValueError('Model LastValueNaive returned NaN for one or more series. fail_on_forecast_nan=True') in model 862: LastValueNaive\nModel Number: 863 with model LastValueNaive in generation 10 of 10\nModel Number: 864 with model LastValueNaive in generation 10 of 10\nModel Number: 865 with model SeasonalNaive in generation 10 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1396: RuntimeWarning: All-NaN slice encountered\n  overwrite_input, interpolation)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 866 with model SeasonalNaive in generation 10 of 10\nModel Number: 867 with model SeasonalNaive in generation 10 of 10\nModel Number: 868 with model SeasonalNaive in generation 10 of 10\nModel Number: 869 with model DatepartRegression in generation 10 of 10\nTemplate Eval Error: ValueError('Failed to convert a NumPy array to a Tensor (Unsupported object type int).') in model 869: DatepartRegression\nModel Number: 870 with model DatepartRegression in generation 10 of 10\nModel Number: 871 with model DatepartRegression in generation 10 of 10\nModel Number: 872 with model Theta in generation 10 of 10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 873 with model Theta in generation 10 of 10\nModel Number: 874 with model Theta in generation 10 of 10\nModel Number: 875 with model Theta in generation 10 of 10\nModel Number: 876 with model GLM in generation 10 of 10\nModel Number: 877 with model GLM in generation 10 of 10\nTemplate Eval Error: ValueError('regression_type=user and no future_regressor passed') in model 877: GLM\nModel Number: 878 with model GLM in generation 10 of 10\nModel Number: 879 with model GLM in generation 10 of 10\nTemplate Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.') in model 879: GLM\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/links.py:517: RuntimeWarning: overflow encountered in exp\n  return np.exp(z)\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/families/family.py:134: RuntimeWarning: invalid value encountered in multiply\n  return 1. / (self.link.deriv(mu)**2 * self.variance(mu))\n/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py:1199: RuntimeWarning: invalid value encountered in multiply\n  - self._offset_exposure)\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 880 with model Ensemble in generation 11 of 0\nModel Number: 881 with model Ensemble in generation 11 of 0\nModel Number: 882 with model Ensemble in generation 11 of 0\nModel Number: 883 with model Ensemble in generation 11 of 0\nModel Number: 884 with model Ensemble in generation 11 of 0\nModel Number: 885 with model Ensemble in generation 11 of 0\nModel Number: 886 with model Ensemble in generation 11 of 0\nModel Number: 887 with model Ensemble in generation 11 of 0\nValidation Round: 1\nModel Number: 1 of 133 with model Ensemble for Validation 1\n1 - Ensemble with avg smape 7.17: \nModel Number: 2 of 133 with model Ensemble for Validation 1\n2 - Ensemble with avg smape 7.17: \nModel Number: 3 of 133 with model Ensemble for Validation 1\n3 - Ensemble with avg smape 7.61: \nModel Number: 4 of 133 with model Ensemble for Validation 1\n4 - Ensemble with avg smape 7.15: \nModel Number: 5 of 133 with model Ensemble for Validation 1\n5 - Ensemble with avg smape 6.99: \nModel Number: 6 of 133 with model Ensemble for Validation 1\n6 - Ensemble with avg smape 7.33: \nModel Number: 7 of 133 with model SectionalMotif for Validation 1\n7 - SectionalMotif with avg smape 7.68: \nModel Number: 8 of 133 with model NVAR for Validation 1\n8 - NVAR with avg smape 7.71: \nModel Number: 9 of 133 with model MultivariateMotif for Validation 1\n9 - MultivariateMotif with avg smape 8.26: \nModel Number: 10 of 133 with model Ensemble for Validation 1\n10 - Ensemble with avg smape 5.87: \nModel Number: 11 of 133 with model MultivariateMotif for Validation 1\n11 - MultivariateMotif with avg smape 4.03: \nModel Number: 12 of 133 with model SectionalMotif for Validation 1\n12 - SectionalMotif with avg smape 7.25: \nModel Number: 13 of 133 with model UnivariateMotif for Validation 1\n13 - UnivariateMotif with avg smape 9.25: \nModel Number: 14 of 133 with model MultivariateMotif for Validation 1\n14 - MultivariateMotif with avg smape 7.64: \nModel Number: 15 of 133 with model Ensemble for Validation 1\n15 - Ensemble with avg smape 3.71: \nModel Number: 16 of 133 with model UnivariateMotif for Validation 1\n16 - UnivariateMotif with avg smape 9.26: \nModel Number: 17 of 133 with model MultivariateMotif for Validation 1\n17 - MultivariateMotif with avg smape 5.0: \nModel Number: 18 of 133 with model MultivariateMotif for Validation 1\n18 - MultivariateMotif with avg smape 6.51: \nModel Number: 19 of 133 with model MultivariateMotif for Validation 1\n19 - MultivariateMotif with avg smape 6.51: \nModel Number: 20 of 133 with model MultivariateMotif for Validation 1\n20 - MultivariateMotif with avg smape 6.28: \nModel Number: 21 of 133 with model UnivariateMotif for Validation 1\n21 - UnivariateMotif with avg smape 8.29: \nModel Number: 22 of 133 with model UnivariateMotif for Validation 1\n22 - UnivariateMotif with avg smape 13.21: \nModel Number: 23 of 133 with model MultivariateRegression for Validation 1\n23 - MultivariateRegression with avg smape 3.13: \nModel Number: 24 of 133 with model MultivariateMotif for Validation 1\n24 - MultivariateMotif with avg smape 4.23: \nModel Number: 25 of 133 with model NVAR for Validation 1\n25 - NVAR with avg smape 7.82: \nModel Number: 26 of 133 with model WindowRegression for Validation 1\n26 - WindowRegression with avg smape 4.15: \nModel Number: 27 of 133 with model UnivariateMotif for Validation 1\n27 - UnivariateMotif with avg smape 5.52: \nModel Number: 28 of 133 with model MultivariateRegression for Validation 1\n28 - MultivariateRegression with avg smape 3.8: \nModel Number: 29 of 133 with model MultivariateRegression for Validation 1\n29 - MultivariateRegression with avg smape 3.8: \nModel Number: 30 of 133 with model MultivariateRegression for Validation 1\n30 - MultivariateRegression with avg smape 3.8: \nModel Number: 31 of 133 with model MultivariateRegression for Validation 1\n31 - MultivariateRegression with avg smape 3.8: \nModel Number: 32 of 133 with model MultivariateRegression for Validation 1\n32 - MultivariateRegression with avg smape 3.8: \nModel Number: 33 of 133 with model MultivariateRegression for Validation 1\n33 - MultivariateRegression with avg smape 3.8: \nModel Number: 34 of 133 with model ETS for Validation 1\n34 - ETS with avg smape 4.99: \nModel Number: 35 of 133 with model WindowRegression for Validation 1\n35 - WindowRegression with avg smape 13.96: \nModel Number: 36 of 133 with model NVAR for Validation 1\n36 - NVAR with avg smape 4.82: \nModel Number: 37 of 133 with model NVAR for Validation 1\n37 - NVAR with avg smape 4.79: \nModel Number: 38 of 133 with model MultivariateRegression for Validation 1\n38 - MultivariateRegression with avg smape 10.37: \nModel Number: 39 of 133 with model ETS for Validation 1\n39 - ETS with avg smape 5.42: \nModel Number: 40 of 133 with model ETS for Validation 1\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n40 - ETS with avg smape 5.42: \nModel Number: 41 of 133 with model ETS for Validation 1\n41 - ETS with avg smape 5.42: \nModel Number: 42 of 133 with model SectionalMotif for Validation 1\n42 - SectionalMotif with avg smape 4.35: \nModel Number: 43 of 133 with model ETS for Validation 1\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\n43 - ETS with avg smape 4.69: \nModel Number: 44 of 133 with model UnivariateMotif for Validation 1\n44 - UnivariateMotif with avg smape 10.23: \nModel Number: 45 of 133 with model UnivariateMotif for Validation 1\n45 - UnivariateMotif with avg smape 6.05: \nModel Number: 46 of 133 with model SectionalMotif for Validation 1\n46 - SectionalMotif with avg smape 11.26: \nModel Number: 47 of 133 with model UnivariateMotif for Validation 1\n47 - UnivariateMotif with avg smape 8.12: \nModel Number: 48 of 133 with model NVAR for Validation 1\n48 - NVAR with avg smape 4.93: \nModel Number: 49 of 133 with model WindowRegression for Validation 1\n49 - WindowRegression with avg smape 13.13: \nModel Number: 50 of 133 with model WindowRegression for Validation 1\n50 - WindowRegression with avg smape 5.68: \nModel Number: 51 of 133 with model SectionalMotif for Validation 1\n51 - SectionalMotif with avg smape 8.41: \nModel Number: 52 of 133 with model ETS for Validation 1\n52 - ETS with avg smape 11.33: \nModel Number: 53 of 133 with model WindowRegression for Validation 1\n53 - WindowRegression with avg smape 6.91: \nModel Number: 54 of 133 with model WindowRegression for Validation 1\n54 - WindowRegression with avg smape 6.88: \nModel Number: 55 of 133 with model NVAR for Validation 1\n55 - NVAR with avg smape 5.7: \nModel Number: 56 of 133 with model ZeroesNaive for Validation 1\n56 - ZeroesNaive with avg smape 5.42: \nModel Number: 57 of 133 with model ZeroesNaive for Validation 1\n57 - ZeroesNaive with avg smape 5.42: \nModel Number: 58 of 133 with model SeasonalNaive for Validation 1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"58 - SeasonalNaive with avg smape 4.09: \nModel Number: 59 of 133 with model WindowRegression for Validation 1\n59 - WindowRegression with avg smape 6.16: \nModel Number: 60 of 133 with model ETS for Validation 1\n60 - ETS with avg smape 11.07: \nModel Number: 61 of 133 with model NVAR for Validation 1\n61 - NVAR with avg smape 5.17: \nModel Number: 62 of 133 with model NVAR for Validation 1\n62 - NVAR with avg smape 5.27: \nModel Number: 63 of 133 with model FBProphet for Validation 1\nInitial log joint probability = -9.36985\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n       1      -9.36985             0       118.494       1e-12       0.001       19   \nOptimization terminated with error: \n  Line search failed to achieve a sufficient decrease, no more progress can be made\n\nInitial log joint probability = 55.842\nIteration  1. Log joint probability =    51.9172. Improved by -3.92477.\nIteration  2. Log joint probability =    76.0662. Improved by 24.149.\nIteration  3. Log joint probability =    91.9793. Improved by 15.9131.\nIteration  4. Log joint probability =    92.1993. Improved by 0.219937.\nIteration  5. Log joint probability =    94.6089. Improved by 2.40965.\nIteration  6. Log joint probability =    94.6473. Improved by 0.0383679.\nIteration  7. Log joint probability =    94.6707. Improved by 0.0234235.\nIteration  8. Log joint probability =    94.6958. Improved by 0.0250909.\nIteration  9. Log joint probability =     94.699. Improved by 0.0031928.\nIteration 10. Log joint probability =    94.7015. Improved by 0.00255442.\nIteration 11. Log joint probability =    94.7049. Improved by 0.00339119.\nIteration 12. Log joint probability =    94.7061. Improved by 0.00121485.\nIteration 13. Log joint probability =    94.7085. Improved by 0.00232563.\nIteration 14. Log joint probability =    94.7093. Improved by 0.000817357.\nIteration 15. Log joint probability =    94.7096. Improved by 0.00029422.\nIteration 16. Log joint probability =    94.7101. Improved by 0.000534724.\nIteration 17. Log joint probability =    94.7105. Improved by 0.000337992.\nIteration 18. Log joint probability =    94.7106. Improved by 0.000109118.\nIteration 19. Log joint probability =    94.7106. Improved by 3.89183e-06.\nIteration 20. Log joint probability =    94.7106. Improved by 3.30522e-06.\nIteration 21. Log joint probability =    94.7106. Improved by 3.35181e-06.\nIteration 22. Log joint probability =    94.7106. Improved by 3.6937e-06.\nIteration 23. Log joint probability =    94.7106. Improved by 3.11657e-06.\nIteration 24. Log joint probability =    94.7106. Improved by 3.92872e-06.\nIteration 25. Log joint probability =    94.7106. Improved by 2.88133e-06.\nIteration 26. Log joint probability =    94.7106. Improved by 4.16373e-06.\nIteration 27. Log joint probability =    94.7106. Improved by 2.64609e-06.\nIteration 28. Log joint probability =    94.7106. Improved by 4.39875e-06.\nIteration 29. Log joint probability =    94.7106. Improved by 2.41085e-06.\nIteration 30. Log joint probability =    94.7106. Improved by 4.63377e-06.\nIteration 31. Log joint probability =    94.7106. Improved by 2.17561e-06.\nIteration 32. Log joint probability =    94.7106. Improved by 4.86878e-06.\nIteration 33. Log joint probability =    94.7106. Improved by 1.94037e-06.\nIteration 34. Log joint probability =    94.7106. Improved by 5.1038e-06.\nIteration 35. Log joint probability =    94.7106. Improved by 1.70512e-06.\nIteration 36. Log joint probability =    94.7106. Improved by 5.33882e-06.\nIteration 37. Log joint probability =    94.7106. Improved by 1.46988e-06.\nIteration 38. Log joint probability =    94.7106. Improved by 5.57384e-06.\nIteration 39. Log joint probability =    94.7106. Improved by 1.23464e-06.\nIteration 40. Log joint probability =    94.7106. Improved by 5.80885e-06.\nIteration 41. Log joint probability =    94.7106. Improved by 9.99396e-07.\nIteration 42. Log joint probability =    94.7106. Improved by 6.04387e-06.\nIteration 43. Log joint probability =    94.7106. Improved by 7.64153e-07.\nIteration 44. Log joint probability =    94.7107. Improved by 6.27889e-06.\nIteration 45. Log joint probability =    94.7107. Improved by 5.28909e-07.\nIteration 46. Log joint probability =    94.7107. Improved by 6.51391e-06.\nIteration 47. Log joint probability =    94.7107. Improved by 2.93665e-07.\nIteration 48. Log joint probability =    94.7107. Improved by 6.74893e-06.\nIteration 49. Log joint probability =    94.7107. Improved by 5.84214e-08.\nIteration 50. Log joint probability =    94.7107. Improved by 6.98395e-06.\nIteration 51. Log joint probability =    94.7108. Improved by 7.43059e-05.\nIteration 52. Log joint probability =    94.7108. Improved by 8.91033e-06.\nIteration 53. Log joint probability =    94.7108. Improved by 3.5439e-05.\nIteration 54. Log joint probability =    94.7108. Improved by 2.41993e-06.\nIteration 55. Log joint probability =    94.7108. Improved by 1.88063e-05.\nIteration 56. Log joint probability =    94.7108. Improved by 1.01942e-06.\nIteration 57. Log joint probability =    94.7108. Improved by 8.7965e-06.\nIteration 58. Log joint probability =    94.7108. Improved by 1.9405e-06.\nIteration 59. Log joint probability =    94.7108. Improved by 5.03871e-06.\nIteration 60. Log joint probability =    94.7108. Improved by 1.20876e-06.\nIteration 61. Log joint probability =    94.7108. Improved by 7.90065e-07.\nIteration 62. Log joint probability =    94.7108. Improved by 6.98612e-07.\nIteration 63. Log joint probability =    94.7108. Improved by 3.79694e-07.\nIteration 64. Log joint probability =    94.7108. Improved by 4.03304e-07.\nIteration 65. Log joint probability =    94.7108. Improved by 9.93776e-08.\nIteration 66. Log joint probability =    94.7108. Improved by 2.36429e-07.\nIteration 67. Log joint probability =    94.7108. Improved by 7.01379e-08.\nIteration 68. Log joint probability =    94.7108. Improved by 1.19918e-07.\nIteration 69. Log joint probability =    94.7108. Improved by 1.13685e-08.\nIteration 70. Log joint probability =    94.7108. Improved by 9.08873e-08.\nIteration 71. Log joint probability =    94.7108. Improved by 2.51597e-08.\nIteration 72. Log joint probability =    94.7108. Improved by 1.64335e-08.\nIteration 73. Log joint probability =    94.7108. Improved by 9.42433e-09.\n63 - FBProphet with avg smape 6.62: \nModel Number: 64 of 133 with model FBProphet for Validation 1\nInitial log joint probability = -9.36985\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      92       98.7984   0.000120947       94.5774   1.196e-06       0.001      150  LS failed, Hessian reset \n      99       98.8099   0.000106226       106.256           1           1      157   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     156       98.8175   9.02642e-09       98.6767      0.5272      0.5272      235   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n64 - FBProphet with avg smape 6.27: \nModel Number: 65 of 133 with model SectionalMotif for Validation 1\n65 - SectionalMotif with avg smape 6.54: \nModel Number: 66 of 133 with model ZeroesNaive for Validation 1\n66 - ZeroesNaive with avg smape 5.4: \nModel Number: 67 of 133 with model ZeroesNaive for Validation 1\n67 - ZeroesNaive with avg smape 3.12: \nModel Number: 68 of 133 with model LastValueNaive for Validation 1\n68 - LastValueNaive with avg smape 7.48: \nModel Number: 69 of 133 with model LastValueNaive for Validation 1\n69 - LastValueNaive with avg smape 7.48: \nModel Number: 70 of 133 with model FBProphet for Validation 1\nInitial log joint probability = -4.90436\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      53       121.806   7.19195e-09        96.932       0.467       0.467       77   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n70 - FBProphet with avg smape 12.43: \nModel Number: 71 of 133 with model ZeroesNaive for Validation 1\n71 - ZeroesNaive with avg smape 4.56: \nModel Number: 72 of 133 with model SeasonalNaive for Validation 1\n72 - SeasonalNaive with avg smape 7.48: \nModel Number: 73 of 133 with model FBProphet for Validation 1\nInitial log joint probability = -13.645\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      56       40.3313   8.07453e-09       95.1539      0.2737      0.2737       83   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n73 - FBProphet with avg smape 11.63: \nModel Number: 74 of 133 with model DatepartRegression for Validation 1\n74 - DatepartRegression with avg smape 15.62: \nModel Number: 75 of 133 with model SectionalMotif for Validation 1\n75 - SectionalMotif with avg smape 7.94: \nModel Number: 76 of 133 with model SectionalMotif for Validation 1\n76 - SectionalMotif with avg smape 7.48: \nModel Number: 77 of 133 with model LastValueNaive for Validation 1\n77 - LastValueNaive with avg smape 10.89: \nModel Number: 78 of 133 with model SeasonalNaive for Validation 1\n78 - SeasonalNaive with avg smape 9.67: \nModel Number: 79 of 133 with model FBProphet for Validation 1\nInitial log joint probability = -9.44334\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      89       95.4516   5.57568e-09       98.6994      0.2103      0.2103      121   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n79 - FBProphet with avg smape 12.41: \nModel Number: 80 of 133 with model LastValueNaive for Validation 1\n80 - LastValueNaive with avg smape 11.47: \nModel Number: 81 of 133 with model DatepartRegression for Validation 1\n81 - DatepartRegression with avg smape 11.71: \nModel Number: 82 of 133 with model SeasonalNaive for Validation 1\n82 - SeasonalNaive with avg smape 9.62: \nModel Number: 83 of 133 with model LastValueNaive for Validation 1\n83 - LastValueNaive with avg smape 11.58: \nModel Number: 84 of 133 with model Theta for Validation 1\n84 - Theta with avg smape 14.77: \nModel Number: 85 of 133 with model Theta for Validation 1\n85 - Theta with avg smape 14.77: \nModel Number: 86 of 133 with model WindowRegression for Validation 1\n86 - WindowRegression with avg smape 22.61: \nModel Number: 87 of 133 with model SeasonalNaive for Validation 1\n87 - SeasonalNaive with avg smape 7.78: \nModel Number: 88 of 133 with model Theta for Validation 1\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f= -8.89634D-01    |proj g|=  3.51724D-01\n\nAt iterate    5    f= -8.90123D-01    |proj g|=  1.61660D-07\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      5      8      1     0     0   1.617D-07  -8.901D-01\n  F = -0.89012315577665435     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n88 - Theta with avg smape 15.16: \nModel Number: 89 of 133 with model Theta for Validation 1\n","output_type":"stream"},{"name":"stderr","text":" This problem is unconstrained.\n This problem is unconstrained.\n\n Warning:  more than 10 function and gradient\n   evaluations in the last line search.  Termination\n   may possibly be caused by a bad search direction.\n","output_type":"stream"},{"name":"stdout","text":"RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f= -1.79864D+00    |proj g|=  8.75452D-01\n\nAt iterate    5    f= -1.79913D+00    |proj g|=  1.23911D-04\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      6     19      1     0     0   1.239D-04  -1.799D+00\n  F =  -1.7991253951562736     \n\nCONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n89 - Theta with avg smape 15.16: \nModel Number: 90 of 133 with model ETS for Validation 1\n90 - ETS with avg smape 20.93: \nModel Number: 91 of 133 with model GLM for Validation 1\n91 - GLM with avg smape 5.8: \nModel Number: 92 of 133 with model Theta for Validation 1\n92 - Theta with avg smape 16.04: \nModel Number: 93 of 133 with model LastValueNaive for Validation 1\n93 - LastValueNaive with avg smape 7.84: \nModel Number: 94 of 133 with model SeasonalNaive for Validation 1\n94 - SeasonalNaive with avg smape 8.37: \nModel Number: 95 of 133 with model ZeroesNaive for Validation 1\n95 - ZeroesNaive with avg smape 4.82: \nModel Number: 96 of 133 with model LastValueNaive for Validation 1\n96 - LastValueNaive with avg smape 10.84: \nModel Number: 97 of 133 with model DatepartRegression for Validation 1\n97 - DatepartRegression with avg smape 12.5: \nModel Number: 98 of 133 with model Theta for Validation 1\n98 - Theta with avg smape 14.59: \nModel Number: 99 of 133 with model SeasonalNaive for Validation 1\n99 - SeasonalNaive with avg smape 15.97: \nModel Number: 100 of 133 with model SeasonalNaive for Validation 1\n100 - SeasonalNaive with avg smape 15.97: \nModel Number: 101 of 133 with model Theta for Validation 1\n101 - Theta with avg smape 14.87: \nModel Number: 102 of 133 with model Theta for Validation 1\n102 - Theta with avg smape 14.87: \nModel Number: 103 of 133 with model FBProphet for Validation 1\nInitial log joint probability = -10.1712\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       90.6555    0.00572816        5.1892      0.8174      0.8174      119   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       90.7412    0.00100558       5.06534           1           1      233   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     225       90.7468   0.000193641       5.05495   3.871e-05       0.001      306  LS failed, Hessian reset \n     299       90.7491   2.25943e-05       5.27657           1           1      391   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     386       90.7495   9.79551e-08       4.77001      0.9273      0.9273      487   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\n103 - FBProphet with avg smape 10.26: \nModel Number: 104 of 133 with model DatepartRegression for Validation 1\n104 - DatepartRegression with avg smape 5.79: \nModel Number: 105 of 133 with model GLM for Validation 1\n105 - GLM with avg smape 16.94: \nModel Number: 106 of 133 with model ZeroesNaive for Validation 1\n106 - ZeroesNaive with avg smape 7.79: \nModel Number: 107 of 133 with model ZeroesNaive for Validation 1\n107 - ZeroesNaive with avg smape 11.76: \nModel Number: 108 of 133 with model AverageValueNaive for Validation 1\n108 - AverageValueNaive with avg smape 11.98: \nModel Number: 109 of 133 with model LastValueNaive for Validation 1\n109 - LastValueNaive with avg smape 10.84: \nModel Number: 110 of 133 with model UnobservedComponents for Validation 1\n110 - UnobservedComponents with avg smape 17.86: \nModel Number: 111 of 133 with model DatepartRegression for Validation 1\n111 - DatepartRegression with avg smape 6.64: \nModel Number: 112 of 133 with model AverageValueNaive for Validation 1\n112 - AverageValueNaive with avg smape 11.47: \nModel Number: 113 of 133 with model DatepartRegression for Validation 1\n113 - DatepartRegression with avg smape 17.97: \nModel Number: 114 of 133 with model FBProphet for Validation 1\nInitial log joint probability = -19.3463\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99        45.171   3.31607e-07       96.0893           1           1      123   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     128       45.5548   0.000757608       99.5795   8.863e-06       0.001      205  LS failed, Hessian reset \n     193       45.7188   5.33586e-06       94.1809   5.318e-08       0.001      336  LS failed, Hessian reset \n     199       45.7191   2.28651e-07        83.983      0.3836      0.3836      344   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     245       45.7192     7.835e-09       86.5039      0.3701           1      408   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n114 - FBProphet with avg smape 14.48: \nModel Number: 115 of 133 with model DatepartRegression for Validation 1\n115 - DatepartRegression with avg smape 12.22: \nModel Number: 116 of 133 with model GLS for Validation 1\n116 - GLS with avg smape 12.6: \nModel Number: 117 of 133 with model UnobservedComponents for Validation 1\n117 - UnobservedComponents with avg smape 13.85: \nModel Number: 118 of 133 with model AverageValueNaive for Validation 1\n118 - AverageValueNaive with avg smape 11.78: \nModel Number: 119 of 133 with model FBProphet for Validation 1\nInitial log joint probability = -19.3463\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       43.8692     0.0255086       143.108           1           1      116   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199        45.565   9.87318e-05       141.046   6.839e-07       0.001      265  LS failed, Hessian reset \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     275       45.5837   1.71064e-06       140.271   1.216e-08       0.001      390  LS failed, Hessian reset \n     299        45.584   1.05659e-06       132.005       0.215           1      418   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     321       45.5841   3.93126e-09       134.223      0.6527      0.6527      441   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n119 - FBProphet with avg smape 17.88: \nModel Number: 120 of 133 with model GLS for Validation 1\n120 - GLS with avg smape 11.44: \nModel Number: 121 of 133 with model GLS for Validation 1\n121 - GLS with avg smape 11.44: \nModel Number: 122 of 133 with model GLS for Validation 1\n122 - GLS with avg smape 11.44: \nModel Number: 123 of 133 with model GLS for Validation 1\n123 - GLS with avg smape 11.44: \nModel Number: 124 of 133 with model UnobservedComponents for Validation 1\n124 - UnobservedComponents with avg smape 11.14: \nModel Number: 125 of 133 with model AverageValueNaive for Validation 1\n125 - AverageValueNaive with avg smape 11.44: \nModel Number: 126 of 133 with model GLM for Validation 1\n126 - GLM with avg smape 16.03: \nModel Number: 127 of 133 with model GLS for Validation 1\n127 - GLS with avg smape 12.26: \nModel Number: 128 of 133 with model AverageValueNaive for Validation 1\n128 - AverageValueNaive with avg smape 11.44: \nModel Number: 129 of 133 with model UnobservedComponents for Validation 1\n129 - UnobservedComponents with avg smape 12.52: \nModel Number: 130 of 133 with model GLS for Validation 1\n130 - GLS with avg smape 11.57: \nModel Number: 131 of 133 with model GLM for Validation 1\n131 - GLM with avg smape 11.78: \nModel Number: 132 of 133 with model GLM for Validation 1\n132 - GLM with avg smape 11.78: \nModel Number: 133 of 133 with model AverageValueNaive for Validation 1\n133 - AverageValueNaive with avg smape 11.34: \nValidation Round: 2\nModel Number: 1 of 133 with model Ensemble for Validation 2\n1 - Ensemble with avg smape 5.48: \nModel Number: 2 of 133 with model Ensemble for Validation 2\n2 - Ensemble with avg smape 5.48: \nModel Number: 3 of 133 with model Ensemble for Validation 2\n3 - Ensemble with avg smape 4.64: \nModel Number: 4 of 133 with model Ensemble for Validation 2\n4 - Ensemble with avg smape 4.89: \nModel Number: 5 of 133 with model Ensemble for Validation 2\n5 - Ensemble with avg smape 6.7: \nModel Number: 6 of 133 with model Ensemble for Validation 2\n6 - Ensemble with avg smape 6.33: \nModel Number: 7 of 133 with model SectionalMotif for Validation 2\n7 - SectionalMotif with avg smape 8.55: \nModel Number: 8 of 133 with model NVAR for Validation 2\n8 - NVAR with avg smape 5.79: \nModel Number: 9 of 133 with model MultivariateMotif for Validation 2\n9 - MultivariateMotif with avg smape 3.73: \nModel Number: 10 of 133 with model Ensemble for Validation 2\n10 - Ensemble with avg smape 6.36: \nModel Number: 11 of 133 with model MultivariateMotif for Validation 2\n11 - MultivariateMotif with avg smape 6.89: \nModel Number: 12 of 133 with model SectionalMotif for Validation 2\n12 - SectionalMotif with avg smape 5.79: \nModel Number: 13 of 133 with model UnivariateMotif for Validation 2\n13 - UnivariateMotif with avg smape 4.91: \nModel Number: 14 of 133 with model MultivariateMotif for Validation 2\n14 - MultivariateMotif with avg smape 4.24: \nModel Number: 15 of 133 with model Ensemble for Validation 2\n15 - Ensemble with avg smape 4.76: \nModel Number: 16 of 133 with model UnivariateMotif for Validation 2\n16 - UnivariateMotif with avg smape 4.49: \nModel Number: 17 of 133 with model MultivariateMotif for Validation 2\n17 - MultivariateMotif with avg smape 5.33: \nModel Number: 18 of 133 with model MultivariateMotif for Validation 2\n18 - MultivariateMotif with avg smape 4.35: \nModel Number: 19 of 133 with model MultivariateMotif for Validation 2\n19 - MultivariateMotif with avg smape 4.35: \nModel Number: 20 of 133 with model MultivariateMotif for Validation 2\n20 - MultivariateMotif with avg smape 5.89: \nModel Number: 21 of 133 with model UnivariateMotif for Validation 2\n21 - UnivariateMotif with avg smape 6.65: \nModel Number: 22 of 133 with model UnivariateMotif for Validation 2\n22 - UnivariateMotif with avg smape 4.67: \nModel Number: 23 of 133 with model MultivariateRegression for Validation 2\n23 - MultivariateRegression with avg smape 3.54: \nModel Number: 24 of 133 with model MultivariateMotif for Validation 2\n24 - MultivariateMotif with avg smape 9.89: \nModel Number: 25 of 133 with model NVAR for Validation 2\n25 - NVAR with avg smape 10.82: \nModel Number: 26 of 133 with model WindowRegression for Validation 2\n26 - WindowRegression with avg smape 9.1: \nModel Number: 27 of 133 with model UnivariateMotif for Validation 2\n27 - UnivariateMotif with avg smape 4.48: \nModel Number: 28 of 133 with model MultivariateRegression for Validation 2\n28 - MultivariateRegression with avg smape 4.82: \nModel Number: 29 of 133 with model MultivariateRegression for Validation 2\n29 - MultivariateRegression with avg smape 4.82: \nModel Number: 30 of 133 with model MultivariateRegression for Validation 2\n30 - MultivariateRegression with avg smape 4.82: \nModel Number: 31 of 133 with model MultivariateRegression for Validation 2\n31 - MultivariateRegression with avg smape 4.82: \nModel Number: 32 of 133 with model MultivariateRegression for Validation 2\n32 - MultivariateRegression with avg smape 4.82: \nModel Number: 33 of 133 with model MultivariateRegression for Validation 2\n33 - MultivariateRegression with avg smape 4.82: \nModel Number: 34 of 133 with model ETS for Validation 2\n34 - ETS with avg smape 3.53: \nModel Number: 35 of 133 with model WindowRegression for Validation 2\n35 - WindowRegression with avg smape 5.43: \nModel Number: 36 of 133 with model NVAR for Validation 2\n36 - NVAR with avg smape 5.01: \nModel Number: 37 of 133 with model NVAR for Validation 2\n37 - NVAR with avg smape 5.01: \nModel Number: 38 of 133 with model MultivariateRegression for Validation 2\n38 - MultivariateRegression with avg smape 10.28: \nModel Number: 39 of 133 with model ETS for Validation 2\n39 - ETS with avg smape 4.37: \nModel Number: 40 of 133 with model ETS for Validation 2\nETS error ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\nETS failed on #Passengers with ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.')\n40 - ETS with avg smape 4.37: \nModel Number: 41 of 133 with model ETS for Validation 2\n41 - ETS with avg smape 4.37: \nModel Number: 42 of 133 with model SectionalMotif for Validation 2\n42 - SectionalMotif with avg smape 4.29: \nModel Number: 43 of 133 with model ETS for Validation 2\nETS error ValueError('Can only dampen the trend component')\nETS failed on #Passengers with ValueError('Can only dampen the trend component')\n43 - ETS with avg smape 3.33: \nModel Number: 44 of 133 with model UnivariateMotif for Validation 2\n44 - UnivariateMotif with avg smape 5.57: \nModel Number: 45 of 133 with model UnivariateMotif for Validation 2\n45 - UnivariateMotif with avg smape 7.77: \nModel Number: 46 of 133 with model SectionalMotif for Validation 2\n46 - SectionalMotif with avg smape 5.3: \nModel Number: 47 of 133 with model UnivariateMotif for Validation 2\n47 - UnivariateMotif with avg smape 5.45: \nModel Number: 48 of 133 with model NVAR for Validation 2\n48 - NVAR with avg smape 7.08: \nModel Number: 49 of 133 with model WindowRegression for Validation 2\n49 - WindowRegression with avg smape 5.09: \nModel Number: 50 of 133 with model WindowRegression for Validation 2\n50 - WindowRegression with avg smape 7.96: \nModel Number: 51 of 133 with model SectionalMotif for Validation 2\n51 - SectionalMotif with avg smape 9.05: \nModel Number: 52 of 133 with model ETS for Validation 2\n52 - ETS with avg smape 4.06: \nModel Number: 53 of 133 with model WindowRegression for Validation 2\n53 - WindowRegression with avg smape 6.41: \nModel Number: 54 of 133 with model WindowRegression for Validation 2\n54 - WindowRegression with avg smape 16.93: \nModel Number: 55 of 133 with model NVAR for Validation 2\n55 - NVAR with avg smape 2.53: \nModel Number: 56 of 133 with model ZeroesNaive for Validation 2\n56 - ZeroesNaive with avg smape 4.08: \nModel Number: 57 of 133 with model ZeroesNaive for Validation 2\n57 - ZeroesNaive with avg smape 4.08: \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  ConvergenceWarning,\n","output_type":"stream"},{"name":"stdout","text":"Model Number: 58 of 133 with model SeasonalNaive for Validation 2\n58 - SeasonalNaive with avg smape 14.48: \nModel Number: 59 of 133 with model WindowRegression for Validation 2\n59 - WindowRegression with avg smape 5.84: \nModel Number: 60 of 133 with model ETS for Validation 2\n60 - ETS with avg smape 7.03: \nModel Number: 61 of 133 with model NVAR for Validation 2\n61 - NVAR with avg smape 5.43: \nModel Number: 62 of 133 with model NVAR for Validation 2\n62 - NVAR with avg smape 5.63: \nModel Number: 63 of 133 with model FBProphet for Validation 2\nInitial log joint probability = -8.01174\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n       1      -8.01174             0       105.255       1e-12       0.001       20   \nOptimization terminated with error: \n  Line search failed to achieve a sufficient decrease, no more progress can be made\n\nInitial log joint probability = 68.2273\nIteration  1. Log joint probability =    47.5982. Improved by -20.6292.\nIteration  2. Log joint probability =    80.4161. Improved by 32.818.\nIteration  3. Log joint probability =    80.7241. Improved by 0.308013.\nIteration  4. Log joint probability =    81.0041. Improved by 0.279982.\nIteration  5. Log joint probability =    81.1218. Improved by 0.117709.\nIteration  6. Log joint probability =     81.316. Improved by 0.194163.\nIteration  7. Log joint probability =    81.3378. Improved by 0.0218366.\nIteration  8. Log joint probability =    81.4964. Improved by 0.15861.\nIteration  9. Log joint probability =    82.7071. Improved by 1.21072.\nIteration 10. Log joint probability =    82.7293. Improved by 0.0221207.\nIteration 11. Log joint probability =    82.7404. Improved by 0.0111111.\nIteration 12. Log joint probability =    82.7494. Improved by 0.00897401.\nIteration 13. Log joint probability =    82.7511. Improved by 0.00179516.\nIteration 14. Log joint probability =    82.7556. Improved by 0.00441625.\nIteration 15. Log joint probability =    82.7559. Improved by 0.000348779.\nIteration 16. Log joint probability =     82.756. Improved by 4.36494e-05.\nIteration 17. Log joint probability =    82.7563. Improved by 0.00036352.\nIteration 18. Log joint probability =    82.7564. Improved by 2.81213e-05.\nIteration 19. Log joint probability =    82.7567. Improved by 0.000378261.\nIteration 20. Log joint probability =    82.7567. Improved by 1.25942e-05.\nIteration 21. Log joint probability =    82.7571. Improved by 0.000393002.\nIteration 22. Log joint probability =    82.7598. Improved by 0.00268044.\nIteration 23. Log joint probability =    82.7607. Improved by 0.000898922.\nIteration 24. Log joint probability =    82.7612. Improved by 0.000474543.\nIteration 25. Log joint probability =    82.7616. Improved by 0.00038105.\nIteration 26. Log joint probability =    82.7617. Improved by 0.000136237.\nIteration 27. Log joint probability =     82.762. Improved by 0.000270043.\nIteration 28. Log joint probability =     82.762. Improved by 9.10903e-06.\nIteration 29. Log joint probability =     82.762. Improved by 1.54687e-05.\nIteration 30. Log joint probability =     82.762. Improved by 9.57733e-06.\nIteration 31. Log joint probability =     82.762. Improved by 1.49973e-05.\nIteration 32. Log joint probability =     82.762. Improved by 1.00456e-05.\nIteration 33. Log joint probability =     82.762. Improved by 1.4526e-05.\nIteration 34. Log joint probability =    82.7621. Improved by 1.02834e-05.\nIteration 35. Log joint probability =    82.7621. Improved by 1.45144e-05.\nIteration 36. Log joint probability =    82.7621. Improved by 1.02404e-05.\nIteration 37. Log joint probability =    82.7621. Improved by 1.44767e-05.\nIteration 38. Log joint probability =    82.7621. Improved by 1.04051e-05.\nIteration 39. Log joint probability =    82.7621. Improved by 1.41986e-05.\nIteration 40. Log joint probability =    82.7621. Improved by 1.07908e-05.\nIteration 41. Log joint probability =    82.7621. Improved by 1.38099e-05.\nIteration 42. Log joint probability =    82.7622. Improved by 1.11765e-05.\nIteration 43. Log joint probability =    82.7622. Improved by 1.34211e-05.\nIteration 44. Log joint probability =    82.7622. Improved by 1.15622e-05.\nIteration 45. Log joint probability =    82.7622. Improved by 1.30324e-05.\nIteration 46. Log joint probability =    82.7622. Improved by 1.19478e-05.\nIteration 47. Log joint probability =    82.7622. Improved by 1.26437e-05.\nIteration 48. Log joint probability =    82.7622. Improved by 1.23335e-05.\nIteration 49. Log joint probability =    82.7622. Improved by 1.2255e-05.\nIteration 50. Log joint probability =    82.7623. Improved by 1.27192e-05.\nIteration 51. Log joint probability =    82.7623. Improved by 1.18662e-05.\nIteration 52. Log joint probability =    82.7623. Improved by 1.31049e-05.\nIteration 53. Log joint probability =    82.7623. Improved by 1.14775e-05.\nIteration 54. Log joint probability =    82.7623. Improved by 1.34905e-05.\nIteration 55. Log joint probability =    82.7623. Improved by 1.10888e-05.\nIteration 56. Log joint probability =    82.7623. Improved by 1.38762e-05.\nIteration 57. Log joint probability =    82.7623. Improved by 1.07001e-05.\nIteration 58. Log joint probability =    82.7624. Improved by 1.42619e-05.\nIteration 59. Log joint probability =    82.7624. Improved by 1.03113e-05.\nIteration 60. Log joint probability =    82.7624. Improved by 1.46476e-05.\nIteration 61. Log joint probability =    82.7624. Improved by 9.92262e-06.\nIteration 62. Log joint probability =    82.7624. Improved by 1.48911e-05.\nIteration 63. Log joint probability =    82.7624. Improved by 9.8174e-06.\nIteration 64. Log joint probability =    82.7624. Improved by 1.48494e-05.\nIteration 65. Log joint probability =    82.7624. Improved by 1.00007e-05.\nIteration 66. Log joint probability =    82.7625. Improved by 1.4663e-05.\nIteration 67. Log joint probability =    82.7625. Improved by 1.0184e-05.\nIteration 68. Log joint probability =    82.7625. Improved by 1.43685e-05.\nIteration 69. Log joint probability =    82.7625. Improved by 1.05009e-05.\nIteration 70. Log joint probability =    82.7625. Improved by 1.41104e-05.\nIteration 71. Log joint probability =    82.7625. Improved by 1.06595e-05.\nIteration 72. Log joint probability =    82.7625. Improved by 1.40661e-05.\nIteration 73. Log joint probability =    82.7625. Improved by 1.07008e-05.\nIteration 74. Log joint probability =    82.7626. Improved by 1.40218e-05.\nIteration 75. Log joint probability =    82.7626. Improved by 1.07421e-05.\nIteration 76. Log joint probability =    82.7626. Improved by 1.39775e-05.\nIteration 77. Log joint probability =    82.7626. Improved by 1.07833e-05.\nIteration 78. Log joint probability =    82.7626. Improved by 1.39331e-05.\nIteration 79. Log joint probability =    82.7626. Improved by 1.08246e-05.\nIteration 80. Log joint probability =    82.7626. Improved by 1.38888e-05.\nIteration 81. Log joint probability =    82.7626. Improved by 1.08659e-05.\nIteration 82. Log joint probability =    82.7627. Improved by 1.38445e-05.\nIteration 83. Log joint probability =    82.7627. Improved by 1.09071e-05.\nIteration 84. Log joint probability =    82.7627. Improved by 1.38002e-05.\nIteration 85. Log joint probability =    82.7627. Improved by 1.09484e-05.\nIteration 86. Log joint probability =    82.7627. Improved by 1.37559e-05.\nIteration 87. Log joint probability =    82.7627. Improved by 1.09897e-05.\nIteration 88. Log joint probability =    82.7627. Improved by 1.37116e-05.\nIteration 89. Log joint probability =    82.7627. Improved by 1.10309e-05.\nIteration 90. Log joint probability =    82.7628. Improved by 1.36673e-05.\nIteration 91. Log joint probability =    82.7628. Improved by 1.10722e-05.\nIteration 92. Log joint probability =    82.7628. Improved by 1.36229e-05.\nIteration 93. Log joint probability =    82.7628. Improved by 1.11135e-05.\nIteration 94. Log joint probability =    82.7628. Improved by 1.35786e-05.\nIteration 95. Log joint probability =    82.7628. Improved by 1.11548e-05.\nIteration 96. Log joint probability =    82.7628. Improved by 1.35343e-05.\nIteration 97. Log joint probability =    82.7628. Improved by 1.1196e-05.\nIteration 98. Log joint probability =    82.7629. Improved by 1.349e-05.\nIteration 99. Log joint probability =    82.7629. Improved by 1.12373e-05.\nIteration 100. Log joint probability =    82.7629. Improved by 1.34457e-05.\nIteration 101. Log joint probability =    82.7629. Improved by 1.12786e-05.\nIteration 102. Log joint probability =    82.7629. Improved by 1.34014e-05.\nIteration 103. Log joint probability =    82.7629. Improved by 1.13198e-05.\nIteration 104. Log joint probability =    82.7629. Improved by 1.33571e-05.\nIteration 105. Log joint probability =    82.7629. Improved by 1.13611e-05.\nIteration 106. Log joint probability =     82.763. Improved by 1.33127e-05.\nIteration 107. Log joint probability =     82.763. Improved by 1.14024e-05.\nIteration 108. Log joint probability =     82.763. Improved by 1.32684e-05.\nIteration 109. Log joint probability =     82.763. Improved by 1.14437e-05.\nIteration 110. Log joint probability =     82.763. Improved by 1.32241e-05.\nIteration 111. Log joint probability =     82.763. Improved by 1.14849e-05.\nIteration 112. Log joint probability =     82.763. Improved by 1.31798e-05.\nIteration 113. Log joint probability =     82.763. Improved by 1.15262e-05.\nIteration 114. Log joint probability =    82.7631. Improved by 1.31355e-05.\nIteration 115. Log joint probability =    82.7631. Improved by 1.15675e-05.\nIteration 116. Log joint probability =    82.7631. Improved by 1.30912e-05.\nIteration 117. Log joint probability =    82.7631. Improved by 1.16088e-05.\nIteration 118. Log joint probability =    82.7631. Improved by 1.30469e-05.\nIteration 119. Log joint probability =    82.7631. Improved by 1.165e-05.\nIteration 120. Log joint probability =    82.7631. Improved by 1.30025e-05.\nIteration 121. Log joint probability =    82.7631. Improved by 1.16913e-05.\nIteration 122. Log joint probability =    82.7631. Improved by 1.29582e-05.\nIteration 123. Log joint probability =    82.7632. Improved by 1.17326e-05.\nIteration 124. Log joint probability =    82.7632. Improved by 1.29139e-05.\nIteration 125. Log joint probability =    82.7632. Improved by 1.17739e-05.\nIteration 126. Log joint probability =    82.7632. Improved by 1.28696e-05.\nIteration 127. Log joint probability =    82.7632. Improved by 1.18151e-05.\nIteration 128. Log joint probability =    82.7632. Improved by 1.28253e-05.\nIteration 129. Log joint probability =    82.7632. Improved by 1.18564e-05.\nIteration 130. Log joint probability =    82.7632. Improved by 1.2781e-05.\nIteration 131. Log joint probability =    82.7633. Improved by 1.18977e-05.\nIteration 132. Log joint probability =    82.7633. Improved by 1.27367e-05.\nIteration 133. Log joint probability =    82.7633. Improved by 1.1939e-05.\nIteration 134. Log joint probability =    82.7633. Improved by 1.26923e-05.\nIteration 135. Log joint probability =    82.7633. Improved by 1.19802e-05.\nIteration 136. Log joint probability =    82.7633. Improved by 1.2648e-05.\nIteration 137. Log joint probability =    82.7633. Improved by 1.20215e-05.\nIteration 138. Log joint probability =    82.7633. Improved by 1.26037e-05.\nIteration 139. Log joint probability =    82.7634. Improved by 1.20628e-05.\nIteration 140. Log joint probability =    82.7634. Improved by 1.25594e-05.\nIteration 141. Log joint probability =    82.7634. Improved by 1.21041e-05.\nIteration 142. Log joint probability =    82.7634. Improved by 1.25151e-05.\nIteration 143. Log joint probability =    82.7634. Improved by 1.19525e-05.\nIteration 144. Log joint probability =    82.7634. Improved by 1.28573e-05.\nIteration 145. Log joint probability =    82.7634. Improved by 1.15751e-05.\nIteration 146. Log joint probability =    82.7634. Improved by 1.3263e-05.\nIteration 147. Log joint probability =    82.7635. Improved by 1.11664e-05.\nIteration 148. Log joint probability =    82.7635. Improved by 1.36687e-05.\nIteration 149. Log joint probability =    82.7635. Improved by 1.07576e-05.\nIteration 150. Log joint probability =    82.7635. Improved by 1.40744e-05.\nIteration 151. Log joint probability =    82.7635. Improved by 1.03489e-05.\nIteration 152. Log joint probability =    82.7635. Improved by 1.44801e-05.\nIteration 153. Log joint probability =    82.7635. Improved by 9.94014e-06.\nIteration 154. Log joint probability =    82.7635. Improved by 1.48859e-05.\nIteration 155. Log joint probability =    82.7636. Improved by 9.53139e-06.\nIteration 156. Log joint probability =    82.7636. Improved by 1.52916e-05.\nIteration 157. Log joint probability =    82.7636. Improved by 9.12265e-06.\nIteration 158. Log joint probability =    82.7636. Improved by 1.56973e-05.\nIteration 159. Log joint probability =    82.7636. Improved by 8.71392e-06.\nIteration 160. Log joint probability =    82.7636. Improved by 1.6103e-05.\nIteration 161. Log joint probability =    82.7636. Improved by 8.30518e-06.\nIteration 162. Log joint probability =    82.7636. Improved by 1.65087e-05.\nIteration 163. Log joint probability =    82.7637. Improved by 7.89644e-06.\nIteration 164. Log joint probability =    82.7637. Improved by 1.69144e-05.\nIteration 165. Log joint probability =    82.7637. Improved by 7.4877e-06.\nIteration 166. Log joint probability =    82.7637. Improved by 1.73201e-05.\nIteration 167. Log joint probability =    82.7637. Improved by 7.07896e-06.\nIteration 168. Log joint probability =    82.7637. Improved by 1.77258e-05.\nIteration 169. Log joint probability =    82.7637. Improved by 6.67023e-06.\nIteration 170. Log joint probability =    82.7637. Improved by 1.81315e-05.\nIteration 171. Log joint probability =    82.7637. Improved by 6.26149e-06.\nIteration 172. Log joint probability =    82.7638. Improved by 1.85372e-05.\nIteration 173. Log joint probability =    82.7638. Improved by 5.85276e-06.\nIteration 174. Log joint probability =    82.7638. Improved by 1.89429e-05.\nIteration 175. Log joint probability =    82.7638. Improved by 5.44403e-06.\nIteration 176. Log joint probability =    82.7638. Improved by 1.93486e-05.\nIteration 177. Log joint probability =    82.7638. Improved by 5.03529e-06.\nIteration 178. Log joint probability =    82.7638. Improved by 1.97543e-05.\nIteration 179. Log joint probability =    82.7638. Improved by 4.62656e-06.\nIteration 180. Log joint probability =    82.7639. Improved by 2.016e-05.\nIteration 181. Log joint probability =    82.7639. Improved by 4.21783e-06.\nIteration 182. Log joint probability =    82.7639. Improved by 2.05658e-05.\nIteration 183. Log joint probability =    82.7639. Improved by 3.8091e-06.\nIteration 184. Log joint probability =    82.7639. Improved by 2.09715e-05.\nIteration 185. Log joint probability =    82.7639. Improved by 3.40037e-06.\nIteration 186. Log joint probability =    82.7639. Improved by 2.13772e-05.\nIteration 187. Log joint probability =    82.7639. Improved by 2.99164e-06.\nIteration 188. Log joint probability =     82.764. Improved by 2.17829e-05.\nIteration 189. Log joint probability =     82.764. Improved by 2.58291e-06.\nIteration 190. Log joint probability =     82.764. Improved by 2.21886e-05.\nIteration 191. Log joint probability =     82.764. Improved by 2.17418e-06.\nIteration 192. Log joint probability =     82.764. Improved by 2.25943e-05.\nIteration 193. Log joint probability =     82.764. Improved by 1.76545e-06.\nIteration 194. Log joint probability =     82.764. Improved by 2.3e-05.\nIteration 195. Log joint probability =     82.764. Improved by 1.35672e-06.\nIteration 196. Log joint probability =    82.7641. Improved by 2.34057e-05.\nIteration 197. Log joint probability =    82.7641. Improved by 9.0373e-07.\nIteration 198. Log joint probability =    82.7641. Improved by 2.39029e-05.\nIteration 199. Log joint probability =    82.7641. Improved by 2.40364e-07.\nIteration 200. Log joint probability =    82.7641. Improved by 2.47234e-05.\nIteration 201. Log joint probability =    82.7643. Improved by 0.000203535.\nIteration 202. Log joint probability =    82.7643. Improved by 2.48493e-05.\nIteration 203. Log joint probability =    82.7644. Improved by 1.98703e-05.\nIteration 204. Log joint probability =    82.7644. Improved by 3.66068e-05.\nIteration 205. Log joint probability =    82.7644. Improved by 1.3784e-05.\nIteration 206. Log joint probability =    82.7644. Improved by 4.63714e-06.\nIteration 207. Log joint probability =    82.7644. Improved by 7.57105e-06.\nIteration 208. Log joint probability =    82.7644. Improved by 1.63534e-06.\nIteration 209. Log joint probability =    82.7644. Improved by 4.96303e-06.\nIteration 210. Log joint probability =    82.7644. Improved by 2.33429e-06.\nIteration 211. Log joint probability =    82.7644. Improved by 9.78864e-07.\nIteration 212. Log joint probability =    82.7644. Improved by 6.33106e-07.\nIteration 213. Log joint probability =    82.7644. Improved by 3.27406e-07.\nIteration 214. Log joint probability =    82.7644. Improved by 1.23514e-07.\nIteration 215. Log joint probability =    82.7644. Improved by 7.03313e-08.\nIteration 216. Log joint probability =    82.7644. Improved by 4.20864e-08.\nIteration 217. Log joint probability =    82.7644. Improved by 1.33223e-08.\nIteration 218. Log joint probability =    82.7644. Improved by 4.82102e-09.\n63 - FBProphet with avg smape 8.11: \nModel Number: 64 of 133 with model FBProphet for Validation 2\nInitial log joint probability = -8.01174\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      64       87.6343   7.58521e-07        101.47   7.522e-09       0.001      116  LS failed, Hessian reset \n      92       87.6344   1.83697e-07       100.986   1.804e-09       0.001      194  LS failed, Hessian reset \n      99       87.6344    1.1531e-08       99.3424      0.5317      0.5317      201   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     116       87.6346   6.66999e-07       97.5906   6.547e-09       0.001      261  LS failed, Hessian reset \n     138       87.6346   7.29414e-09       100.668      0.4748      0.4748      289   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n64 - FBProphet with avg smape 12.44: \nModel Number: 65 of 133 with model SectionalMotif for Validation 2\n65 - SectionalMotif with avg smape 6.05: \nModel Number: 66 of 133 with model ZeroesNaive for Validation 2\n66 - ZeroesNaive with avg smape 3.71: \nModel Number: 67 of 133 with model ZeroesNaive for Validation 2\n67 - ZeroesNaive with avg smape 2.94: \nModel Number: 68 of 133 with model LastValueNaive for Validation 2\n68 - LastValueNaive with avg smape 4.34: \nModel Number: 69 of 133 with model LastValueNaive for Validation 2\n69 - LastValueNaive with avg smape 4.34: \nModel Number: 70 of 133 with model FBProphet for Validation 2\nInitial log joint probability = -8.31849\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      68       64.9143   9.15065e-09        93.431      0.3452           1       91   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n70 - FBProphet with avg smape 7.09: \nModel Number: 71 of 133 with model ZeroesNaive for Validation 2\n71 - ZeroesNaive with avg smape 3.65: \nModel Number: 72 of 133 with model SeasonalNaive for Validation 2\n72 - SeasonalNaive with avg smape 13.02: \nModel Number: 73 of 133 with model FBProphet for Validation 2\nInitial log joint probability = -11.0557\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      88       49.7633   7.79484e-09       100.803      0.3928           1      127   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n73 - FBProphet with avg smape 7.5: \nModel Number: 74 of 133 with model DatepartRegression for Validation 2\n74 - DatepartRegression with avg smape 5.96: \nModel Number: 75 of 133 with model SectionalMotif for Validation 2\n75 - SectionalMotif with avg smape 7.81: \nModel Number: 76 of 133 with model SectionalMotif for Validation 2\n76 - SectionalMotif with avg smape 8.29: \nModel Number: 77 of 133 with model LastValueNaive for Validation 2\n77 - LastValueNaive with avg smape 3.22: \nModel Number: 78 of 133 with model SeasonalNaive for Validation 2\n78 - SeasonalNaive with avg smape 15.13: \nModel Number: 79 of 133 with model FBProphet for Validation 2\nInitial log joint probability = -10.6729\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      57       77.0896    7.5488e-05       104.736   7.645e-07       0.001      106  LS failed, Hessian reset \n      99        77.093   5.60796e-06       102.676           1           1      156   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     101       77.0933   3.75173e-06       101.108   4.089e-08       0.001      194  LS failed, Hessian reset \n     156       77.0942   6.84225e-06       98.8303   6.652e-08       0.001      298  LS failed, Hessian reset \n     180       77.0946    4.5444e-09       93.0925      0.2704      0.2704      329   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n79 - FBProphet with avg smape 7.07: \nModel Number: 80 of 133 with model LastValueNaive for Validation 2\n80 - LastValueNaive with avg smape 6.13: \nModel Number: 81 of 133 with model DatepartRegression for Validation 2\n81 - DatepartRegression with avg smape 6.33: \nModel Number: 82 of 133 with model SeasonalNaive for Validation 2\n82 - SeasonalNaive with avg smape 14.6: \nModel Number: 83 of 133 with model LastValueNaive for Validation 2\n83 - LastValueNaive with avg smape 6.1: \nModel Number: 84 of 133 with model Theta for Validation 2\n84 - Theta with avg smape 6.95: \nModel Number: 85 of 133 with model Theta for Validation 2\n85 - Theta with avg smape 6.95: \nModel Number: 86 of 133 with model WindowRegression for Validation 2\nTemplate Eval Error: ValueError('Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required.') in model 86: WindowRegression\nModel Number: 87 of 133 with model SeasonalNaive for Validation 2\n87 - SeasonalNaive with avg smape 9.4: \nModel Number: 88 of 133 with model Theta for Validation 2\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f= -8.90069D-01    |proj g|=  3.98466D-01\n\nAt iterate    5    f= -8.90724D-01    |proj g|=  1.40333D-06\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      5      8      1     0     0   1.403D-06  -8.907D-01\n  F = -0.89072366411162784     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n88 - Theta with avg smape 6.99: \nModel Number: 89 of 133 with model Theta for Validation 2\n","output_type":"stream"},{"name":"stderr","text":" This problem is unconstrained.\n This problem is unconstrained.\n","output_type":"stream"},{"name":"stdout","text":"RUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            2     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f= -1.85463D+00    |proj g|=  1.04999D+00\n\nAt iterate    5    f= -1.85528D+00    |proj g|=  1.25648D-06\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    2      5      9      1     0     0   1.256D-06  -1.855D+00\n  F =  -1.8552806641310449     \n\nCONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n89 - Theta with avg smape 6.99: \nModel Number: 90 of 133 with model ETS for Validation 2\n90 - ETS with avg smape 3.64: \nModel Number: 91 of 133 with model GLM for Validation 2\n91 - GLM with avg smape 7.75: \nModel Number: 92 of 133 with model Theta for Validation 2\n92 - Theta with avg smape 7.05: \nModel Number: 93 of 133 with model LastValueNaive for Validation 2\n93 - LastValueNaive with avg smape 13.24: \nModel Number: 94 of 133 with model SeasonalNaive for Validation 2\n94 - SeasonalNaive with avg smape 9.69: \nModel Number: 95 of 133 with model ZeroesNaive for Validation 2\n95 - ZeroesNaive with avg smape 13.37: \nModel Number: 96 of 133 with model LastValueNaive for Validation 2\n96 - LastValueNaive with avg smape 6.76: \nModel Number: 97 of 133 with model DatepartRegression for Validation 2\n97 - DatepartRegression with avg smape 11.55: \nModel Number: 98 of 133 with model Theta for Validation 2\n98 - Theta with avg smape 9.88: \nModel Number: 99 of 133 with model SeasonalNaive for Validation 2\n99 - SeasonalNaive with avg smape 5.53: \nModel Number: 100 of 133 with model SeasonalNaive for Validation 2\n100 - SeasonalNaive with avg smape 5.53: \nModel Number: 101 of 133 with model Theta for Validation 2\n101 - Theta with avg smape 9.92: \nModel Number: 102 of 133 with model Theta for Validation 2\n102 - Theta with avg smape 9.92: \nModel Number: 103 of 133 with model FBProphet for Validation 2\nInitial log joint probability = -11.0246\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       76.0295    0.00235721       5.19122           1           1      120   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     143       76.0587   0.000336021       3.80163   6.501e-05       0.001      211  LS failed, Hessian reset \n     199       76.0657    0.00409565        6.0122           1           1      280   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       76.0718   9.33046e-07       4.55131           1           1      413   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     358        76.072   1.21548e-05       4.55518   2.282e-06       0.001      521  LS failed, Hessian reset \n     399       76.0721   7.57616e-07       4.74144           1           1      570   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     424       76.0721   1.32224e-07       5.28107      0.3501           1      603   \nOptimization terminated normally: \n  Convergence detected: relative gradient magnitude is below tolerance\n103 - FBProphet with avg smape 32.94: \nModel Number: 104 of 133 with model DatepartRegression for Validation 2\n104 - DatepartRegression with avg smape 12.0: \nModel Number: 105 of 133 with model GLM for Validation 2\n105 - GLM with avg smape 6.47: \nModel Number: 106 of 133 with model ZeroesNaive for Validation 2\n106 - ZeroesNaive with avg smape 3.22: \nModel Number: 107 of 133 with model ZeroesNaive for Validation 2\n107 - ZeroesNaive with avg smape 3.22: \nModel Number: 108 of 133 with model AverageValueNaive for Validation 2\n108 - AverageValueNaive with avg smape 13.83: \nModel Number: 109 of 133 with model LastValueNaive for Validation 2\n109 - LastValueNaive with avg smape 6.76: \nModel Number: 110 of 133 with model UnobservedComponents for Validation 2\n110 - UnobservedComponents with avg smape 12.91: \nModel Number: 111 of 133 with model DatepartRegression for Validation 2\n111 - DatepartRegression with avg smape 14.44: \nModel Number: 112 of 133 with model AverageValueNaive for Validation 2\n112 - AverageValueNaive with avg smape 14.61: \nModel Number: 113 of 133 with model DatepartRegression for Validation 2\n113 - DatepartRegression with avg smape 6.1: \nModel Number: 114 of 133 with model FBProphet for Validation 2\nInitial log joint probability = -15.6216\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99        45.026   2.36385e-06       92.3815      0.2217      0.2217      118   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       45.0516   1.58566e-06       95.1238           1           1      243   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     200       45.0516   1.97999e-07       90.0377   2.081e-09       0.001      287  LS failed, Hessian reset \n     204       45.0516   9.61116e-09       88.5336      0.2192      0.2192      292   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n114 - FBProphet with avg smape 8.78: \nModel Number: 115 of 133 with model DatepartRegression for Validation 2\n115 - DatepartRegression with avg smape 12.53: \nModel Number: 116 of 133 with model GLS for Validation 2\n116 - GLS with avg smape 16.89: \nModel Number: 117 of 133 with model UnobservedComponents for Validation 2\n117 - UnobservedComponents with avg smape 12.09: \nModel Number: 118 of 133 with model AverageValueNaive for Validation 2\n118 - AverageValueNaive with avg smape 16.07: \nModel Number: 119 of 133 with model FBProphet for Validation 2\nInitial log joint probability = -15.6216\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       43.5845     0.0246557       145.061           1           1      113   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     176       44.8557   0.000697415       141.438   4.883e-06       0.001      246  LS failed, Hessian reset \n     199       44.9532   7.02181e-05       143.809           1           1      273   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299        45.069   3.82802e-08       141.283      0.9611      0.9611      391   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     304        45.069   6.21423e-09       141.968      0.6667      0.6667      397   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n119 - FBProphet with avg smape 9.21: \nModel Number: 120 of 133 with model GLS for Validation 2\n120 - GLS with avg smape 12.78: \nModel Number: 121 of 133 with model GLS for Validation 2\n121 - GLS with avg smape 12.78: \nModel Number: 122 of 133 with model GLS for Validation 2\n122 - GLS with avg smape 12.78: \nModel Number: 123 of 133 with model GLS for Validation 2\n123 - GLS with avg smape 12.78: \nModel Number: 124 of 133 with model UnobservedComponents for Validation 2\n124 - UnobservedComponents with avg smape 13.21: \nModel Number: 125 of 133 with model AverageValueNaive for Validation 2\n125 - AverageValueNaive with avg smape 12.67: \nModel Number: 126 of 133 with model GLM for Validation 2\n126 - GLM with avg smape 11.86: \nModel Number: 127 of 133 with model GLS for Validation 2\n127 - GLS with avg smape 16.56: \nModel Number: 128 of 133 with model AverageValueNaive for Validation 2\n128 - AverageValueNaive with avg smape 12.68: \nModel Number: 129 of 133 with model UnobservedComponents for Validation 2\n129 - UnobservedComponents with avg smape 12.36: \nModel Number: 130 of 133 with model GLS for Validation 2\n130 - GLS with avg smape 13.14: \nModel Number: 131 of 133 with model GLM for Validation 2\n131 - GLM with avg smape 12.21: \nModel Number: 132 of 133 with model GLM for Validation 2\n132 - GLM with avg smape 12.21: \nModel Number: 133 of 133 with model AverageValueNaive for Validation 2\n133 - AverageValueNaive with avg smape 12.96: \n","output_type":"stream"}]},{"cell_type":"code","source":"prediction = model.predict()\nforecast = prediction.forecast\nprint(\"Passengers Forecast\")\nprint(forecast)","metadata":{"id":"gMrB2n4OYiZ0","outputId":"ce4e3188-955f-4da4-f586-e2f73df7ceff","execution":{"iopub.status.busy":"2022-05-05T08:41:10.812799Z","iopub.execute_input":"2022-05-05T08:41:10.813179Z","iopub.status.idle":"2022-05-05T08:41:11.141821Z","shell.execute_reply.started":"2022-05-05T08:41:10.813147Z","shell.execute_reply":"2022-05-05T08:41:11.140945Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Passengers Forecast\n            #Passengers\n1961-01-01   443.053324\n1961-02-01   417.053324\n1961-03-01   444.966031\n1961-04-01   486.966031\n1961-05-01   497.966031\n1961-06-01   560.966031\n1961-07-01   647.966031\n1961-08-01   631.966031\n1961-09-01   533.966031\n1961-10-01   486.966031\n1961-11-01   415.966031\n1961-12-01   457.966031\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\ndata_air=data.set_index('month')\nplt.plot(np.arange(144), data_air)\nplt.plot(np.arange(144, 144+12), forecast)\nplt.show()","metadata":{"id":"yhslxHrMYxbj","outputId":"3415521e-be45-4120-f2f0-adfd8ada31ed","execution":{"iopub.status.busy":"2022-05-05T08:41:11.143423Z","iopub.execute_input":"2022-05-05T08:41:11.143734Z","iopub.status.idle":"2022-05-05T08:41:11.332957Z","shell.execute_reply.started":"2022-05-05T08:41:11.143693Z","shell.execute_reply":"2022-05-05T08:41:11.332083Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Darts","metadata":{"id":"OqNl2R3gpHlP"}},{"cell_type":"markdown","source":"pip install darts","metadata":{"id":"UwQ6qX9S2gzI","outputId":"5bd53c67-b39c-4c20-df74-9b3d41fb2e28","_kg_hide-output":true}},{"cell_type":"code","source":"#Loading the package\nfrom darts import TimeSeries\nfrom darts.models import ExponentialSmoothing\n\n# Create a TimeSeries, specifying the time and value columns\nseries = TimeSeries.from_dataframe(data, 'month', '#Passengers')\n\n# Set aside the last 36 months as a validation series\ntrain, val = series[:-36], series[-36:]","metadata":{"id":"trST7FXzpHlQ","execution":{"iopub.status.busy":"2022-05-05T08:41:11.334107Z","iopub.execute_input":"2022-05-05T08:41:11.334322Z","iopub.status.idle":"2022-05-05T08:41:13.076076Z","shell.execute_reply.started":"2022-05-05T08:41:11.334296Z","shell.execute_reply":"2022-05-05T08:41:13.075026Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from darts.models import ExponentialSmoothing\n\nmodel = ExponentialSmoothing()\nmodel.fit(train)\nprediction = model.predict(len(val), num_samples=1000)","metadata":{"id":"VJsHsNAvpHlR","execution":{"iopub.status.busy":"2022-05-05T08:41:13.077686Z","iopub.execute_input":"2022-05-05T08:41:13.077930Z","iopub.status.idle":"2022-05-05T08:41:13.219694Z","shell.execute_reply.started":"2022-05-05T08:41:13.077897Z","shell.execute_reply":"2022-05-05T08:41:13.219085Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/statsmodels/tsa/holtwinters/model.py:429: FutureWarning: After 0.13 initialization must be handled at model creation\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"series.plot()\nprediction.plot(label='forecast', low_quantile=0.05, high_quantile=0.95)\nplt.legend()","metadata":{"id":"nbsM233upHlS","outputId":"5ff33c52-4914-474c-ec06-9670d66868e5","execution":{"iopub.status.busy":"2022-05-05T08:41:13.220823Z","iopub.execute_input":"2022-05-05T08:41:13.223146Z","iopub.status.idle":"2022-05-05T08:41:13.754784Z","shell.execute_reply.started":"2022-05-05T08:41:13.223110Z","shell.execute_reply":"2022-05-05T08:41:13.753932Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<matplotlib.legend.Legend at 0x7fe9786f9810>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Kats","metadata":{"id":"RH4AyhfVpHlV"}},{"cell_type":"markdown","source":"pip install kats","metadata":{"id":"_WU0j1ZjzYRz","outputId":"fa27dfaf-b665-4ee6-cb00-523002d2a547","_kg_hide-output":true}},{"cell_type":"code","source":"from kats.consts import TimeSeriesData\nfrom kats.models.prophet import ProphetModel, ProphetParams","metadata":{"id":"qxWecFEEpHlW","execution":{"iopub.status.busy":"2022-05-05T08:41:13.756598Z","iopub.execute_input":"2022-05-05T08:41:13.756935Z","iopub.status.idle":"2022-05-05T08:41:14.497624Z","shell.execute_reply.started":"2022-05-05T08:41:13.756890Z","shell.execute_reply":"2022-05-05T08:41:14.496889Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data.columns = ['month','#Passengers']\ndata['month'] = pd.to_datetime(data['month'],infer_datetime_format=True,format='%y%m')","metadata":{"id":"CTYJuYAZexl8","execution":{"iopub.status.busy":"2022-05-05T08:41:14.499331Z","iopub.execute_input":"2022-05-05T08:41:14.499817Z","iopub.status.idle":"2022-05-05T08:41:14.510577Z","shell.execute_reply.started":"2022-05-05T08:41:14.499772Z","shell.execute_reply":"2022-05-05T08:41:14.509642Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df_s = TimeSeriesData(time=data['month'], value=data['#Passengers'])\ndf_s","metadata":{"id":"mSXq-x5fgxVt","outputId":"9c7021c6-0f79-475c-8eee-cdccd796bbb3","execution":{"iopub.status.busy":"2022-05-05T08:41:14.512625Z","iopub.execute_input":"2022-05-05T08:41:14.512953Z","iopub.status.idle":"2022-05-05T08:41:14.536797Z","shell.execute_reply.started":"2022-05-05T08:41:14.512910Z","shell.execute_reply":"2022-05-05T08:41:14.535922Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"         month  #Passengers\n0   1949-01-01          112\n1   1949-02-01          118\n2   1949-03-01          132\n3   1949-04-01          129\n4   1949-05-01          121\n..         ...          ...\n139 1960-08-01          606\n140 1960-09-01          508\n141 1960-10-01          461\n142 1960-11-01          390\n143 1960-12-01          432\n\n[144 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>month</th>\n      <th>#Passengers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1949-01-01</td>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1949-02-01</td>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1949-03-01</td>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1949-04-01</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1949-05-01</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>1960-08-01</td>\n      <td>606</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>1960-09-01</td>\n      <td>508</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>1960-10-01</td>\n      <td>461</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>1960-11-01</td>\n      <td>390</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>1960-12-01</td>\n      <td>432</td>\n    </tr>\n  </tbody>\n</table>\n<p>144 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# create a model param instance\nparams = ProphetParams(seasonality_mode='multiplicative')\n\n# create a prophet model instance\nmodel = ProphetModel(df_s, params)\n\n# fit model simply by calling m.fit()\nmodel.fit()\n\n# make prediction for next 30 month\nforecast = model.predict(steps=30, freq=\"MS\")\nforecast.head()","metadata":{"id":"AR4fMVJmpHlX","outputId":"fda97c32-b11a-417e-9689-cd06d99f3a54","execution":{"iopub.status.busy":"2022-05-05T08:41:14.538394Z","iopub.execute_input":"2022-05-05T08:41:14.538715Z","iopub.status.idle":"2022-05-05T08:41:16.773989Z","shell.execute_reply.started":"2022-05-05T08:41:14.538673Z","shell.execute_reply":"2022-05-05T08:41:16.773391Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Initial log joint probability = -2.46502\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       501.449     0.0176543       240.422       0.649           1      137   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       503.353    0.00108498       90.0341      0.3062      0.3062      271   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       503.446   0.000140197        80.679           1           1      396   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     303       503.467   9.96247e-05       114.472   7.079e-07       0.001      451  LS failed, Hessian reset \n     359       503.486   8.82342e-09       72.6598    0.004982           1      538   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"        time        fcst  fcst_lower  fcst_upper\n0 1961-01-01  452.077721  437.969393  464.900288\n1 1961-02-01  433.529496  420.675247  446.622315\n2 1961-03-01  492.499917  480.237041  505.872860\n3 1961-04-01  495.895518  482.655084  509.877115\n4 1961-05-01  504.532773  491.436447  517.733568","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>fcst</th>\n      <th>fcst_lower</th>\n      <th>fcst_upper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1961-01-01</td>\n      <td>452.077721</td>\n      <td>437.969393</td>\n      <td>464.900288</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1961-02-01</td>\n      <td>433.529496</td>\n      <td>420.675247</td>\n      <td>446.622315</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1961-03-01</td>\n      <td>492.499917</td>\n      <td>480.237041</td>\n      <td>505.872860</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1961-04-01</td>\n      <td>495.895518</td>\n      <td>482.655084</td>\n      <td>509.877115</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1961-05-01</td>\n      <td>504.532773</td>\n      <td>491.436447</td>\n      <td>517.733568</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_s.plot(cols=['#Passengers'])","metadata":{"id":"_bJVFegBjYFT","outputId":"77dad35b-8384-4489-a48d-417e3dfc28cc","execution":{"iopub.status.busy":"2022-05-05T08:41:16.775057Z","iopub.execute_input":"2022-05-05T08:41:16.775368Z","iopub.status.idle":"2022-05-05T08:41:17.179614Z","shell.execute_reply.started":"2022-05-05T08:41:16.775340Z","shell.execute_reply":"2022-05-05T08:41:17.178738Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='month'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"# Creating two slices\nts_1 = df_s[0:3]\nts_2 = df_s[3:7]\n \nts_1.extend(ts_2)\nts_1","metadata":{"id":"atbp5I7CtmZR","outputId":"5343e1c0-d82c-453b-9332-69b479eb0f59","execution":{"iopub.status.busy":"2022-05-05T08:41:17.181066Z","iopub.execute_input":"2022-05-05T08:41:17.181335Z","iopub.status.idle":"2022-05-05T08:41:17.199316Z","shell.execute_reply.started":"2022-05-05T08:41:17.181298Z","shell.execute_reply":"2022-05-05T08:41:17.198336Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"       month  #Passengers\n0 1949-01-01          112\n1 1949-02-01          118\n2 1949-03-01          132\n3 1949-04-01          129\n4 1949-05-01          121\n5 1949-06-01          135\n6 1949-07-01          148","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>month</th>\n      <th>#Passengers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1949-01-01</td>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1949-02-01</td>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1949-03-01</td>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1949-04-01</td>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1949-05-01</td>\n      <td>121</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1949-06-01</td>\n      <td>135</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1949-07-01</td>\n      <td>148</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# create a model param instance\nparams = ProphetParams(seasonality_mode='multiplicative')\n\n# create a prophet model instance\nmodel = ProphetModel(df_s, params)\n\n# fit model simply by calling m.fit()\nmodel.fit()\n\n# make prediction for next 30 month\nforecast = model.predict(steps=30, freq=\"MS\")\nforecast.head()","metadata":{"id":"VNLIMW8rtux5","outputId":"fc866be3-d1eb-463e-ba04-f4d5c070bb7c","execution":{"iopub.status.busy":"2022-05-05T08:41:17.201056Z","iopub.execute_input":"2022-05-05T08:41:17.201815Z","iopub.status.idle":"2022-05-05T08:41:19.432875Z","shell.execute_reply.started":"2022-05-05T08:41:17.201767Z","shell.execute_reply":"2022-05-05T08:41:19.432079Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Initial log joint probability = -2.46502\n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n      99       501.449     0.0176543       240.422       0.649           1      137   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     199       503.353    0.00108498       90.0341      0.3062      0.3062      271   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     299       503.446   0.000140197        80.679           1           1      396   \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes \n     303       503.467   9.96247e-05       114.472   7.079e-07       0.001      451  LS failed, Hessian reset \n     359       503.486   8.82342e-09       72.6598    0.004982           1      538   \nOptimization terminated normally: \n  Convergence detected: absolute parameter change was below tolerance\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"        time        fcst  fcst_lower  fcst_upper\n0 1961-01-01  452.077721  438.133669  465.379116\n1 1961-02-01  433.529496  420.569616  446.864959\n2 1961-03-01  492.499917  478.899274  505.942641\n3 1961-04-01  495.895518  482.972782  509.208243\n4 1961-05-01  504.532773  492.160215  518.405717","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>fcst</th>\n      <th>fcst_lower</th>\n      <th>fcst_upper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1961-01-01</td>\n      <td>452.077721</td>\n      <td>438.133669</td>\n      <td>465.379116</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1961-02-01</td>\n      <td>433.529496</td>\n      <td>420.569616</td>\n      <td>446.864959</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1961-03-01</td>\n      <td>492.499917</td>\n      <td>478.899274</td>\n      <td>505.942641</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1961-04-01</td>\n      <td>495.895518</td>\n      <td>482.972782</td>\n      <td>509.208243</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1961-05-01</td>\n      <td>504.532773</td>\n      <td>492.160215</td>\n      <td>518.405717</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"model.plot()","metadata":{"id":"QBOKFNNWt1YV","outputId":"a21c27f4-2041-4551-b8da-bfe43e5afbc2","execution":{"iopub.status.busy":"2022-05-05T08:41:19.436679Z","iopub.execute_input":"2022-05-05T08:41:19.436907Z","iopub.status.idle":"2022-05-05T08:41:20.135788Z","shell.execute_reply.started":"2022-05-05T08:41:19.436880Z","shell.execute_reply":"2022-05-05T08:41:20.135128Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:xlabel='time', ylabel='y'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## GreyKite","metadata":{"id":"MPnxJB35pHlZ"}},{"cell_type":"markdown","source":"pip install greykite","metadata":{"id":"QwMPWGGlzacH","outputId":"e1fd168a-4969-40db-ff12-f62766146943","_kg_hide-output":true}},{"cell_type":"code","source":"from greykite.framework.templates.autogen.forecast_config import ForecastConfig\nfrom greykite.framework.templates.autogen.forecast_config import MetadataParam\nfrom greykite.framework.templates.forecaster import Forecaster \nfrom greykite.framework.templates.model_templates import ModelTemplateEnum\nfrom greykite.framework.utils.result_summary import summarize_grid_search_results","metadata":{"id":"wwGNIHCGpHla","execution":{"iopub.status.busy":"2022-05-05T08:41:20.137208Z","iopub.execute_input":"2022-05-05T08:41:20.137654Z","iopub.status.idle":"2022-05-05T08:41:20.201036Z","shell.execute_reply.started":"2022-05-05T08:41:20.137619Z","shell.execute_reply":"2022-05-05T08:41:20.200254Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Specifies dataset information\nmetadata = MetadataParam(\n     time_col=\"month\",  # name of the time column\n     value_col=\"#Passengers\",  # name of the value column\n     freq=\"MS\"  #\"MS\" for Montly at start date\n )\nforecaster = Forecaster()\nresult = forecaster.run_forecast_config(\n     df=data,\n     config=ForecastConfig(\n         model_template=ModelTemplateEnum.SILVERKITE.name,\n         forecast_horizon=100,  # forecasts 100 steps ahead\n         coverage=0.95,  # 95% prediction intervals\n         metadata_param=metadata\n    )\n)","metadata":{"id":"ct6IDEX1pHlb","outputId":"a70f743b-4f5b-45f4-dcdb-b01a64b2c604","_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-05T08:41:20.202148Z","iopub.execute_input":"2022-05-05T08:41:20.202360Z","iopub.status.idle":"2022-05-05T08:41:35.267570Z","shell.execute_reply.started":"2022-05-05T08:41:20.202333Z","shell.execute_reply":"2022-05-05T08:41:35.266557Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/greykite/framework/pipeline/pipeline.py:186: UserWarning:\n\nNot enough training data to forecast the full forecast_horizon. Exercise extra caution with forecasted values after 72 periods.\n\n/opt/conda/lib/python3.7/site-packages/greykite/framework/pipeline/pipeline.py:199: UserWarning:\n\ntest_horizon should be <= than 1/3 of the data set size to allow enough data to train a backtest model. Consider reducing to 48. If this is smaller than the forecast_horizon, you will need to make a trade-off between setting test_horizon=forecast_horizon and having enough data left over to properly train a realistic backtest model.\n\n/opt/conda/lib/python3.7/site-packages/greykite/sklearn/cross_validation.py:191: UserWarning:\n\nThere are no CV splits under the requested settings. Decrease `forecast_horizon` and/or `min_train_periods`. Using default 90/10 CV split\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_simple_silverkite_helper.py:130: UserWarning:\n\nRequested holiday 'Easter Monday [England, Wales, Northern Ireland]' does not occur in the provided countries\n\n","output_type":"stream"},{"name":"stdout","text":"Fitting 1 folds for each of 1 candidates, totalling 1 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_silverkite.py:2560: UserWarning:\n\nThe granularity of data is larger than daily. Ensure the daily events data match the timestamps\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_silverkite.py:2560: UserWarning:\n\nThe granularity of data is larger than daily. Ensure the daily events data match the timestamps\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_silverkite.py:2560: UserWarning:\n\nThe granularity of data is larger than daily. Ensure the daily events data match the timestamps\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_simple_silverkite_helper.py:130: UserWarning:\n\nRequested holiday 'Easter Monday [England, Wales, Northern Ireland]' does not occur in the provided countries\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_silverkite.py:2560: UserWarning:\n\nThe granularity of data is larger than daily. Ensure the daily events data match the timestamps\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_silverkite.py:2560: UserWarning:\n\nThe granularity of data is larger than daily. Ensure the daily events data match the timestamps\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_simple_silverkite_helper.py:130: UserWarning:\n\nRequested holiday 'Easter Monday [England, Wales, Northern Ireland]' does not occur in the provided countries\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_silverkite.py:2560: UserWarning:\n\nThe granularity of data is larger than daily. Ensure the daily events data match the timestamps\n\n/opt/conda/lib/python3.7/site-packages/greykite/sklearn/transform/null_transformer.py:178: RuntimeWarning:\n\nInput data has many null values. Missing 40.98% of one input.\n\n/opt/conda/lib/python3.7/site-packages/greykite/algo/forecast/silverkite/forecast_silverkite.py:2560: UserWarning:\n\nThe granularity of data is larger than daily. Ensure the daily events data match the timestamps\n\n","output_type":"stream"}]},{"cell_type":"code","source":"ts = result.timeseries\nfig = ts.plot()\nplotly.io.show(fig)","metadata":{"id":"MiaKfp5TpHlb","outputId":"625909ba-ac60-4e7b-f134-91db74236ab2","execution":{"iopub.status.busy":"2022-05-05T08:41:35.268971Z","iopub.execute_input":"2022-05-05T08:41:35.269563Z","iopub.status.idle":"2022-05-05T08:41:35.330365Z","shell.execute_reply.started":"2022-05-05T08:41:35.269517Z","shell.execute_reply":"2022-05-05T08:41:35.328264Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.11.1.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"11d94ab9-69de-44df-b1c3-222be6a8ea2c\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"11d94ab9-69de-44df-b1c3-222be6a8ea2c\")) {                    Plotly.newPlot(                        \"11d94ab9-69de-44df-b1c3-222be6a8ea2c\",                        [{\"line\":{\"color\":\"rgb(32, 149, 212)\"},\"mode\":\"lines\",\"name\":\"#Passengers\",\"opacity\":0.8,\"x\":[\"1949-01-01T00:00:00\",\"1949-02-01T00:00:00\",\"1949-03-01T00:00:00\",\"1949-04-01T00:00:00\",\"1949-05-01T00:00:00\",\"1949-06-01T00:00:00\",\"1949-07-01T00:00:00\",\"1949-08-01T00:00:00\",\"1949-09-01T00:00:00\",\"1949-10-01T00:00:00\",\"1949-11-01T00:00:00\",\"1949-12-01T00:00:00\",\"1950-01-01T00:00:00\",\"1950-02-01T00:00:00\",\"1950-03-01T00:00:00\",\"1950-04-01T00:00:00\",\"1950-05-01T00:00:00\",\"1950-06-01T00:00:00\",\"1950-07-01T00:00:00\",\"1950-08-01T00:00:00\",\"1950-09-01T00:00:00\",\"1950-10-01T00:00:00\",\"1950-11-01T00:00:00\",\"1950-12-01T00:00:00\",\"1951-01-01T00:00:00\",\"1951-02-01T00:00:00\",\"1951-03-01T00:00:00\",\"1951-04-01T00:00:00\",\"1951-05-01T00:00:00\",\"1951-06-01T00:00:00\",\"1951-07-01T00:00:00\",\"1951-08-01T00:00:00\",\"1951-09-01T00:00:00\",\"1951-10-01T00:00:00\",\"1951-11-01T00:00:00\",\"1951-12-01T00:00:00\",\"1952-01-01T00:00:00\",\"1952-02-01T00:00:00\",\"1952-03-01T00:00:00\",\"1952-04-01T00:00:00\",\"1952-05-01T00:00:00\",\"1952-06-01T00:00:00\",\"1952-07-01T00:00:00\",\"1952-08-01T00:00:00\",\"1952-09-01T00:00:00\",\"1952-10-01T00:00:00\",\"1952-11-01T00:00:00\",\"1952-12-01T00:00:00\",\"1953-01-01T00:00:00\",\"1953-02-01T00:00:00\",\"1953-03-01T00:00:00\",\"1953-04-01T00:00:00\",\"1953-05-01T00:00:00\",\"1953-06-01T00:00:00\",\"1953-07-01T00:00:00\",\"1953-08-01T00:00:00\",\"1953-09-01T00:00:00\",\"1953-10-01T00:00:00\",\"1953-11-01T00:00:00\",\"1953-12-01T00:00:00\",\"1954-01-01T00:00:00\",\"1954-02-01T00:00:00\",\"1954-03-01T00:00:00\",\"1954-04-01T00:00:00\",\"1954-05-01T00:00:00\",\"1954-06-01T00:00:00\",\"1954-07-01T00:00:00\",\"1954-08-01T00:00:00\",\"1954-09-01T00:00:00\",\"1954-10-01T00:00:00\",\"1954-11-01T00:00:00\",\"1954-12-01T00:00:00\",\"1955-01-01T00:00:00\",\"1955-02-01T00:00:00\",\"1955-03-01T00:00:00\",\"1955-04-01T00:00:00\",\"1955-05-01T00:00:00\",\"1955-06-01T00:00:00\",\"1955-07-01T00:00:00\",\"1955-08-01T00:00:00\",\"1955-09-01T00:00:00\",\"1955-10-01T00:00:00\",\"1955-11-01T00:00:00\",\"1955-12-01T00:00:00\",\"1956-01-01T00:00:00\",\"1956-02-01T00:00:00\",\"1956-03-01T00:00:00\",\"1956-04-01T00:00:00\",\"1956-05-01T00:00:00\",\"1956-06-01T00:00:00\",\"1956-07-01T00:00:00\",\"1956-08-01T00:00:00\",\"1956-09-01T00:00:00\",\"1956-10-01T00:00:00\",\"1956-11-01T00:00:00\",\"1956-12-01T00:00:00\",\"1957-01-01T00:00:00\",\"1957-02-01T00:00:00\",\"1957-03-01T00:00:00\",\"1957-04-01T00:00:00\",\"1957-05-01T00:00:00\",\"1957-06-01T00:00:00\",\"1957-07-01T00:00:00\",\"1957-08-01T00:00:00\",\"1957-09-01T00:00:00\",\"1957-10-01T00:00:00\",\"1957-11-01T00:00:00\",\"1957-12-01T00:00:00\",\"1958-01-01T00:00:00\",\"1958-02-01T00:00:00\",\"1958-03-01T00:00:00\",\"1958-04-01T00:00:00\",\"1958-05-01T00:00:00\",\"1958-06-01T00:00:00\",\"1958-07-01T00:00:00\",\"1958-08-01T00:00:00\",\"1958-09-01T00:00:00\",\"1958-10-01T00:00:00\",\"1958-11-01T00:00:00\",\"1958-12-01T00:00:00\",\"1959-01-01T00:00:00\",\"1959-02-01T00:00:00\",\"1959-03-01T00:00:00\",\"1959-04-01T00:00:00\",\"1959-05-01T00:00:00\",\"1959-06-01T00:00:00\",\"1959-07-01T00:00:00\",\"1959-08-01T00:00:00\",\"1959-09-01T00:00:00\",\"1959-10-01T00:00:00\",\"1959-11-01T00:00:00\",\"1959-12-01T00:00:00\",\"1960-01-01T00:00:00\",\"1960-02-01T00:00:00\",\"1960-03-01T00:00:00\",\"1960-04-01T00:00:00\",\"1960-05-01T00:00:00\",\"1960-06-01T00:00:00\",\"1960-07-01T00:00:00\",\"1960-08-01T00:00:00\",\"1960-09-01T00:00:00\",\"1960-10-01T00:00:00\",\"1960-11-01T00:00:00\",\"1960-12-01T00:00:00\"],\"y\":[112,118,132,129,121,135,148,148,136,119,104,118,115,126,141,135,125,149,170,170,158,133,114,140,145,150,178,163,172,178,199,199,184,162,146,166,171,180,193,181,183,218,230,242,209,191,172,194,196,196,236,235,229,243,264,272,237,211,180,201,204,188,235,227,234,264,302,293,259,229,203,229,242,233,267,269,270,315,364,347,312,274,237,278,284,277,317,313,318,374,413,405,355,306,271,306,315,301,356,348,355,422,465,467,404,347,305,336,340,318,362,348,363,435,491,505,404,359,310,337,360,342,406,396,420,472,548,559,463,407,362,405,417,391,419,461,472,535,622,606,508,461,390,432],\"type\":\"scatter\"}],                        {\"legend\":{\"traceorder\":\"reversed\"},\"showlegend\":true,\"title\":{\"text\":\"#Passengers vs month\",\"x\":0.5},\"xaxis\":{\"title\":{\"text\":\"month\"}},\"yaxis\":{\"title\":{\"text\":\"#Passengers\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('11d94ab9-69de-44df-b1c3-222be6a8ea2c');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"backtest = result.backtest\nfig = backtest.plot()\nplotly.io.show(fig)","metadata":{"id":"LQi1QZwHpHlc","outputId":"9aba80e1-5d4a-4787-c954-ca5bc35c959b","execution":{"iopub.status.busy":"2022-05-05T08:41:35.331365Z","iopub.execute_input":"2022-05-05T08:41:35.331734Z","iopub.status.idle":"2022-05-05T08:41:35.380815Z","shell.execute_reply.started":"2022-05-05T08:41:35.331703Z","shell.execute_reply":"2022-05-05T08:41:35.379977Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"283f46f3-5ce4-4801-9209-6be8e8fb9312\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"283f46f3-5ce4-4801-9209-6be8e8fb9312\")) {                    Plotly.newPlot(                        \"283f46f3-5ce4-4801-9209-6be8e8fb9312\",                        [{\"legendgroup\":\"interval\",\"line\":{\"color\":\"rgba(0, 90, 181, 0.5)\",\"width\":0.0},\"mode\":\"lines\",\"name\":\"Lower Bound\",\"x\":[\"1949-01-01T00:00:00\",\"1949-02-01T00:00:00\",\"1949-03-01T00:00:00\",\"1949-04-01T00:00:00\",\"1949-05-01T00:00:00\",\"1949-06-01T00:00:00\",\"1949-07-01T00:00:00\",\"1949-08-01T00:00:00\",\"1949-09-01T00:00:00\",\"1949-10-01T00:00:00\",\"1949-11-01T00:00:00\",\"1949-12-01T00:00:00\",\"1950-01-01T00:00:00\",\"1950-02-01T00:00:00\",\"1950-03-01T00:00:00\",\"1950-04-01T00:00:00\",\"1950-05-01T00:00:00\",\"1950-06-01T00:00:00\",\"1950-07-01T00:00:00\",\"1950-08-01T00:00:00\",\"1950-09-01T00:00:00\",\"1950-10-01T00:00:00\",\"1950-11-01T00:00:00\",\"1950-12-01T00:00:00\",\"1951-01-01T00:00:00\",\"1951-02-01T00:00:00\",\"1951-03-01T00:00:00\",\"1951-04-01T00:00:00\",\"1951-05-01T00:00:00\",\"1951-06-01T00:00:00\",\"1951-07-01T00:00:00\",\"1951-08-01T00:00:00\",\"1951-09-01T00:00:00\",\"1951-10-01T00:00:00\",\"1951-11-01T00:00:00\",\"1951-12-01T00:00:00\",\"1952-01-01T00:00:00\",\"1952-02-01T00:00:00\",\"1952-03-01T00:00:00\",\"1952-04-01T00:00:00\",\"1952-05-01T00:00:00\",\"1952-06-01T00:00:00\",\"1952-07-01T00:00:00\",\"1952-08-01T00:00:00\",\"1952-09-01T00:00:00\",\"1952-10-01T00:00:00\",\"1952-11-01T00:00:00\",\"1952-12-01T00:00:00\",\"1953-01-01T00:00:00\",\"1953-02-01T00:00:00\",\"1953-03-01T00:00:00\",\"1953-04-01T00:00:00\",\"1953-05-01T00:00:00\",\"1953-06-01T00:00:00\",\"1953-07-01T00:00:00\",\"1953-08-01T00:00:00\",\"1953-09-01T00:00:00\",\"1953-10-01T00:00:00\",\"1953-11-01T00:00:00\",\"1953-12-01T00:00:00\",\"1954-01-01T00:00:00\",\"1954-02-01T00:00:00\",\"1954-03-01T00:00:00\",\"1954-04-01T00:00:00\",\"1954-05-01T00:00:00\",\"1954-06-01T00:00:00\",\"1954-07-01T00:00:00\",\"1954-08-01T00:00:00\",\"1954-09-01T00:00:00\",\"1954-10-01T00:00:00\",\"1954-11-01T00:00:00\",\"1954-12-01T00:00:00\",\"1955-01-01T00:00:00\",\"1955-02-01T00:00:00\",\"1955-03-01T00:00:00\",\"1955-04-01T00:00:00\",\"1955-05-01T00:00:00\",\"1955-06-01T00:00:00\",\"1955-07-01T00:00:00\",\"1955-08-01T00:00:00\",\"1955-09-01T00:00:00\",\"1955-10-01T00:00:00\",\"1955-11-01T00:00:00\",\"1955-12-01T00:00:00\",\"1956-01-01T00:00:00\",\"1956-02-01T00:00:00\",\"1956-03-01T00:00:00\",\"1956-04-01T00:00:00\",\"1956-05-01T00:00:00\",\"1956-06-01T00:00:00\",\"1956-07-01T00:00:00\",\"1956-08-01T00:00:00\",\"1956-09-01T00:00:00\",\"1956-10-01T00:00:00\",\"1956-11-01T00:00:00\",\"1956-12-01T00:00:00\",\"1957-01-01T00:00:00\",\"1957-02-01T00:00:00\",\"1957-03-01T00:00:00\",\"1957-04-01T00:00:00\",\"1957-05-01T00:00:00\",\"1957-06-01T00:00:00\",\"1957-07-01T00:00:00\",\"1957-08-01T00:00:00\",\"1957-09-01T00:00:00\",\"1957-10-01T00:00:00\",\"1957-11-01T00:00:00\",\"1957-12-01T00:00:00\",\"1958-01-01T00:00:00\",\"1958-02-01T00:00:00\",\"1958-03-01T00:00:00\",\"1958-04-01T00:00:00\",\"1958-05-01T00:00:00\",\"1958-06-01T00:00:00\",\"1958-07-01T00:00:00\",\"1958-08-01T00:00:00\",\"1958-09-01T00:00:00\",\"1958-10-01T00:00:00\",\"1958-11-01T00:00:00\",\"1958-12-01T00:00:00\",\"1959-01-01T00:00:00\",\"1959-02-01T00:00:00\",\"1959-03-01T00:00:00\",\"1959-04-01T00:00:00\",\"1959-05-01T00:00:00\",\"1959-06-01T00:00:00\",\"1959-07-01T00:00:00\",\"1959-08-01T00:00:00\",\"1959-09-01T00:00:00\",\"1959-10-01T00:00:00\",\"1959-11-01T00:00:00\",\"1959-12-01T00:00:00\",\"1960-01-01T00:00:00\",\"1960-02-01T00:00:00\",\"1960-03-01T00:00:00\",\"1960-04-01T00:00:00\",\"1960-05-01T00:00:00\",\"1960-06-01T00:00:00\",\"1960-07-01T00:00:00\",\"1960-08-01T00:00:00\",\"1960-09-01T00:00:00\",\"1960-10-01T00:00:00\",\"1960-11-01T00:00:00\",\"1960-12-01T00:00:00\"],\"y\":[71.39099048931656,91.97915412643091,94.86308297491799,96.62040212790065,90.89924346163139,97.6785348582252,101.77898415761634,97.4830752367671,105.29362246357691,90.91552184962099,94.69701248091201,105.78448872677284,94.72555061099744,113.96684279924851,115.55409880839214,109.3100854611926,111.59025929510555,124.42277876552656,122.46999999109049,124.22731914406843,125.98463829705109,111.60653768309514,115.38802831438618,126.47550456024699,117.44073792278469,134.65785863272265,136.2451146418663,138.00243379484897,130.25710365026663,145.11379459900073,143.16101582456463,144.9183349775426,147.20930709320498,132.2975535165693,136.07904414786032,147.16652039372116,138.13175375625883,155.3440730477462,156.98352512198738,158.7360428565147,153.0102376562608,151.78339513422657,163.88053039981054,165.63304813433786,161.34928807154154,148.33510295440954,156.77950793376974,167.86233764564594,158.822769589733,176.039890299671,177.62714630881464,171.3831329616151,173.66330679552806,186.49582626594906,182.5188760131999,184.27619516617784,188.0576857974736,173.67958518351764,177.46107581480868,188.5485520606695,179.5137854232072,196.73090613314514,198.31816214228883,200.07548129527146,194.3543226290022,205.16267062111012,205.23406332498715,198.9900499777829,208.74870163094775,194.3706010169918,198.15209164828283,209.23956789414365,195.53708549016156,217.42192196661932,219.00917797576298,218.74232565043258,215.0453384624764,227.17523744142204,225.9250791584613,221.6291702376121,229.4397174644219,210.3939010839462,218.84310748175702,229.93058372761783,218.8716456118424,238.1081363816429,239.74758845588406,227.445545616402,235.77430099015749,248.6020190421326,246.6445937337072,248.39711146823453,250.68328216544631,235.766882054826,239.54357126766638,250.6264009795426,241.5868329236297,258.15561721389594,260.3912096427113,262.148528795694,248.4260376292425,260.5559366081882,267.3071108254096,269.0644299783876,270.8217491313702,256.4436485174143,260.22513914870535,271.31261539456614,262.27784875710387,279.49496946704187,281.0822254761855,280.1127526593797,277.11838596289886,289.9509054333199,287.99812665888385,289.75544581186176,290.14420293403583,277.13466435088844,280.9161549821795,292.0036312280403,282.96886459057805,300.185985300516,301.7732413096596,302.827939971167,297.8094017963731,310.64192126679404,306.6649710140449,308.4222901670229,312.2037807983186,297.8256801843626,301.6071708156537,312.6946470615145,303.6598804240522,320.22386329586783,322.51165178978073,323.5615490328327,318.5383643240542,331.3660823760293,329.4086570676038,325.107946728304,332.91369253666323,318.53094538872267,322.3076346015631,333.3904643134393],\"type\":\"scatter\"},{\"fill\":\"tonexty\",\"fillcolor\":\"rgba(0, 90, 181, 0.15)\",\"legendgroup\":\"interval\",\"line\":{\"color\":\"rgba(0, 90, 181, 0.5)\",\"width\":0.0},\"mode\":\"lines\",\"name\":\"Upper Bound\",\"x\":[\"1949-01-01T00:00:00\",\"1949-02-01T00:00:00\",\"1949-03-01T00:00:00\",\"1949-04-01T00:00:00\",\"1949-05-01T00:00:00\",\"1949-06-01T00:00:00\",\"1949-07-01T00:00:00\",\"1949-08-01T00:00:00\",\"1949-09-01T00:00:00\",\"1949-10-01T00:00:00\",\"1949-11-01T00:00:00\",\"1949-12-01T00:00:00\",\"1950-01-01T00:00:00\",\"1950-02-01T00:00:00\",\"1950-03-01T00:00:00\",\"1950-04-01T00:00:00\",\"1950-05-01T00:00:00\",\"1950-06-01T00:00:00\",\"1950-07-01T00:00:00\",\"1950-08-01T00:00:00\",\"1950-09-01T00:00:00\",\"1950-10-01T00:00:00\",\"1950-11-01T00:00:00\",\"1950-12-01T00:00:00\",\"1951-01-01T00:00:00\",\"1951-02-01T00:00:00\",\"1951-03-01T00:00:00\",\"1951-04-01T00:00:00\",\"1951-05-01T00:00:00\",\"1951-06-01T00:00:00\",\"1951-07-01T00:00:00\",\"1951-08-01T00:00:00\",\"1951-09-01T00:00:00\",\"1951-10-01T00:00:00\",\"1951-11-01T00:00:00\",\"1951-12-01T00:00:00\",\"1952-01-01T00:00:00\",\"1952-02-01T00:00:00\",\"1952-03-01T00:00:00\",\"1952-04-01T00:00:00\",\"1952-05-01T00:00:00\",\"1952-06-01T00:00:00\",\"1952-07-01T00:00:00\",\"1952-08-01T00:00:00\",\"1952-09-01T00:00:00\",\"1952-10-01T00:00:00\",\"1952-11-01T00:00:00\",\"1952-12-01T00:00:00\",\"1953-01-01T00:00:00\",\"1953-02-01T00:00:00\",\"1953-03-01T00:00:00\",\"1953-04-01T00:00:00\",\"1953-05-01T00:00:00\",\"1953-06-01T00:00:00\",\"1953-07-01T00:00:00\",\"1953-08-01T00:00:00\",\"1953-09-01T00:00:00\",\"1953-10-01T00:00:00\",\"1953-11-01T00:00:00\",\"1953-12-01T00:00:00\",\"1954-01-01T00:00:00\",\"1954-02-01T00:00:00\",\"1954-03-01T00:00:00\",\"1954-04-01T00:00:00\",\"1954-05-01T00:00:00\",\"1954-06-01T00:00:00\",\"1954-07-01T00:00:00\",\"1954-08-01T00:00:00\",\"1954-09-01T00:00:00\",\"1954-10-01T00:00:00\",\"1954-11-01T00:00:00\",\"1954-12-01T00:00:00\",\"1955-01-01T00:00:00\",\"1955-02-01T00:00:00\",\"1955-03-01T00:00:00\",\"1955-04-01T00:00:00\",\"1955-05-01T00:00:00\",\"1955-06-01T00:00:00\",\"1955-07-01T00:00:00\",\"1955-08-01T00:00:00\",\"1955-09-01T00:00:00\",\"1955-10-01T00:00:00\",\"1955-11-01T00:00:00\",\"1955-12-01T00:00:00\",\"1956-01-01T00:00:00\",\"1956-02-01T00:00:00\",\"1956-03-01T00:00:00\",\"1956-04-01T00:00:00\",\"1956-05-01T00:00:00\",\"1956-06-01T00:00:00\",\"1956-07-01T00:00:00\",\"1956-08-01T00:00:00\",\"1956-09-01T00:00:00\",\"1956-10-01T00:00:00\",\"1956-11-01T00:00:00\",\"1956-12-01T00:00:00\",\"1957-01-01T00:00:00\",\"1957-02-01T00:00:00\",\"1957-03-01T00:00:00\",\"1957-04-01T00:00:00\",\"1957-05-01T00:00:00\",\"1957-06-01T00:00:00\",\"1957-07-01T00:00:00\",\"1957-08-01T00:00:00\",\"1957-09-01T00:00:00\",\"1957-10-01T00:00:00\",\"1957-11-01T00:00:00\",\"1957-12-01T00:00:00\",\"1958-01-01T00:00:00\",\"1958-02-01T00:00:00\",\"1958-03-01T00:00:00\",\"1958-04-01T00:00:00\",\"1958-05-01T00:00:00\",\"1958-06-01T00:00:00\",\"1958-07-01T00:00:00\",\"1958-08-01T00:00:00\",\"1958-09-01T00:00:00\",\"1958-10-01T00:00:00\",\"1958-11-01T00:00:00\",\"1958-12-01T00:00:00\",\"1959-01-01T00:00:00\",\"1959-02-01T00:00:00\",\"1959-03-01T00:00:00\",\"1959-04-01T00:00:00\",\"1959-05-01T00:00:00\",\"1959-06-01T00:00:00\",\"1959-07-01T00:00:00\",\"1959-08-01T00:00:00\",\"1959-09-01T00:00:00\",\"1959-10-01T00:00:00\",\"1959-11-01T00:00:00\",\"1959-12-01T00:00:00\",\"1960-01-01T00:00:00\",\"1960-02-01T00:00:00\",\"1960-03-01T00:00:00\",\"1960-04-01T00:00:00\",\"1960-05-01T00:00:00\",\"1960-06-01T00:00:00\",\"1960-07-01T00:00:00\",\"1960-08-01T00:00:00\",\"1960-09-01T00:00:00\",\"1960-10-01T00:00:00\",\"1960-11-01T00:00:00\",\"1960-12-01T00:00:00\"],\"y\":[135.24748221721723,155.83564585433157,158.71957470281865,160.4768938558013,154.75573518953203,161.53502658612587,165.635475885517,161.33956696466777,169.15011419147757,154.77201357752165,158.55350420881265,169.6409804546735,158.5820423388981,177.82333452714917,179.4105905362928,173.16657718909326,175.44675102300621,188.27927049342722,186.32649171899115,188.0838108719691,189.84113002495175,175.4630294109958,179.24452004228684,190.33199628814765,181.29722965068535,198.51435036062333,200.10160636976698,201.85892552274964,194.1135953781673,208.9702863269014,207.0175075524653,208.77482670544327,211.06579882110566,196.15404524446998,199.935535875761,211.02301212162183,201.9882454841595,219.20056477564688,220.84001684988806,222.59253458441538,216.8667293841615,215.63988686212724,227.7370221277112,229.48953986223853,225.2057797994422,212.1915946823102,220.6359996616704,231.7188293735466,222.6792613176337,239.89638202757166,241.48363803671532,235.23962468951578,237.51979852342873,250.35231799384974,246.37536774110058,248.13268689407852,251.91417752537427,237.53607691141832,241.31756754270936,252.40504378857017,243.37027715110787,260.5873978610458,262.1746538701895,263.93197302317213,258.21081435690286,269.0191623490108,269.0905550528878,262.84654170568353,272.6051933588484,258.22709274489245,262.0085833761835,273.0960596220443,259.3935772180622,281.27841369452,282.8656697036636,282.59881737833325,278.90183019037704,291.0317291693227,289.781570886362,285.48566196551275,293.2962091923226,274.25039281184684,282.69959920965766,293.7870754555185,282.72813733974306,301.9646281095436,303.60408018378473,291.3020373443027,299.63079271805816,312.4585107700333,310.5010854616079,312.2536031961352,314.539773893347,299.6233737827267,303.40006299556705,314.4828927074433,305.44332465153036,322.0121089417966,324.247701370612,326.0050205235947,312.2825293571432,324.41242833608885,331.1636025533103,332.92092170628825,334.6782408592709,320.300140245315,324.081630876606,335.1691071224668,326.13434048500454,343.35146119494254,344.9387172040862,343.96924438728036,340.97487769079953,353.8073971612206,351.8546183867845,353.61193753976244,354.0006946619365,340.9911560787891,344.77264671008015,355.860122955941,346.8253563184787,364.04247702841667,365.6297330375603,366.68443169906766,361.66589352427377,374.4984129946947,370.5214627419456,372.27878189492355,376.06027252621925,361.6821719122633,365.4636625435544,376.5511387894152,367.51637215195285,384.0803550237685,386.3681435176814,387.4180407607334,382.3948560519549,395.22257410392996,393.2651487955045,388.9644384562047,396.7701842645639,382.38743711662335,386.1641263294638,397.24695604133996],\"type\":\"scatter\"},{\"line\":{\"color\":\"rgba(250, 43, 20, 0.7)\"},\"marker\":{\"color\":\"rgba(250, 43, 20, 0.7)\",\"size\":2.0},\"mode\":\"lines+markers\",\"name\":\"Actual\",\"opacity\":1.0,\"x\":[\"1949-01-01T00:00:00\",\"1949-02-01T00:00:00\",\"1949-03-01T00:00:00\",\"1949-04-01T00:00:00\",\"1949-05-01T00:00:00\",\"1949-06-01T00:00:00\",\"1949-07-01T00:00:00\",\"1949-08-01T00:00:00\",\"1949-09-01T00:00:00\",\"1949-10-01T00:00:00\",\"1949-11-01T00:00:00\",\"1949-12-01T00:00:00\",\"1950-01-01T00:00:00\",\"1950-02-01T00:00:00\",\"1950-03-01T00:00:00\",\"1950-04-01T00:00:00\",\"1950-05-01T00:00:00\",\"1950-06-01T00:00:00\",\"1950-07-01T00:00:00\",\"1950-08-01T00:00:00\",\"1950-09-01T00:00:00\",\"1950-10-01T00:00:00\",\"1950-11-01T00:00:00\",\"1950-12-01T00:00:00\",\"1951-01-01T00:00:00\",\"1951-02-01T00:00:00\",\"1951-03-01T00:00:00\",\"1951-04-01T00:00:00\",\"1951-05-01T00:00:00\",\"1951-06-01T00:00:00\",\"1951-07-01T00:00:00\",\"1951-08-01T00:00:00\",\"1951-09-01T00:00:00\",\"1951-10-01T00:00:00\",\"1951-11-01T00:00:00\",\"1951-12-01T00:00:00\",\"1952-01-01T00:00:00\",\"1952-02-01T00:00:00\",\"1952-03-01T00:00:00\",\"1952-04-01T00:00:00\",\"1952-05-01T00:00:00\",\"1952-06-01T00:00:00\",\"1952-07-01T00:00:00\",\"1952-08-01T00:00:00\",\"1952-09-01T00:00:00\",\"1952-10-01T00:00:00\",\"1952-11-01T00:00:00\",\"1952-12-01T00:00:00\",\"1953-01-01T00:00:00\",\"1953-02-01T00:00:00\",\"1953-03-01T00:00:00\",\"1953-04-01T00:00:00\",\"1953-05-01T00:00:00\",\"1953-06-01T00:00:00\",\"1953-07-01T00:00:00\",\"1953-08-01T00:00:00\",\"1953-09-01T00:00:00\",\"1953-10-01T00:00:00\",\"1953-11-01T00:00:00\",\"1953-12-01T00:00:00\",\"1954-01-01T00:00:00\",\"1954-02-01T00:00:00\",\"1954-03-01T00:00:00\",\"1954-04-01T00:00:00\",\"1954-05-01T00:00:00\",\"1954-06-01T00:00:00\",\"1954-07-01T00:00:00\",\"1954-08-01T00:00:00\",\"1954-09-01T00:00:00\",\"1954-10-01T00:00:00\",\"1954-11-01T00:00:00\",\"1954-12-01T00:00:00\",\"1955-01-01T00:00:00\",\"1955-02-01T00:00:00\",\"1955-03-01T00:00:00\",\"1955-04-01T00:00:00\",\"1955-05-01T00:00:00\",\"1955-06-01T00:00:00\",\"1955-07-01T00:00:00\",\"1955-08-01T00:00:00\",\"1955-09-01T00:00:00\",\"1955-10-01T00:00:00\",\"1955-11-01T00:00:00\",\"1955-12-01T00:00:00\",\"1956-01-01T00:00:00\",\"1956-02-01T00:00:00\",\"1956-03-01T00:00:00\",\"1956-04-01T00:00:00\",\"1956-05-01T00:00:00\",\"1956-06-01T00:00:00\",\"1956-07-01T00:00:00\",\"1956-08-01T00:00:00\",\"1956-09-01T00:00:00\",\"1956-10-01T00:00:00\",\"1956-11-01T00:00:00\",\"1956-12-01T00:00:00\",\"1957-01-01T00:00:00\",\"1957-02-01T00:00:00\",\"1957-03-01T00:00:00\",\"1957-04-01T00:00:00\",\"1957-05-01T00:00:00\",\"1957-06-01T00:00:00\",\"1957-07-01T00:00:00\",\"1957-08-01T00:00:00\",\"1957-09-01T00:00:00\",\"1957-10-01T00:00:00\",\"1957-11-01T00:00:00\",\"1957-12-01T00:00:00\",\"1958-01-01T00:00:00\",\"1958-02-01T00:00:00\",\"1958-03-01T00:00:00\",\"1958-04-01T00:00:00\",\"1958-05-01T00:00:00\",\"1958-06-01T00:00:00\",\"1958-07-01T00:00:00\",\"1958-08-01T00:00:00\",\"1958-09-01T00:00:00\",\"1958-10-01T00:00:00\",\"1958-11-01T00:00:00\",\"1958-12-01T00:00:00\",\"1959-01-01T00:00:00\",\"1959-02-01T00:00:00\",\"1959-03-01T00:00:00\",\"1959-04-01T00:00:00\",\"1959-05-01T00:00:00\",\"1959-06-01T00:00:00\",\"1959-07-01T00:00:00\",\"1959-08-01T00:00:00\",\"1959-09-01T00:00:00\",\"1959-10-01T00:00:00\",\"1959-11-01T00:00:00\",\"1959-12-01T00:00:00\",\"1960-01-01T00:00:00\",\"1960-02-01T00:00:00\",\"1960-03-01T00:00:00\",\"1960-04-01T00:00:00\",\"1960-05-01T00:00:00\",\"1960-06-01T00:00:00\",\"1960-07-01T00:00:00\",\"1960-08-01T00:00:00\",\"1960-09-01T00:00:00\",\"1960-10-01T00:00:00\",\"1960-11-01T00:00:00\",\"1960-12-01T00:00:00\"],\"y\":[112,118,132,129,121,135,148,148,136,119,104,118,115,126,141,135,125,149,170,170,158,133,114,140,145,150,178,163,172,178,199,199,184,162,146,166,171,180,193,181,183,218,230,242,209,191,172,194,196,196,236,235,229,243,264,272,237,211,180,201,204,188,235,227,234,264,302,293,259,229,203,229,242,233,267,269,270,315,364,347,312,274,237,278,284,277,317,313,318,374,413,405,355,306,271,306,315,301,356,348,355,422,465,467,404,347,305,336,340,318,362,348,363,435,491,505,404,359,310,337,360,342,406,396,420,472,548,559,463,407,362,405,417,391,419,461,472,535,622,606,508,461,390,432],\"type\":\"scatter\"},{\"line\":{\"color\":\"rgba(0, 90, 181, 0.7)\",\"dash\":\"solid\"},\"name\":\"Forecast\",\"x\":[\"1949-01-01T00:00:00\",\"1949-02-01T00:00:00\",\"1949-03-01T00:00:00\",\"1949-04-01T00:00:00\",\"1949-05-01T00:00:00\",\"1949-06-01T00:00:00\",\"1949-07-01T00:00:00\",\"1949-08-01T00:00:00\",\"1949-09-01T00:00:00\",\"1949-10-01T00:00:00\",\"1949-11-01T00:00:00\",\"1949-12-01T00:00:00\",\"1950-01-01T00:00:00\",\"1950-02-01T00:00:00\",\"1950-03-01T00:00:00\",\"1950-04-01T00:00:00\",\"1950-05-01T00:00:00\",\"1950-06-01T00:00:00\",\"1950-07-01T00:00:00\",\"1950-08-01T00:00:00\",\"1950-09-01T00:00:00\",\"1950-10-01T00:00:00\",\"1950-11-01T00:00:00\",\"1950-12-01T00:00:00\",\"1951-01-01T00:00:00\",\"1951-02-01T00:00:00\",\"1951-03-01T00:00:00\",\"1951-04-01T00:00:00\",\"1951-05-01T00:00:00\",\"1951-06-01T00:00:00\",\"1951-07-01T00:00:00\",\"1951-08-01T00:00:00\",\"1951-09-01T00:00:00\",\"1951-10-01T00:00:00\",\"1951-11-01T00:00:00\",\"1951-12-01T00:00:00\",\"1952-01-01T00:00:00\",\"1952-02-01T00:00:00\",\"1952-03-01T00:00:00\",\"1952-04-01T00:00:00\",\"1952-05-01T00:00:00\",\"1952-06-01T00:00:00\",\"1952-07-01T00:00:00\",\"1952-08-01T00:00:00\",\"1952-09-01T00:00:00\",\"1952-10-01T00:00:00\",\"1952-11-01T00:00:00\",\"1952-12-01T00:00:00\",\"1953-01-01T00:00:00\",\"1953-02-01T00:00:00\",\"1953-03-01T00:00:00\",\"1953-04-01T00:00:00\",\"1953-05-01T00:00:00\",\"1953-06-01T00:00:00\",\"1953-07-01T00:00:00\",\"1953-08-01T00:00:00\",\"1953-09-01T00:00:00\",\"1953-10-01T00:00:00\",\"1953-11-01T00:00:00\",\"1953-12-01T00:00:00\",\"1954-01-01T00:00:00\",\"1954-02-01T00:00:00\",\"1954-03-01T00:00:00\",\"1954-04-01T00:00:00\",\"1954-05-01T00:00:00\",\"1954-06-01T00:00:00\",\"1954-07-01T00:00:00\",\"1954-08-01T00:00:00\",\"1954-09-01T00:00:00\",\"1954-10-01T00:00:00\",\"1954-11-01T00:00:00\",\"1954-12-01T00:00:00\",\"1955-01-01T00:00:00\",\"1955-02-01T00:00:00\",\"1955-03-01T00:00:00\",\"1955-04-01T00:00:00\",\"1955-05-01T00:00:00\",\"1955-06-01T00:00:00\",\"1955-07-01T00:00:00\",\"1955-08-01T00:00:00\",\"1955-09-01T00:00:00\",\"1955-10-01T00:00:00\",\"1955-11-01T00:00:00\",\"1955-12-01T00:00:00\",\"1956-01-01T00:00:00\",\"1956-02-01T00:00:00\",\"1956-03-01T00:00:00\",\"1956-04-01T00:00:00\",\"1956-05-01T00:00:00\",\"1956-06-01T00:00:00\",\"1956-07-01T00:00:00\",\"1956-08-01T00:00:00\",\"1956-09-01T00:00:00\",\"1956-10-01T00:00:00\",\"1956-11-01T00:00:00\",\"1956-12-01T00:00:00\",\"1957-01-01T00:00:00\",\"1957-02-01T00:00:00\",\"1957-03-01T00:00:00\",\"1957-04-01T00:00:00\",\"1957-05-01T00:00:00\",\"1957-06-01T00:00:00\",\"1957-07-01T00:00:00\",\"1957-08-01T00:00:00\",\"1957-09-01T00:00:00\",\"1957-10-01T00:00:00\",\"1957-11-01T00:00:00\",\"1957-12-01T00:00:00\",\"1958-01-01T00:00:00\",\"1958-02-01T00:00:00\",\"1958-03-01T00:00:00\",\"1958-04-01T00:00:00\",\"1958-05-01T00:00:00\",\"1958-06-01T00:00:00\",\"1958-07-01T00:00:00\",\"1958-08-01T00:00:00\",\"1958-09-01T00:00:00\",\"1958-10-01T00:00:00\",\"1958-11-01T00:00:00\",\"1958-12-01T00:00:00\",\"1959-01-01T00:00:00\",\"1959-02-01T00:00:00\",\"1959-03-01T00:00:00\",\"1959-04-01T00:00:00\",\"1959-05-01T00:00:00\",\"1959-06-01T00:00:00\",\"1959-07-01T00:00:00\",\"1959-08-01T00:00:00\",\"1959-09-01T00:00:00\",\"1959-10-01T00:00:00\",\"1959-11-01T00:00:00\",\"1959-12-01T00:00:00\",\"1960-01-01T00:00:00\",\"1960-02-01T00:00:00\",\"1960-03-01T00:00:00\",\"1960-04-01T00:00:00\",\"1960-05-01T00:00:00\",\"1960-06-01T00:00:00\",\"1960-07-01T00:00:00\",\"1960-08-01T00:00:00\",\"1960-09-01T00:00:00\",\"1960-10-01T00:00:00\",\"1960-11-01T00:00:00\",\"1960-12-01T00:00:00\"],\"y\":[103.31923635326689,123.90739999038124,126.79132883886831,128.54864799185097,122.82748932558171,129.60678072217553,133.70723002156666,129.41132110071743,137.22186832752723,122.84376771357131,126.62525834486233,137.71273459072316,126.65379647494777,145.89508866319883,147.48234467234246,141.23833132514292,143.51850515905588,156.35102462947688,154.3982458550408,156.15556500801875,157.91288416100141,143.53478354704546,147.3162741783365,158.4037504241973,149.36898378673501,166.586104496673,168.17336050581665,169.9306796587993,162.18534951421697,177.04204046295106,175.08926168851497,176.84658084149294,179.13755295715532,164.22579938051965,168.00729001181065,179.0947662576715,170.05999962020917,187.27231891169654,188.91177098593772,190.66428872046504,184.93848352021115,183.7116409981769,195.80877626376088,197.5612939982882,193.27753393549187,180.26334881835987,188.70775379772007,199.79058350959627,190.75101545368335,207.96813616362132,209.55539217276498,203.31137882556544,205.5915526594784,218.4240721298994,214.44712187715024,216.20444103012818,219.98593166142393,205.60783104746798,209.38932167875902,220.47679792461983,211.44203128715753,228.65915199709548,230.24640800623916,232.0037271592218,226.28256849295255,237.09091648506046,237.16230918893748,230.91829584173323,240.6769474948981,226.29884688094214,230.08033751223317,241.16781375809398,227.4653313541119,249.35016783056966,250.93742383971332,250.6705715143829,246.97358432642673,259.1034833053724,257.85332502241164,253.55741610156244,261.36796332837224,242.32214694789653,250.77135334570735,261.85882959156817,250.79989147579275,270.03638224559324,271.6758343198344,259.37379148035234,267.7025468541078,280.53026490608295,278.57283959765755,280.32535733218486,282.61152802939665,267.69512791877634,271.4718171316167,282.55464684349295,273.51507878758,290.0838630778463,292.31945550666165,294.07677465964434,280.35428349319284,292.4841824721385,299.23535668935995,300.9926758423379,302.74999499532055,288.37189438136465,292.1533850126557,303.2408612585165,294.2060946210542,311.4232153309922,313.01047134013584,312.04099852333,309.0466318268492,321.87915129727025,319.9263725228342,321.6836916758121,322.07244879798617,309.0629102148388,312.8444008461298,323.93187709199066,314.8971104545284,332.11423116446633,333.70148717360996,334.7561858351173,329.73764766032343,342.5701671307444,338.59321687799525,340.3505360309732,344.1320266622689,329.75392604831296,333.53541667960405,344.62289292546484,335.5881262880025,352.15210915981817,354.43989765373107,355.48979489678305,350.46661018800455,363.2943282399796,361.33690293155416,357.03619259225434,364.84193840061357,350.459191252673,354.23588046551345,365.3187101773896],\"type\":\"scatter\"}],                        {\"legend\":{\"traceorder\":\"reversed\"},\"showlegend\":true,\"title\":{\"text\":\"Forecast vs Actual\",\"x\":0.5},\"xaxis\":{\"title\":{\"text\":\"ts\"}},\"yaxis\":{\"title\":{\"text\":\"#Passengers\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"shapes\":[{\"line\":{\"color\":\"rgba(100, 100, 100, 0.9)\",\"width\":1.0},\"type\":\"line\",\"x0\":\"1952-08-01T00:00:00\",\"x1\":\"1952-08-01T00:00:00\",\"xref\":\"x\",\"y0\":0,\"y1\":1,\"yref\":\"paper\"}],\"annotations\":[{\"arrowhead\":0,\"ax\":-60,\"ay\":0,\"showarrow\":true,\"text\":\"Train End Date\",\"x\":\"1952-08-01T00:00:00\",\"xref\":\"x\",\"y\":0.97,\"yref\":\"paper\"}]},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('283f46f3-5ce4-4801-9209-6be8e8fb9312');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"fig = backtest.plot_components()\nplotly.io.show(fig) ","metadata":{"id":"Gvg6Sgpty7lK","outputId":"563b7b2c-b6ad-4143-8997-e7151a84ab9b","execution":{"iopub.status.busy":"2022-05-05T08:41:35.382346Z","iopub.execute_input":"2022-05-05T08:41:35.382635Z","iopub.status.idle":"2022-05-05T08:41:35.511403Z","shell.execute_reply.started":"2022-05-05T08:41:35.382598Z","shell.execute_reply":"2022-05-05T08:41:35.510527Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"485417bf-b38c-4202-99e3-563734389ea1\" class=\"plotly-graph-div\" style=\"height:1050px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"485417bf-b38c-4202-99e3-563734389ea1\")) {                    Plotly.newPlot(                        \"485417bf-b38c-4202-99e3-563734389ea1\",                        [{\"mode\":\"lines\",\"name\":\"y\",\"opacity\":0.8,\"showlegend\":false,\"x\":[\"1949-01-01T00:00:00\",\"1949-02-01T00:00:00\",\"1949-03-01T00:00:00\",\"1949-04-01T00:00:00\",\"1949-05-01T00:00:00\",\"1949-06-01T00:00:00\",\"1949-07-01T00:00:00\",\"1949-08-01T00:00:00\",\"1949-09-01T00:00:00\",\"1949-10-01T00:00:00\",\"1949-11-01T00:00:00\",\"1949-12-01T00:00:00\",\"1950-01-01T00:00:00\",\"1950-02-01T00:00:00\",\"1950-03-01T00:00:00\",\"1950-04-01T00:00:00\",\"1950-05-01T00:00:00\",\"1950-06-01T00:00:00\",\"1950-07-01T00:00:00\",\"1950-08-01T00:00:00\",\"1950-09-01T00:00:00\",\"1950-10-01T00:00:00\",\"1950-11-01T00:00:00\",\"1950-12-01T00:00:00\",\"1951-01-01T00:00:00\",\"1951-02-01T00:00:00\",\"1951-03-01T00:00:00\",\"1951-04-01T00:00:00\",\"1951-05-01T00:00:00\",\"1951-06-01T00:00:00\",\"1951-07-01T00:00:00\",\"1951-08-01T00:00:00\",\"1951-09-01T00:00:00\",\"1951-10-01T00:00:00\",\"1951-11-01T00:00:00\",\"1951-12-01T00:00:00\",\"1952-01-01T00:00:00\",\"1952-02-01T00:00:00\",\"1952-03-01T00:00:00\",\"1952-04-01T00:00:00\",\"1952-05-01T00:00:00\",\"1952-06-01T00:00:00\",\"1952-07-01T00:00:00\",\"1952-08-01T00:00:00\"],\"y\":[112,118,132,129,121,135,148,148,136,119,104,118,115,126,141,135,125,149,170,170,158,133,114,140,145,150,178,163,172,178,199,199,184,162,146,166,171,180,193,181,183,218,230,242],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"name\":\"trend\",\"opacity\":0.8,\"showlegend\":false,\"x\":[\"1949-01-01T00:00:00\",\"1949-02-01T00:00:00\",\"1949-03-01T00:00:00\",\"1949-04-01T00:00:00\",\"1949-05-01T00:00:00\",\"1949-06-01T00:00:00\",\"1949-07-01T00:00:00\",\"1949-08-01T00:00:00\",\"1949-09-01T00:00:00\",\"1949-10-01T00:00:00\",\"1949-11-01T00:00:00\",\"1949-12-01T00:00:00\",\"1950-01-01T00:00:00\",\"1950-02-01T00:00:00\",\"1950-03-01T00:00:00\",\"1950-04-01T00:00:00\",\"1950-05-01T00:00:00\",\"1950-06-01T00:00:00\",\"1950-07-01T00:00:00\",\"1950-08-01T00:00:00\",\"1950-09-01T00:00:00\",\"1950-10-01T00:00:00\",\"1950-11-01T00:00:00\",\"1950-12-01T00:00:00\",\"1951-01-01T00:00:00\",\"1951-02-01T00:00:00\",\"1951-03-01T00:00:00\",\"1951-04-01T00:00:00\",\"1951-05-01T00:00:00\",\"1951-06-01T00:00:00\",\"1951-07-01T00:00:00\",\"1951-08-01T00:00:00\",\"1951-09-01T00:00:00\",\"1951-10-01T00:00:00\",\"1951-11-01T00:00:00\",\"1951-12-01T00:00:00\",\"1952-01-01T00:00:00\",\"1952-02-01T00:00:00\",\"1952-03-01T00:00:00\",\"1952-04-01T00:00:00\",\"1952-05-01T00:00:00\",\"1952-06-01T00:00:00\",\"1952-07-01T00:00:00\",\"1952-08-01T00:00:00\"],\"y\":[0.0,1.7573191529779484,3.3445751621215978,5.101894315104251,6.802525753470766,8.559844906448715,10.260476344819935,12.017795497797882,13.775114650780536,15.475746089147052,17.233065242125,18.93369668049622,20.691015833474168,22.448334986452117,24.035590995595765,25.792910148578418,27.493541586944936,29.25086073992288,30.951492178294103,32.70881133127205,34.466130484254705,36.16676192262122,37.92408107559917,39.62471251397039,41.382031666948336,43.139350819926285,44.72660682906994,46.48392598205259,48.184557420419104,49.94187657339705,51.64250801176827,53.39982716474622,55.15714631772887,56.85777775609539,58.61509690907334,60.315728347444555,62.073047500422504,63.825565234949835,65.465017309191,67.21753504371833,68.91351994810022,70.66603768263226,72.36202258701415,74.11454032154148],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"name\":\"events\",\"opacity\":0.8,\"showlegend\":false,\"x\":[\"1949-01-01T00:00:00\",\"1949-02-01T00:00:00\",\"1949-03-01T00:00:00\",\"1949-04-01T00:00:00\",\"1949-05-01T00:00:00\",\"1949-06-01T00:00:00\",\"1949-07-01T00:00:00\",\"1949-08-01T00:00:00\",\"1949-09-01T00:00:00\",\"1949-10-01T00:00:00\",\"1949-11-01T00:00:00\",\"1949-12-01T00:00:00\",\"1950-01-01T00:00:00\",\"1950-02-01T00:00:00\",\"1950-03-01T00:00:00\",\"1950-04-01T00:00:00\",\"1950-05-01T00:00:00\",\"1950-06-01T00:00:00\",\"1950-07-01T00:00:00\",\"1950-08-01T00:00:00\",\"1950-09-01T00:00:00\",\"1950-10-01T00:00:00\",\"1950-11-01T00:00:00\",\"1950-12-01T00:00:00\",\"1951-01-01T00:00:00\",\"1951-02-01T00:00:00\",\"1951-03-01T00:00:00\",\"1951-04-01T00:00:00\",\"1951-05-01T00:00:00\",\"1951-06-01T00:00:00\",\"1951-07-01T00:00:00\",\"1951-08-01T00:00:00\",\"1951-09-01T00:00:00\",\"1951-10-01T00:00:00\",\"1951-11-01T00:00:00\",\"1951-12-01T00:00:00\",\"1952-01-01T00:00:00\",\"1952-02-01T00:00:00\",\"1952-03-01T00:00:00\",\"1952-04-01T00:00:00\",\"1952-05-01T00:00:00\",\"1952-06-01T00:00:00\",\"1952-07-01T00:00:00\",\"1952-08-01T00:00:00\"],\"y\":[-20.127517323479815,-1.2966728393434188,0.0,0.0,-7.421790104635766,-2.3998178610198826,0.0,-6.053228073827174,0.0,-16.078732052322458,-14.054560574009377,-4.667715766519777,-17.483973035273117,0.0,0.0,-8.001332500182203,-7.421790104635766,3.653410212807292,0.0,0.0,0.0,-16.078732052322458,-14.054560574009377,-4.667715766519777,-15.459801556960038,0.0,0.0,0.0,-9.445961582948847,3.653410212807292,0.0,0.0,0.533652962679741,-16.078732052322458,-14.054560574009377,-4.667715766519777,-15.459801556960038,0.0,0.0,0.0,-7.421790104635766,-10.401150361202085,0.0,0.0],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"ts\"},\"showline\":true,\"mirror\":true},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.7444444444444445,1.0],\"title\":{\"text\":\"y\"},\"showline\":true,\"mirror\":true},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"ts\"},\"showline\":true,\"mirror\":true},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.37222222222222223,0.6277777777777778],\"title\":{\"text\":\"trend\"},\"showline\":true,\"mirror\":true},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"ts\"},\"showline\":true,\"mirror\":true},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.2555555555555556],\"title\":{\"text\":\"events\"},\"showline\":true,\"mirror\":true},\"title\":{\"text\":\"Component plots\",\"x\":0.5},\"showlegend\":true,\"height\":1050},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('485417bf-b38c-4202-99e3-563734389ea1');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]}]}